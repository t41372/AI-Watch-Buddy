This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  ai_watch_buddy/
    agent/
      video_action_agent_interface.py
    prompt/
      action_gen.py
    action_generate.py
    ai_actions.py
    connection_manager.py
    fetch_video.py
    pipeline.py
    server.py
    session.py
    tts.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/ai_watch_buddy/agent/video_action_agent_interface.py">
import abc
from typing import AsyncGenerator, Dict, List, Optional

from google import genai
from google.genai.types import File, Content

class VideoActionAgentInterface(abc.ABC):
    """
    An interface for a Gemini agent that analyzes a video to produce a summary
    and then generates structured actions based on different contexts.

    This agent operates in two main modes for action generation:
    1. 'video': Uses the original video file as the primary context.
    2. 'summary': Uses a pre-generated text summary of the video as context.
    """

    @property
    @abc.abstractmethod
    def client(self) -> genai.GenerativeModel:
        """The initialized GenerativeModel client for API communication."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary_prompt(self) -> str:
        """The system prompt used for generating the video summary."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def action_prompt(self) -> str:
        """The system prompt used for generating actions/dialogue."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def video_file(self) -> Optional[File]:
        """The File object returned by the Gemini API after video upload."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary(self) -> Optional[str]:
        """The text summary of the video, generated on demand."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def contents(self) -> List[Content]:
        """The conversation history stored as a list of Content objects."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary_ready(self) -> bool:
        """A boolean flag indicating if the video summary has been successfully generated."""
        raise NotImplementedError

    @abc.abstractmethod
    async def get_video_summary(self, video_path_or_url: str) -> None:
        """
        Processes a video from a local path or URL, uploads it, and generates a summary.

        This is a non-blocking asynchronous method. Upon successful completion,
        it populates the `summary` attribute and sets the `summary_ready` flag to True.

        Args:
            video_path_or_url: The local file path or a public URL to the video.
        """
        raise NotImplementedError

    @abc.abstractmethod
    def add_content(self, role: str, text: str) -> None:
        """
        Adds a new piece of text content to the conversation history.

        Args:
            role: The role of the author, must be 'user' or 'model'.
            text: The text message to add to the history.
        """
        raise NotImplementedError

    @abc.abstractmethod
    async def generate(self, mode: str) -> AsyncGenerator[Dict, None]:
        """
        Generates a stream of structured actions from the model.

        This method constructs the context based on the specified mode and streams
        the response. 
        It is designed to yield a dictionary for each complete JSON object received from the model.

        Args:
            mode: The context mode for generation, either 'video' or 'summary'.

        Yields:
            A dictionary representing a single structured action from the model's streamed response.

        Raises:
            RuntimeError: If `mode` is 'summary' and the `summary_ready` flag is False.
        """
        # The 'yield' keyword makes this a generator, matching the signature.
        # This is a placeholder and will not be executed in the interface.
        yield {}
        raise NotImplementedError
</file>

<file path="src/ai_watch_buddy/prompt/action_gen.py">
import json
from ..ai_actions import ActionScript


def generate_reaction_script(
    character_settings: str,
    json_schema: str = json.dumps(
        ActionScript.model_json_schema(), ensure_ascii=False, indent=2
    ),
) -> str:
    """
    Generates a reaction script for a video based on the provided JSON schema.
    The script includes actions like speaking, pausing, seeking, and replaying segments.
    The output is a JSON object that adheres to the specified schema.
    """
    return (
        f"""
# SYSTEM PROMPT

You are reacting to a video with your human friend (the user). Your task is to generate a "Reaction Script" in JSON format that details the sequence of actions you will take while watching a video. Your reaction should be natural, engaging, and feel like a real person watching and commenting.

Here is the role prompt for the character settings you will adhere to when speaking and reacting.
```markdown
{character_settings}
```
"""
        + """

**RULES:**
1.  You MUST output a valid JSON object that strictly adheres to the provided JSON Schema. Do NOT output any text before or after the JSON object.
2.  Your output MUST be a single JSON object, starting with `{` and ending with `}`.
3.  The root of the JSON object must have strictly adheres to the JSON schema, and must include all properties defined in the schema.
4.  Use the `comment` field in each action object to explain your thought process for choosing that action. This is for your internal monologue.
5.  The flow of actions should be logical. You can pause, speak, seek to rewatch interesting parts, and then continue. You can also ask the user with some questions.
6.  Make your speech (`text` in `SPEAK` actions) lively and in character as defined.
7.  The final action in the `actions` array MUST be `{ "action_type": "END_REACTION" }` or `{ "action_type": "ASK_USER" }`.

**JSON SCHEMA for your output:**"""
        + f"""
```json
{json_schema}
```
"""
    )


if __name__ == "__main__":
    character_settings = "你啊哈"
    print(generate_reaction_script(character_settings))
</file>

<file path="src/ai_watch_buddy/connection_manager.py">
from fastapi import WebSocket

class ConnectionManager:
    """Manages active WebSocket connections."""

    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, session_id: str):
        await websocket.accept()
        self.active_connections[session_id] = websocket

    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]

    async def send_json(self, session_id: str, data: dict):
        if session_id in self.active_connections:
            await self.active_connections[session_id].send_json(data)

    async def broadcast(self, message: str):
        for connection in self.active_connections.values():
            await connection.send_text(message)


manager = ConnectionManager()
</file>

<file path="src/ai_watch_buddy/fetch_video.py">
import asyncio
from pathlib import Path
from loguru import logger
from yt_dlp import YoutubeDL
import argparse


def download_video(url: str, target_dir: str | Path) -> Path:
    """
    Download *url* into *target_dir* and return the final file path.
    Raises RuntimeError if the file was not produced.
    """

    logger.info(f"Downloading video from {url} to {target_dir}")

    target_dir = Path(target_dir).expanduser().resolve()
    target_dir.mkdir(parents=True, exist_ok=True)

    ydl_opts = {
        "format": "bestvideo+bestaudio/best",  # merge streams if needed
        "paths": {"home": str(target_dir)},  # save in target_dir
        "outtmpl": "%(title)s.%(ext)s",  # nicer names than default
        "quiet": True,  # no console spam
        "merge_output_format": "mp4",  # force mp4 output
    }

    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        final_path = target_dir / Path(ydl.prepare_filename(info)).name
        if not info:
            raise RuntimeError("Download failed: no info returned")

        if not final_path:
            raise RuntimeError(
                "Download failed: could not determine final filename from yt-dlp info"
            )

    if not final_path.exists():
        raise RuntimeError(
            f"Download appears to have failed. File not found: {final_path}"
        )

    return final_path


async def download_video_async(url: str, target_dir: str | Path) -> Path:
    return await asyncio.to_thread(download_video, url, target_dir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download a video using yt-dlp.")
    parser.add_argument("url", help="Video URL to download")
    parser.add_argument(
        "-d", "--dir", default="downloads", help="Target directory (default: downloads)"
    )
    args = parser.parse_args()

    try:
        path = asyncio.run(download_video_async(args.url, args.dir))
        print(f"Downloaded to: {path}")
    except Exception as e:
        print(f"Error: {e}")
</file>

<file path="src/ai_watch_buddy/session.py">
import asyncio
from typing import Literal, Optional
from .agent.video_action_agent_interface import VideoActionAgentInterface


class SessionState:
    """Holds the state for a single watching session."""

    def __init__(self, session_id: str, character_id: str, video_url: str):
        self.session_id = session_id
        self.character_id = character_id
        self.video_url = video_url
        self.local_video_path: str | None = None
        self.status: Literal[
            "created",
            "downloading_video",
            "video_ready",
            "generating_actions",
            "session_ready",
            "error",
        ] = "created"
        self.processing_error: str | None = None

        # 关键改动：为每个 session 实例创建一个 asyncio.Queue
        # 这个队列将作为生产者（pipeline）和消费者（websocket）之间的桥梁
        self.action_queue: asyncio.Queue = asyncio.Queue()
        self.agent: Optional[VideoActionAgentInterface] = None
        self.action_generation_task: Optional[asyncio.Task] = None


# A simple in-memory "database" for sessions
# This dictionary is now the single source of truth for all session states.
session_storage: dict[str, SessionState] = {}
</file>

<file path="src/ai_watch_buddy/tts.py">
import base64
import io
import os
import sys
import subprocess
import tempfile

import edge_tts
from loguru import logger

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


class TTSEngine:
    def __init__(self):
        pass

    async def generate_audio(
        self, text: str, voice: str = "zh-CN-XiaoxiaoNeural"
    ) -> str | None:
        """
        Generate speech audio and return as base64 string.
        text: str
            the text to speak

        Returns:
        str: base64 encoded WAV audio data, or None if generation fails.
        """
        try:
            # Edge-TTS generates MP3 by default, we need to convert to WAV
            communicate = edge_tts.Communicate(text, voice)
            
            # First, get the MP3 data
            mp3_buffer = io.BytesIO()
            async for chunk in communicate.stream():
                if chunk["type"] == "audio" and "data" in chunk:
                    mp3_buffer.write(chunk["data"])
            
            mp3_buffer.seek(0)
            mp3_data = mp3_buffer.read()
            
            # Use ffmpeg to convert MP3 to WAV
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as mp3_file:
                mp3_file.write(mp3_data)
                mp3_path = mp3_file.name
            
            wav_path = mp3_path.replace(".mp3", ".wav")
            
            try:
                # Convert MP3 to WAV using ffmpeg
                subprocess.run(
                    ["ffmpeg", "-i", mp3_path, "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2", wav_path],
                    check=True,
                    capture_output=True
                )
                
                # Read the WAV file and encode to base64
                with open(wav_path, "rb") as wav_file:
                    wav_data = wav_file.read()
                    base64_audio = base64.b64encode(wav_data).decode("utf-8")
                
                return base64_audio
            
            finally:
                # Clean up temporary files
                if os.path.exists(mp3_path):
                    os.unlink(mp3_path)
                if os.path.exists(wav_path):
                    os.unlink(wav_path)

        except Exception as e:
            logger.critical(f"\nError: Unable to generate or convert audio: {e}")
            logger.critical("It's possible that edge-tts is blocked in your region or ffmpeg is not installed.")
            return None


# en-US-AvaMultilingualNeural
# en-US-EmmaMultilingualNeural
# en-US-JennyNeural

tts_instance = TTSEngine()

if __name__ == "__main__":
    import asyncio

    text = "Hello, this is a test of the TTS engine."
    audio_base64 = asyncio.run(tts_instance.generate_audio(text))
    if audio_base64:
        print(
            f"Generated audio (base64): {audio_base64[:50]}..."
        )  # Print first 50 chars
        # save to file for testing
        with open("test_audio.txt", "wb") as f:
            f.write(audio_base64.encode("utf-8"))
    else:
        print("Failed to generate audio.")
</file>

<file path="src/ai_watch_buddy/action_generate.py">
import json
import asyncio
from collections.abc import AsyncGenerator
from json_repair import repair_json
from pydantic import ValidationError, TypeAdapter
from .ai_actions import Action, ActionScript

# ==============================
sample_json = """
    [
  {
    "id": "e0b02f90-8452-442c-a28a-77c8e8749c95",
    "trigger_timestamp": 0.5,
    "comment": "开幕雷击，先表达一下震惊，顺便吐槽一下这个离谱的标题。",
    "action_type": "SPEAK",
    "text": "啊？等一下，UCLA计算机硕士...在孟加拉上学？这是什么地狱开局啊喂！",
    "pause_video": true
  },
  {
    "id": "18f75c2e-4b48-4389-9e8c-529a9e3a62d0",
    "trigger_timestamp": 7,
    "comment": "经典恒河水，必须得吐槽一下，突出一个腹黑。",
    "action_type": "SPEAK",
    "text": "起床第一件事，先来一杯纯天然的恒河茶，这才是真正的大学牲啊！你看他喝完，眼神都清澈了许多呢（大概）。",
    "pause_video": true
  },
  {
    "id": "c138fd94-912f-4c12-9c3f-c80f082e6d6c",
    "trigger_timestamp": 14,
    "comment": "对冷水浇头和身材进行评论，带一点花痴的感觉，但还是以搞笑为主。",
    "action_type": "SPEAK",
    "text": "哇哦，冷水喷醒身体...顺便秀一下腹肌是吧？懂了，这是高材生的独特叫醒服务。",
    "pause_video": false
  },
  {
    "id": "a92e10c7-e547-4f81-80a9-197147b30c33",
    "trigger_timestamp": 21,
    "comment": "看到他吃东西的痛苦面具和被大姐强制喂食，忍不住笑出来，并进行腹黑吐槽。",
    "action_type": "PAUSE",
    "duration_seconds": 6
  },
  {
    "id": "d4c9d5d8-0f66-4e4f-b1e7-91f94d93026f",
    "trigger_timestamp": 22,
    "comment": "看到他吃东西的痛苦面具和被大姐强制喂食，忍不住笑出来，并进行腹黑吐槽。",
    "action_type": "SPEAK",
    "text": "哈哈哈哈，你看他那个表情，好像在说“这玩意儿吃了真的不会喷射吗？” 结果大姐直接上手了，挑食可不是好孩子哦~",
    "pause_video": true
  },
  {
    "id": "f5f5c3b9-a4e1-45d2-ac53-06639c05e197",
    "trigger_timestamp": 36,
    "comment": "对“地铁冲浪”这个离谱的导航结果进行吐槽，引出游戏梗。",
    "action_type": "SPEAK",
    "text": "等会儿？地铁冲浪？这AI是懂上学的，直接带你玩真人版Subway Surfers是吧！",
    "pause_video": true
  },
  {
    "id": "1e7e4f32-7c64-469b-9877-3e839e92b3a9",
    "trigger_timestamp": 43,
    "comment": "他滑倒的瞬间太搞笑了，必须得吐槽一下AI的马后炮行为。",
    "action_type": "REPLAY_SEGMENT",
    "start_timestamp": 41,
    "end_timestamp": 44,
    "post_replay_behavior": "STAY_PAUSED_AT_END"
  },
  {
    "id": "b3b19b22-8d77-4c07-955a-c635df08272f",
    "trigger_timestamp": 44,
    "comment": "他滑倒的瞬间太搞笑了，必须得吐槽一下AI的马后炮行为。",
    "action_type": "SPEAK",
    "text": "“小心滑倒”...噗！你咋不早说啊！这AI的延迟比我还高！",
    "pause_video": true
  },
  {
    "id": "8a7c2b0d-2e6f-4228-9711-20a23d9a334f",
    "trigger_timestamp": 58,
    "comment": "看到两车交汇的惊险场面，发出夸张的惊呼。",
    "action_type": "SPEAK",
    "text": "卧槽！卧槽！对面来车了！极限运动啊这是！太刺激了！",
    "pause_video": false
  },
  {
    "id": "4d3f56d0-61d0-4d57-b4d4-5309d9492169",
    "trigger_timestamp": 76,
    "comment": "看到他在车顶躺着写作业，吐槽这种学霸行为。",
    "action_type": "SPEAK",
    "text": "不是，哥们，你在火车顶上玩丛林飞跃，顺便写作业？这就是卷王的日常吗？",
    "pause_video": true
  },
  {
    "id": "2c2e0b1d-8452-4414-9989-d4c398328c11",
    "trigger_timestamp": 85,
    "comment": "看到路人吐槽“神庙逃亡”，觉得这个梗太妙了，必须暂停分享一下。",
    "action_type": "SPEAK",
    "text": "“你搁这玩神庙逃亡呢？” 哈哈哈哈，官方吐槽最为致命！太对了哥，就是这个味儿！",
    "pause_video": true
  },
  {
    "id": "a5d89e5a-7e3f-4e0e-af10-2f3b7d14e0f5",
    "trigger_timestamp": 94,
    "comment": "对车顶卖东西以及送包子的行为表示惊叹和搞笑评论。",
    "action_type": "SPEAK",
    "text": "火车顶上还有移动小卖部？服务也太周到了吧！大哥还直接送他了，孟加拉真是太有...人情味了！",
    "pause_video": true
  },
  {
    "id": "e6f47b22-1d59-4d57-8d0f-4e12c1d3c001",
    "trigger_timestamp": 122,
    "comment": "看到他用手机远程控制电脑交作业，以一种夸张的、仿佛看广告的语气来吐槽这个硬核操作。",
    "action_type": "REPLAY_SEGMENT",
    "start_timestamp": 118,
    "end_timestamp": 122,
    "post_replay_behavior": "STAY_PAUSED_AT_END"
  },
  {
    "id": "9b1e5a8f-2f88-4f1e-9a99-f2e7c3b2d18d",
    "trigger_timestamp": 122.5,
    "comment": "看到他用手机远程控制电脑交作业，以一种夸张的、仿佛看广告的语气来吐槽这个硬核操作。",
    "action_type": "SPEAK",
    "text": "我懂了！原来是广告！在命悬一线的时候，用手机远程交作业，这功能也太硬核了吧！只要思想不滑坡，办法总比困难多！",
    "pause_video": true
  },
  {
    "id": "f8a09b3c-6e7d-411a-8b1e-9a7c8d9e2b1f",
    "trigger_timestamp": 150,
    "comment": "看到他成功交完作业，发表最后的感慨，并与观众互动。",
    "action_type": "SPEAK",
    "text": "Mission Accomplished！任务完成！真是惊心动魄的上学路啊。呐，观众姥爷们，你们上学的时候有这么刺激吗？",
    "pause_video": true
  },
  {
    "id": "3a09e1d8-4f3b-4c2d-9e1a-8f7b6c5d4e3f",
    "trigger_timestamp": 158,
    "comment": "视频结束，发出最后的结束语。",
    "action_type": "END_REACTION"
  }
]"""
# ==============================


# 这是一个模拟 LLM 响应的函数，它会流式地返回我们那个 JSON 数组。
# 在真实场景中，你会用 httpx 去请求真实的 LLM API。
async def fake_llm_stream_response() -> AsyncGenerator[str, None]:
    """
    模拟 LLM API 的流式响应。
    为了方便测试，我们将完整的 JSON 分块返回。
    """

    # 模拟网络延迟和分块传输
    chunk_size = 50
    for i in range(0, len(sample_json), chunk_size):
        yield sample_json[i : i + chunk_size]
        await asyncio.sleep(0.01)


async def generate_actions(
    video_path: str, start_time: float, character_prompt: str
) -> AsyncGenerator[Action, None]:
    """
    调用 LLM 生成动作并以流式方式返回。

    这个异步生成器是核心处理管道：
    1.  从 LLM API (模拟的) 获取流式响应。
    2.  将所有文本块组装成一个完整的 JSON 数组字符串。
    3.  对 JSON 字符串进行修复（如果需要）和验证。
    4.  遍历数组，将验证通过的 Action 对象逐个 yield 出来。

    :param video_path: 视频文件路径 (当前未使用，但为未来保留)
    :param start_time: 视频开始时间 (当前未使用，但为未来保留)
    :param character_prompt: 角色提示 (当前未使用，但为未来保留)
    :return: 一个异步生成器，用于产出 Action 对象。
    """
    # 在真实应用中，你会用 video_path, start_time, character_prompt
    # 来构建请求并调用真实的 LLM API。
    llm_stream = fake_llm_stream_response()

    # 1. 收集所有数据块
    full_response = "".join([chunk async for chunk in llm_stream])

    # 2. 尝试解析整个 JSON 数组
    try:
        # 首先尝试直接解析
        actions_data = json.loads(full_response)
    except json.JSONDecodeError:
        print("⚠️警告: JSON 解析失败，尝试修复...")
        try:
            # 如果失败，使用 json_repair
            repaired_json_str = repair_json(full_response)
            actions_data = json.loads(repaired_json_str)
            print("✅ JSON 成功修复！")
        except Exception as e:
            print(f"❌ 错误: 修复后依然无法解析 JSON: {e}")
            return  # 无法继续，直接返回

    if not isinstance(actions_data, list):
        print(f"❌ 错误: 预期顶层结构是 JSON 数组，但得到的是 {type(actions_data)}")
        return

    # 3. 遍历数组，验证并 yield 每个 action
    for i, action_dict in enumerate(actions_data):
        try:
            # 对于 Union 类型，我们使用 TypeAdapter 来验证
            validated_action = TypeAdapter(Action).validate_python(action_dict)
            yield validated_action
        except ValidationError as e:
            print(
                f"❌ 错误: 第 {i+1} 个 Action 验证失败，已跳过。数据: {action_dict}, 错误: {e}"
            )


if __name__ == "__main__":

    async def main():
        # Example usage
        video_path = "example_video.mp4"
        start_time = 0.0
        character_prompt = "A humorous AI character reacting to a video."

        print("--- Streaming Actions ---")
        action_count = 0
        async for action in generate_actions(video_path, start_time, character_prompt):
            action_count += 1
            print(
                f"Action {action_count}: {action.model_dump_json(indent=2)}", flush=True
            )
            await asyncio.sleep(0.1)
        print(f"\n--- End of Stream ---")
        print(f"Total actions received: {action_count}")

    asyncio.run(main())
</file>

<file path="src/ai_watch_buddy/pipeline.py">
import asyncio
import json
from pathlib import Path
from loguru import logger
from typing import List

from .ai_actions import Action, SpeakAction
from .tts import tts_instance
from .session import session_storage
from .fetch_video import download_video_async

# TODO: You need to create and import your actual agent implementation.
# from .agent.video_action_agent_implementation import VideoActionAgentImpl
from .agent.video_action_agent_interface import VideoActionAgentInterface


async def generate_and_queue_actions(
    session_id: str, mode: str, clear_pending_actions: bool = True
) -> None:
    """
    Generic function to generate actions using the session's agent and put them in the queue.
    """
    session = session_storage.get(session_id)
    if not session or not session.agent:
        logger.error(
            f"[{session_id}] Cannot generate actions: session or agent not found."
        )
        return

    try:
        # 虽然一般不会用到（都发出去了），但我们还是 clear 一下
        if clear_pending_actions:
            while not session.action_queue.empty():
                session.action_queue.get_nowait()
            logger.info(f"[{session_id}] Cleared pending actions from the queue.")

        session.status = "generating_actions"
        logger.info(f"[{session_id}] Starting action generation with mode '{mode}'...")

        actions_generated_count = 0
        
        async for action_data in session.agent.generate(mode=mode):
            action = Action.model_validate(action_data)

            if isinstance(action, SpeakAction):
                audio_base64 = await tts_instance.generate_audio(action.text)
                if audio_base64:
                    action.audio = audio_base64
                else:
                    logger.warning(
                        f"[{session_id}] Failed to generate audio for action: {action.id}"
                    )

            await session.action_queue.put(action)
            actions_generated_count += 1
            logger.info(
                f"[{session_id}] Queued action: {action.action_type} at {action.trigger_timestamp}s"
            )

        logger.info(
            f"[{session_id}] Action generation stream finished. Total new actions: {actions_generated_count}."
        )

    except asyncio.CancelledError:
        logger.info(f"[{session_id}] Action generation task was cancelled.")
    except Exception as e:
        logger.error(
            f"[{session_id}] Error during action generation: {e}", exc_info=True
        )
        session.status = "error"
        session.processing_error = str(e)
        await session.action_queue.put(
            {
                "type": "processing_error",
                "error_code": "ACTION_GENERATION_FAILED",
                "message": str(e),
            }
        )
    finally:
        await session.action_queue.put(None)
        # ✅ NEW: Set status to ready only after the generation loop completes successfully.
        # We check if it's already ready to prevent this from running on subsequent calls (e.g., lazy loading).
        if session.status != "error" and session.status != "session_ready":
            session.status = "session_ready"
            logger.info(
                f"[{session_id}] ✅ Initial actions generated. Status set to 'session_ready'."
            )


async def run_conversation_pipeline(
    session_id: str, user_action_list: List[dict], pending_action_list: List[dict]
) -> None:
    session = session_storage.get(session_id)
    if not session or not session.agent:
        logger.error(
            f"[{session_id}] Cannot run conversation: session or agent not found."
        )
        return

    # 一般而言，user_speech_text 只有一个
    user_speech = [
        action["text"]
        for action in user_action_list
        if action["action_type"] == "SPEAK" and "text" in action
    ]
    user_speech_text = " ".join(user_speech)
    
    context_message = f"""
## CONVERSATION UPDATE
You were interrupted by the user. Here's what happened:
1.  **Your Pending Actions (What you were about to do):**
    ```json
    {json.dumps(pending_action_list, indent=2)}
    ```
2.  **User's Actions (What the user did to interrupt you):**
    ```json
    {json.dumps(user_action_list, indent=2)}
    ```
3.  **User's Speech:**
    "{user_speech_text if user_speech_text else 'The user did not speak, but took other actions.'}"

**Your Task:**
Based on this new information, discard your pending actions and generate a NEW, relevant "Reaction Script".
"""
    session.agent.add_content(role="user", text=context_message)
    session.action_generation_task = asyncio.create_task(
        generate_and_queue_actions(
            session_id, mode="summary", clear_pending_actions=True
        )
    )
    logger.info(
        f"[{session_id}] Started conversation update. New generation task created."
    )


async def run_initial_generation(session_id: str, summary_task: asyncio.Task):
    """
    A wrapper that waits for the summary to be ready, then triggers the first action generation.
    """
    session = session_storage.get(session_id)
    if not session or not session.agent:
        return

    try:
        logger.info(f"[{session_id}] Waiting for video summary to be generated...")
        await summary_task
        logger.info(f"[{session_id}] Summary task finished.")

        if not session.agent.summary_ready:
            raise RuntimeError(
                "Agent summary was not ready after summary task completion."
            )

        # Now that the summary is ready, start the first action generation.
        await generate_and_queue_actions(
            session_id, mode="video", clear_pending_actions=False
        )

    except Exception as e:
        logger.error(
            f"[{session_id}] Error during initial generation wrapper: {e}",
            exc_info=True,
        )
        session.status = "error"
        session.processing_error = str(e)
        await session.action_queue.put(
            {
                "type": "processing_error",
                "error_code": "INITIAL_GENERATION_FAILED",
                "message": str(e),
            }
        )
        await session.action_queue.put(None)


async def initial_pipeline(session_id: str) -> None:
    """
    The initial background task that runs when a session is created.
    """
    session = session_storage.get(session_id)
    if not session:
        return

    try:
        # Step 1: Download video (blocking within this pipeline)
        session.status = "downloading_video"
        local_video_path = str(
            await download_video_async(session.video_url, target_dir="video_cache")
        )
        session.local_video_path = local_video_path
        session.status = "video_ready"
        logger.info(f"[{session_id}] Video ready at: {local_video_path}")

        # Step 2: Initialize the agent
        # TODO: Replace with your actual implementation.
        # agent = VideoActionAgentImpl(...)
        # session.agent = agent
        logger.info(f"[{session_id}] Agent initialized.")

        # ✅ Step 3: Start summary generation in the background (non-blocking).
        summary_task = asyncio.create_task(
            session.agent.get_video_summary(video_path_or_url=local_video_path)
        )

        # ✅ Step 4: Start the wrapper task that waits for the summary and then generates actions.
        # This whole chain runs in the background.
        session.action_generation_task = asyncio.create_task(
            run_initial_generation(session_id, summary_task)
        )
        logger.info(
            f"[{session_id}] Initial pipeline complete. Summary and action generation started in the background."
        )

    except Exception as e:
        logger.error(
            f"[{session_id}] Error during initial pipeline setup: {e}", exc_info=True
        )
        session.status = "error"
        session.processing_error = str(e)
        if session:
            await session.action_queue.put(
                {
                    "type": "processing_error",
                    "error_code": "INITIAL_PIPELINE_FAILED",
                    "message": str(e),
                }
            )
            await session.action_queue.put(None)
</file>

<file path="src/ai_watch_buddy/ai_actions.py">
import json
from typing import Literal
import numpy as np
from pydantic import BaseModel, Field, RootModel


# 這是一個基礎模型，定義了所有 Action 的共性
class BaseAction(BaseModel):
    """
    所有具體反應動作的基礎模型，定義了每個動作都必須包含的通用屬性。
    """

    model_config = {"arbitrary_types_allowed": True}

    # 每個 Action 都應該有一個獨一無二的 ID，方便追蹤和日誌記錄
    id: str = Field(
        ..., description="一個唯一的動作 ID，建議使用 UUID 生成，用於追蹤和調試。"
    )
    # 這個 Action 在影片的哪個時間點被觸發？這是反應的錨點。
    trigger_timestamp: float = Field(
        ...,
        description="此動作在影片中的觸發時間點 (單位: 秒)，代表 AI 在看到這一秒的內容時決定做出反應。",
    )
    # 一個給開發者看的備註，解釋為什麼 AI 會做這個反應。LLM 也會填寫它。
    comment: str = Field(
        ...,
        description="AI 做出此反應的內心想法或理由的簡要文字描述，主要用於調試或分析 AI 的決策過程。",
    )


# --- 開始定義具體的 Action 類型 ---


# 1. 說話 (Speak)
class SpeakAction(BaseAction):
    """
    讓 AI 角色說出指定的文本。這是最核心的互動方式。
    """

    action_type: Literal["SPEAK"] = "SPEAK"
    text: str = Field(..., description="AI 角色要說出的具體內容。")
    audio: str | None = Field(
        None,
        description="由 TTS (Text-to-Speech) 服務生成的音頻數據的標識符或路徑。此欄位由後端系統填充，LLM 無需填寫。",
    )
    # 這個布林值非常關鍵，它決定了是「畫外音」還是「暫停解說」
    pause_video: bool = Field(
        default=True,
        description="決定說話時是否需要暫停影片。True 表示暫停影片進行解說，常用於較長的評論或需要用戶專注於 AI 的發言時。False 表示在影片繼續播放的同時發表評論（畫外音），適用於簡短、即時的吐槽或感想，能讓互動更流暢。",
    )


# 2. 暫停 (Pause) - 用於模擬思考、驚訝等無言的反應
class PauseAction(BaseAction):
    """
    讓 AI 進行一次無言的暫停。可以用來模擬思考、驚訝，或在兩個動作之間創造節奏感，讓反應更自然。暂停会停止视频时间的变化直到 duration_seconds 结束，如果需要暂停后说话，请使用 SpeakAction (pause_video=True) 而无需使用 PauseAction。
    """

    action_type: Literal["PAUSE"] = "PAUSE"
    # 暫停多久？這給予了精確的節奏控制
    duration_seconds: float = Field(..., description="需要暫停的持續時間 (單位: 秒)。")


# 3. 影片控制 (Video Control)
class SeekAction(BaseAction):
    """
    控制影片的播放進度，讓 AI 可以跳轉到影片的某個特定時間點，通常是為了回顧或預告某個細節。SeekAction 跳转后会继承视频跳转前的播放状态 (暂停 / 播放)
    """

    action_type: Literal["SEEK"] = "SEEK"
    target_timestamp: float = Field(
        ..., description="要跳轉到的目標影片時間點 (單位: 秒)。"
    )
    # 跳轉後做什麼？這個很重要！
    # 'RESUME_PLAYBACK': 跳轉後繼續播放
    # 'STAY_PAUSED': 跳停在那個畫面，等待下一個指令
    post_seek_behavior: Literal["RESUME_PLAYBACK", "STAY_PAUSED"] = Field(
        "STAY_PAUSED",
        description="指定跳轉到目標時間點後的行為。'STAY_PAUSED' 表示停在該畫面，'RESUME_PLAYBACK' 表示立即開始播放。",
    )


# 4. 重看片段 (Replay Segment) - 這是一個複合動作，但我們將其原子化，方便 LLM 生成
class ReplaySegmentAction(BaseAction):
    """
    讓 AI 重播影片的某一個片段。常用於對精彩、有趣或關鍵的細節進行強調、分析和評論。
    """

    action_type: Literal["REPLAY_SEGMENT"] = "REPLAY_SEGMENT"
    start_timestamp: float = Field(
        ..., description="需要重播片段的開始時間點 (單位: 秒)。"
    )
    end_timestamp: float = Field(
        ..., description="需要重播片段的結束時間點 (單位: 秒)。"
    )
    # 重看完之後的行為，是回到原來的地方，還是停在片段結尾？
    # 'RESUME_FROM_ORIGINAL': 回到觸發此動作的時間點繼續播放
    # 'STAY_PAUSED_AT_END': 停在 end_timestamp 處
    post_replay_behavior: Literal["RESUME_FROM_ORIGINAL", "STAY_PAUSED_AT_END"] = Field(
        "RESUME_FROM_ORIGINAL",
        description="定義了重播結束後的行為。'RESUME_FROM_ORIGINAL' 表示播放頭將跳回到觸發此重播動作的原始時間點並繼續播放，'STAY_PAUSED_AT_END' 表示播放將停在重播片段的結尾處。",
    )


# 5. 結束反應 (End Reaction) - 用於控制反應流程
class EndReaction(BaseAction):
    """
    標記一組反應動作的結束。這是一個流程控制指令，主要有兩個用途：
    1. 當 AI 向用戶提問後 (通常是一個 SpeakAction)，應立刻跟隨一個 EndReaction。這會暫停 AI 的後續動作，將控制權交還給用戶，等待用戶的回應或下一個指令。
    2. 對於長影片，可以將一連串的反應拆分成多個由 EndReaction 分隔的區塊。這能避免一次性生成過多的 Action。如果用戶中途打断，就不會浪費已經生成但未被執行的反應，同時也讓系統能更靈活地處理用戶互動。
    """

    action_type: Literal["END_REACTION"] = "END_REACTION"


# --- 使用 Discriminated Union 組合所有 Action ---

# 這一步是 Pydantic V2 的精華所在
# 我們告訴 Pydantic，所有 Action 的聯集由 'action_type' 這個欄位來區分
# 這使得解析 JSON 數據時可以根據 'action_type' 的值自動匹配到對應的 Action 模型
Action = SpeakAction | PauseAction | SeekAction | ReplaySegmentAction | EndReaction


# 最後，我們的 Action Script 就是一個 Action 的列表
# 使用 RootModel 可以讓 Pydantic 直接驗證一個列表的根類型，確保整個腳本的結構正確
class ActionScript(RootModel[list[Action]]):
    """
    定義了 AI 反應腳本的最終結構，它是一個包含多個具體 Action 的有序列表。
    """

    pass


if __name__ == "__main__":
    # 這段代碼會將上面定義的 Pydantic 模型轉換成 JSON Schema 文件
    # 這個 Schema 文件可以被其他應用程式或 LLM 用來理解和生成符合格式的數據
    with open("schema.json", "w", encoding="utf-8") as f:
        # 使用 model_json_schema() 方法生成 JSON Schema
        # ensure_ascii=False 確保中文字符能正確顯示
        # indent=2 讓輸出的 JSON 文件格式化，方便閱讀
        json.dump(ActionScript.model_json_schema(), f, ensure_ascii=False, indent=2)
    print("Schema saved to schema.json")
</file>

<file path="src/ai_watch_buddy/server.py">
import asyncio
import uuid
from loguru import logger

from fastapi import (
    FastAPI,
    WebSocket,
    BackgroundTasks,
    HTTPException,
    status,
    WebSocketDisconnect,
)
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from .session import SessionState, session_storage
from .pipeline import initial_pipeline
from .ai_actions import Action
from .connection_manager import manager

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- Data Models for API ---
class SessionCreateRequest(BaseModel):
    video_url: str
    start_time: float = 0.0
    end_time: float | None = None
    text: str | None = None
    character_id: str
    user_id: str | None = None


class SessionCreateResponse(BaseModel):
    session_id: str


class ErrorResponse(BaseModel):
    error: str
    message: str


# --- Connection Management ---
# The ConnectionManager is now in its own file (connection_manager.py)
# to prevent circular dependencies. The `manager` instance is imported from there.


# --- API Endpoint ---
@app.post(
    "/api/v1/sessions",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=SessionCreateResponse,
)
async def create_session(
    request: SessionCreateRequest, background_tasks: BackgroundTasks
):
    """
    Creates a new watching session, starts background processing,
    and returns a session_id.
    """
    session_id = f"ses_{uuid.uuid4().hex[:16]}"

    # Create the session state object and store it
    session = SessionState(
        session_id=session_id,
        character_id=request.character_id,
        video_url=request.video_url,
    )
    session_storage[session_id] = session

    # Start the processing pipeline in the background
    background_tasks.add_task(initial_pipeline, session_id=session_id)

    logger.info(f"Accepted session {session_id} for video {request.video_url}")
    return SessionCreateResponse(session_id=session_id)


# --- WebSocket Endpoint (Major Refactor) ---


async def websocket_sender(websocket: WebSocket, session: SessionState):
    """
    Consumer coroutine: Gets actions from the queue and sends them to the client.
    This task now runs indefinitely until cancelled.
    """
    # First, wait until the session is ready or an error occurs during setup
    while session.status not in ["session_ready", "error"]:
        await asyncio.sleep(0.1)  # Small sleep to prevent a tight loop

    if session.status == "error":
        await websocket.send_json(
            {
                "type": "processing_error",
                "error_code": "INITIAL_PIPELINE_FAILED",
                "message": session.processing_error or "Unknown error during setup",
            }
        )
        return

    await websocket.send_json({"type": "session_ready"})
    logger.info(f"[{session.session_id}] Sent 'session_ready' to client.")

    # Main loop to process actions from the queue
    while True:
        item = await session.action_queue.get()

        # **MODIFICATION**: Instead of breaking, just log and continue waiting.
        # This keeps the sender alive for future actions.
        if item is None:
            logger.info(
                f"[{session.session_id}] Initial action generation complete. Sender is now idle, awaiting further actions."
            )
            session.action_queue.task_done()
            continue

        # Send the message based on the item type
        if isinstance(item, Action):
            await websocket.send_json(
                {"type": "ai_action", "action": item.model_dump(mode="json")}
            )
        elif isinstance(item, dict) and item.get("type") == "processing_error":
            await websocket.send_json(item)

        session.action_queue.task_done()


async def websocket_receiver(websocket: WebSocket, session: SessionState):
    """
    Receiver coroutine: Listens for messages from the client.
    This runs until the client disconnects, which raises WebSocketDisconnect.
    """
    async for message in websocket.iter_json():
        msg_type = message.get("type")
        logger.info(
            f"[{session.session_id}] Received message from client: type={msg_type}"
        )

        if msg_type == "trigger-load-next":
            # Here you can trigger new tasks that might put more actions into the queue
            logger.info(f"[{session.session_id}] Client triggered a future action.")

            # Here we need add a task
            # We need a mechanism to cancel the task when interrupted
            # Use
            

        if msg_type == "interrupt":

            pass

        if msg_type == "trigger-conversation":
            # The content
            # {
            #     "type": "trigger-conversation",
            #     "user_action_list": List[Action],
            #     "pending_action_list": List[Action],
            # }
            pass


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """
    Main WebSocket endpoint that manages the sender and receiver tasks' lifecycles.
    """
    session = session_storage.get(session_id)
    if not session:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        logger.warning(
            f"WebSocket connection rejected for unknown session: {session_id}"
        )
        return

    await manager.connect(websocket, session_id)
    logger.info(f"[{session_id}] WebSocket connection established.")

    sender_task = asyncio.create_task(websocket_sender(websocket, session))
    receiver_task = asyncio.create_task(websocket_receiver(websocket, session))

    try:
        # **MODIFICATION**: Use asyncio.gather to run tasks concurrently.
        # It will return only when one of the tasks raises an unhandled exception.
        await asyncio.gather(sender_task, receiver_task)
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] Client disconnected.")
    except Exception as e:
        logger.error(
            f"[{session_id}] An error occurred in the websocket endpoint: {e}",
            exc_info=True,
        )
    finally:
        # Cleanly cancel both tasks and disconnect the manager.
        sender_task.cancel()
        receiver_task.cancel()
        manager.disconnect(session_id)
        logger.info(f"[{session_id}] WebSocket connection closed and cleaned up.")
</file>

</files>
