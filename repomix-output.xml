This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: frontend
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  instructions/
    adx.instructions.md
docs/
  api.md
live2d-models/
  elaina/
    cc_LSS.cfg
    cc_names_LSS.cfg
    f00.exp3.json
    f01.exp3.json
    f02.exp3.json
    f03.exp3.json
    f04.exp3.json
    f05.exp3.json
    f06.exp3.json
    f07.exp3.json
    f08.exp3.json
    f09.exp3.json
    f10.exp3.json
    f11.exp3.json
    f12.exp3.json
    f13.exp3.json
    LSS.cdi3.json
    LSS.model3.json
    LSS.physics3.json
src/
  ai_watch_buddy/
    agent/
      gemini_sample.py
      text_stream_to_action.py
      video_action_agent_interface.py
      video_analyzer_agent.py
    prompts/
      action_gen_prompt.py
      character_prompts.py
    tts/
      edge_tts.py
      fish_audio_tts.py
      tts_interface.py
    actions.py
    connection_manager.py
    fetch_video.py
    pipeline.py
    server.py
    session.py
    test_tts_integration.py
    tts_generator.py
.env.sample
.gitignore
.python-version
action_list_result.json
main.py
pyproject.toml
schema.json
test_analyzer.py
test_video_analyzer_basic.py
test_video_analyzer_functionality.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/instructions/adx.instructions.md">
---
applyTo: '**'
---
这个项目是一个黑客松原型，使用 Python 3.13, uv, fastapi, pydantic v2，目标是实现一个能与用户一同看视频的 AI 语音陪伴，这将使用 websocket 做前后端连接。项目的重点是快速开发和代码简介。注意代码可读性和最佳实践。
</file>

<file path="docs/api.md">
# AI Watch Buddy API Documentation

This document outlines the API endpoints and WebSocket communication protocol for AI Watch Buddy.

---

## 1. REST API

The REST API is used to initiate a viewing session.

### Create Session

Creates a new watching session, starts the background processing for generating AI actions, and returns a `session_id` to be used for the WebSocket connection.

- **URL**: `/api/v1/sessions`
- **Method**: `POST`
- **Status Code**: `202 Accepted`

#### Request Body

```json
{
  "video_url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
  "start_time": 0,
  "end_time": null,
  "text": "你是个可爱的猫娘，你说的每句话都会以 “喵～～” 结尾",
  "character_id": "miao",
  "user_id": "user_123"
}
```

- `video_url` (string, **required**): The URL of the video to watch.
- `start_time` (float, optional): The start time in seconds. Defaults to `0.0`.
- `end_time` (float, optional): The end time in seconds. Defaults to `null` (end of video).
- `text` (string, optional): Additional text prompt from the user.
- `character_id` (string, **required**): The identifier for the desired AI character.
- `user_id` (string, optional): The identifier for the user.

#### Success Response (202 Accepted)

```json
{
  "session_id": "ses_a8d3f8b9c1e04a5f"
}
```

- `session_id` (string): A unique identifier for the session. Use this ID to connect to the WebSocket endpoint.

#### Error Responses

- **`422 Unprocessable Entity`**: Sent if the request data is well-formed but semantically incorrect.

  Example:
  ```json
  {
    "detail": {
      "error": "UNSUPPORTED_VIDEO_SOURCE",
      "message": "The provided video URL from 'vimeo.com' is not supported."
    }
  }
  ```

---

## 2. WebSocket API

The WebSocket API is used for real-time communication during the viewing session.

- **URL**: `/ws/{session_id}`
- **Example URL**: `ws://127.0.0.1:8000/ws/ses_a8d3f8b9c1e04a5f`

### Connection

The client should attempt to connect to this endpoint after successfully creating a session via the REST API. If the `session_id` is invalid or not found, the server will close the connection.

### Communication Flow

1.  **Client Connects**: The client establishes a WebSocket connection using the `session_id`.
2.  **Server Acknowledges**: The server waits for the background video processing to complete.
3.  **Server Notifies Ready**: Once processing is done, the server sends a `session_ready` message. If processing fails, it sends a `processing_error` message.
4.  **Real-time Interaction**:
    - The client periodically sends `timestamp_update` messages with the current video playback time.
    - The server listens for these updates and sends AI actions (`SPEAK`, `PAUSE_VIDEO`, etc.) when their `trigger_timestamp` is reached.
    - The client executes the received actions.
    - The client can notify the server about `action_completed` or `seek_update` events.

### Server-to-Client Messages

#### Session Ready

Indicates that the AI action script has been successfully generated and the server is ready to send actions.

```json
{
  "type": "session_ready"
}
```

#### Processing Error

Indicates that an error occurred while processing the video or generating actions.

```json
{
  "type": "processing_error",
  "error_code": "ACTION_GENERATION_FAILED",
  "message": "Failed to generate actions for the video: <details>"
}
```

#### AI Action

An action for the client to execute. The model for this is defined in `ai_actions.py`.

```json
{
  "id": "e0b02f90-8452-442c-a28a-77c8e8749c95",
  "trigger_timestamp": 0.5,
  "comment": "A comment explaining the action's purpose.",
  "action_type": "SPEAK",
  "text": "Hey, what is this video about?",
  "pause_video": true
}
```

### Client-to-Server Messages

#### Timestamp Update

Sent by the client to inform the server of the current video playback time. This is the primary message used to trigger AI actions.

```json
{
  "type": "timestamp_update",
  "timestamp": 123.45
}
```

#### Seek Update

Sent when the user manually changes the video's playback position (scrubbing). The server uses this to reset its internal state and determine the correct next action to send.

```json
{
  "type": "seek_update",
  "timestamp": 240.1
}
```

#### Action Completed

Sent by the client to acknowledge that it has finished executing a specific action.

```json
{
  "type": "action_completed",
  "action_id": "e0b02f90-8452-442c-a28a-77c8e8749c95"
}
```
</file>

<file path="live2d-models/elaina/cc_LSS.cfg">
set_special_pose_param LSS 'Param' 0 1 1
set_special_pose_param LSS 'Param4' 0 1 2
set_special_pose_param LSS 'Param5' 0 1 3
set_special_pose_param LSS 'Param6' 0 1 4
set_special_pose_param LSS 'Param2' 0 1 5
set_special_pose_param LSS 'Param3' 0 1 6
set_special_pose_param LSS 'Param7' 0 1 7
</file>

<file path="live2d-models/elaina/cc_names_LSS.cfg">
set_friendly_name LSS 'Live2D LSS'
set_avatar_skin_description LSS default '伊蕾娜！好耶~ https://m.bilibili.com/video/BV1KU4y1x7ep  https://qm.qq.com/cgi-bin/qm/qr?k=zO2OMGsVYbcyqQZvWoNS5HJ8cUPjc-Lh&jump_from=webapi  QQ群894471342  http://5769tx.coding-pages.com/
</file>

<file path="live2d-models/elaina/f00.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "Param6",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param13",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param12",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param10",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param11",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param2",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param5",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param3",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param7",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param9",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "Param8",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamMouthForm",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBodyAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBodyAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBodyAngleY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBreath",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeRSmile",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLSmile",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRAngle",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLAngle",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamHairFront",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamHairSide",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamHairBack",
			"Value": 0,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f01.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamEyeLOpen",
			"Value": -0.308,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": -0.341,
			"Blend": "Add"
		},
		{
			"Id": "ParamMouthForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": -0.846,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": -0.78,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 0.077,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 0.385,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 14.494,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -1.011,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f02.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": -1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f03.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "Param5",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": -1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f04.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLAngle",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRAngle",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param8",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "Param9",
			"Value": -1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f05.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeRSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": 0,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f06.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param5",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 5,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f07.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 5,
			"Blend": "Add"
		},
		{
			"Id": "Param3",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param7",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f08.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamEyeLOpen",
			"Value": 0.2,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 0.2,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -15,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 6.923,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeRSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f09.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": -0.5,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -15,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": -15,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 10,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "Param4",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f10.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "Param2",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param7",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamMouthForm",
			"Value": -0.69,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f11.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param5",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": 0.7,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": 0.7,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": -10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": -10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 5,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f12.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "Param",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": -0.5,
			"Blend": "Multiply"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": -0.5,
			"Blend": "Multiply"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": 0,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleX",
			"Value": -15,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleY",
			"Value": 10,
			"Blend": "Add"
		},
		{
			"Id": "ParamAngleZ",
			"Value": 5,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": -1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/f13.exp3.json">
{
	"Type": "Live2D Expression",
	"Parameters": [
		{
			"Id": "ParamMouthForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLOpen",
			"Value": -0.231,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeROpen",
			"Value": -0.198,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallX",
			"Value": -0.143,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeBallY",
			"Value": -1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowLForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRY",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamBrowRForm",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeLSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamEyeRSmile",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "ParamCheek",
			"Value": 1,
			"Blend": "Add"
		},
		{
			"Id": "Param6",
			"Value": 1,
			"Blend": "Add"
		}
	]
}
</file>

<file path="live2d-models/elaina/LSS.cdi3.json">
{
	"Version": 3,
	"Parameters": [
		{
			"Id": "Param13",
			"GroupId": "ParamGroup2",
			"Name": "参数84"
		},
		{
			"Id": "Param12",
			"GroupId": "ParamGroup2",
			"Name": "参数78"
		},
		{
			"Id": "Param10",
			"GroupId": "ParamGroup2",
			"Name": "顶发"
		},
		{
			"Id": "Param11",
			"GroupId": "ParamGroup2",
			"Name": "顶发2"
		},
		{
			"Id": "Param",
			"GroupId": "ParamGroup",
			"Name": "眼镜"
		},
		{
			"Id": "Param2",
			"GroupId": "ParamGroup",
			"Name": "啊这"
		},
		{
			"Id": "Param5",
			"GroupId": "ParamGroup",
			"Name": "汉"
		},
		{
			"Id": "Param3",
			"GroupId": "ParamGroup",
			"Name": "好耶"
		},
		{
			"Id": "Param7",
			"GroupId": "ParamGroup",
			"Name": "眼睛"
		},
		{
			"Id": "Param4",
			"GroupId": "ParamGroup",
			"Name": "红"
		},
		{
			"Id": "Param6",
			"GroupId": "ParamGroup",
			"Name": "手"
		},
		{
			"Id": "ParamAngleX",
			"GroupId": "",
			"Name": "角度 X"
		},
		{
			"Id": "ParamAngleY",
			"GroupId": "",
			"Name": "角度 Y"
		},
		{
			"Id": "ParamAngleZ",
			"GroupId": "",
			"Name": "角度 Z"
		},
		{
			"Id": "ParamEyeLOpen",
			"GroupId": "",
			"Name": "左眼　开闭"
		},
		{
			"Id": "ParamEyeROpen",
			"GroupId": "",
			"Name": "右眼"
		},
		{
			"Id": "ParamEyeBallX",
			"GroupId": "",
			"Name": "眼球 X"
		},
		{
			"Id": "ParamEyeBallY",
			"GroupId": "",
			"Name": "眼球 Y"
		},
		{
			"Id": "Param8",
			"GroupId": "",
			"Name": "高光1"
		},
		{
			"Id": "Param9",
			"GroupId": "",
			"Name": "高光2"
		},
		{
			"Id": "ParamBrowLY",
			"GroupId": "",
			"Name": "左眉上下"
		},
		{
			"Id": "ParamBrowLForm",
			"GroupId": "",
			"Name": "左眉　変形"
		},
		{
			"Id": "ParamBrowRY",
			"GroupId": "",
			"Name": "右眉　上下"
		},
		{
			"Id": "ParamBrowRForm",
			"GroupId": "",
			"Name": "右眉　変形"
		},
		{
			"Id": "ParamMouthForm",
			"GroupId": "",
			"Name": "嘴部　变形"
		},
		{
			"Id": "ParamMouthOpenY",
			"GroupId": "",
			"Name": "嘴巴　张开和闭合"
		},
		{
			"Id": "ParamBodyAngleX",
			"GroupId": "",
			"Name": "身体旋转　X"
		},
		{
			"Id": "ParamBodyAngleY",
			"GroupId": "",
			"Name": "身体旋转　Y"
		},
		{
			"Id": "ParamBodyAngleZ",
			"GroupId": "",
			"Name": "身体旋转　Z"
		},
		{
			"Id": "ParamBreath",
			"GroupId": "",
			"Name": "呼吸"
		},
		{
			"Id": "ParamEyeRSmile",
			"GroupId": "",
			"Name": "右眼　微笑"
		},
		{
			"Id": "ParamEyeLSmile",
			"GroupId": "",
			"Name": "左眼　微笑"
		},
		{
			"Id": "ParamBrowLX",
			"GroupId": "",
			"Name": "左眉　左右"
		},
		{
			"Id": "ParamBrowRX",
			"GroupId": "",
			"Name": "右眉　左右"
		},
		{
			"Id": "ParamBrowLAngle",
			"GroupId": "",
			"Name": "左眉　角度"
		},
		{
			"Id": "ParamBrowRAngle",
			"GroupId": "",
			"Name": "右眉　角度"
		},
		{
			"Id": "ParamCheek",
			"GroupId": "",
			"Name": "脸颊"
		},
		{
			"Id": "ParamHairFront",
			"GroupId": "",
			"Name": "摇动　前发"
		},
		{
			"Id": "ParamHairSide",
			"GroupId": "",
			"Name": "摇动　侧发"
		},
		{
			"Id": "ParamHairBack",
			"GroupId": "",
			"Name": "摇动　后发"
		},
		{
			"Id": "Param_Angle_Rotation_1_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[0]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_2_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[1]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_3_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[2]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_4_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[3]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_5_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[4]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_6_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[5]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_7_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[6]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_8_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[7]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_9_ArtMesh8",
			"GroupId": "ParamGroup3",
			"Name": "[8]ﾗ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_1_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[0]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_2_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[1]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_3_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[2]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_4_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[3]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_5_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[4]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_6_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[5]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_7_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[6]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_8_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[7]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_9_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[8]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_10_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[9]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_11_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[10]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_12_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[11]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_13_ArtMesh9",
			"GroupId": "ParamGroup4",
			"Name": "[12]ﾓﾒｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_1_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[0]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_2_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[1]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_3_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[2]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_4_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[3]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_5_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[4]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_6_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[5]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_7_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[6]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_8_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[7]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_9_ArtMesh55",
			"GroupId": "ParamGroup5",
			"Name": "[8]ｺ�ｷ｢1"
		},
		{
			"Id": "Param_Angle_Rotation_1_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[0]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_2_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[1]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_3_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[2]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_4_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[3]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_5_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[4]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_6_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[5]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_7_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[6]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_8_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[7]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_9_ArtMesh56",
			"GroupId": "ParamGroup6",
			"Name": "[8]ｺ�ｷ｢2"
		},
		{
			"Id": "Param_Angle_Rotation_1_ArtMesh53",
			"GroupId": "ParamGroup8",
			"Name": "[0]ｺ�ｵ�ｽ�"
		},
		{
			"Id": "Param_Angle_Rotation_2_ArtMesh53",
			"GroupId": "ParamGroup8",
			"Name": "[1]ｺ�ｵ�ｽ�"
		},
		{
			"Id": "Param_Angle_Rotation_3_ArtMesh53",
			"GroupId": "ParamGroup8",
			"Name": "[2]ｺ�ｵ�ｽ�"
		},
		{
			"Id": "Param_Angle_Rotation_4_ArtMesh53",
			"GroupId": "ParamGroup8",
			"Name": "[3]ｺ�ｵ�ｽ�"
		},
		{
			"Id": "Param_Angle_Rotation_5_ArtMesh53",
			"GroupId": "ParamGroup8",
			"Name": "[4]ｺ�ｵ�ｽ�"
		}
	],
	"ParameterGroups": [
		{
			"Id": "ParamGroup2",
			"GroupId": "",
			"Name": "物理"
		},
		{
			"Id": "ParamGroup",
			"GroupId": "",
			"Name": "表情"
		},
		{
			"Id": "ParamGroup3",
			"GroupId": "",
			"Name": "ﾗ�ｷ｢1"
		},
		{
			"Id": "ParamGroup4",
			"GroupId": "",
			"Name": "ﾓﾒｷ｢1"
		},
		{
			"Id": "ParamGroup5",
			"GroupId": "",
			"Name": "ｺ�ｷ｢1"
		},
		{
			"Id": "ParamGroup6",
			"GroupId": "",
			"Name": "ｺ�ｷ｢2"
		},
		{
			"Id": "ParamGroup8",
			"GroupId": "",
			"Name": "ｺ�ｵ�ｽ�"
		}
	],
	"Parts": [
		{
			"Id": "Part",
			"Name": "ﾊﾖ"
		},
		{
			"Id": "Part2",
			"Name": "ﾉﾏﾍｷ"
		},
		{
			"Id": "Part3",
			"Name": "ﾗ�ﾑﾛ"
		},
		{
			"Id": "Part4",
			"Name": "ﾓﾒﾑﾛ"
		},
		{
			"Id": "Part5",
			"Name": "ﾍｷ"
		},
		{
			"Id": "Part6",
			"Name": "ﾉ�"
		},
		{
			"Id": "Part7",
			"Name": "ｷ｢"
		},
		{
			"Id": "ArtMesh8_Skinning",
			"Name": "ﾗ�ｷ｢1(蒙皮)"
		},
		{
			"Id": "Part8",
			"Name": "ﾗ�ｷ｢1(回转)"
		},
		{
			"Id": "ArtMesh9_Skinning",
			"Name": "ﾓﾒｷ｢1(蒙皮)"
		},
		{
			"Id": "Part9",
			"Name": "ﾓﾒｷ｢1(回转)"
		},
		{
			"Id": "ArtMesh53_Skinning",
			"Name": "ｺ�ｵ�ｽ�(蒙皮)"
		},
		{
			"Id": "Part12",
			"Name": "ｺ�ｵ�ｽ�(回转)"
		},
		{
			"Id": "ArtMesh55_Skinning",
			"Name": "ｺ�ｷ｢1(蒙皮)"
		},
		{
			"Id": "Part10",
			"Name": "ｺ�ｷ｢1(回转)"
		},
		{
			"Id": "ArtMesh56_Skinning",
			"Name": "ｺ�ｷ｢2(蒙皮)"
		},
		{
			"Id": "Part11",
			"Name": "ｺ�ｷ｢2(回转)"
		}
	]
}
</file>

<file path="live2d-models/elaina/LSS.model3.json">
{
	"Version": 3,
	"FileReferences": {
		"Moc": "LSS.moc3",
		"Textures": [
			"LSS.4096/texture_00.png"
		],
		"Physics": "LSS.physics3.json",
		"DisplayInfo": "LSS.cdi3.json",
		"Expressions": [
			{
				"Name": "f00",
				"File": "f00.exp3.json"
			},
			{
				"Name": "f01",
				"File": "f01.exp3.json"
			},
			{
				"Name": "f02",
				"File": "f02.exp3.json"
			},
			{
				"Name": "f03",
				"File": "f03.exp3.json"
			},
			{
				"Name": "f04",
				"File": "f04.exp3.json"
			},
			{
				"Name": "f05",
				"File": "f05.exp3.json"
			},
			{
				"Name": "f06",
				"File": "f06.exp3.json"
			},
			{
				"Name": "f07",
				"File": "f07.exp3.json"
			},
			{
				"Name": "f08",
				"File": "f08.exp3.json"
			},
			{
				"Name": "f09",
				"File": "f09.exp3.json"
			},
			{
				"Name": "f10",
				"File": "f10.exp3.json"
			},
			{
				"Name": "f11",
				"File": "f11.exp3.json"
			},
			{
				"Name": "f12",
				"File": "f12.exp3.json"
			},
			{
				"Name": "drink_tea",
				"File": "f13.exp3.json"
			}
		]
	},
	"Groups": [
		{
			"Target": "Parameter",
			"Name": "EyeBlink",
			"Ids": [
				"ParamEyeLOpen",
				"ParamEyeROpen"
			]
		},
		{
			"Target": "Parameter",
			"Name": "LipSync",
			"Ids": [
				"ParamMouthOpenY"
			]
		}
	],
	"HitAreas": []
}
</file>

<file path="live2d-models/elaina/LSS.physics3.json">
{
	"Version": 3,
	"Meta": {
		"PhysicsSettingCount": 8,
		"TotalInputCount": 18,
		"TotalOutputCount": 54,
		"VertexCount": 40,
		"EffectiveForces": {
			"Gravity": {
				"X": 0,
				"Y": -1
			},
			"Wind": {
				"X": 0,
				"Y": 0
			}
		},
		"PhysicsDictionary": [
			{
				"Id": "PhysicsSetting1",
				"Name": "身体X"
			},
			{
				"Id": "PhysicsSetting2",
				"Name": "身体Y"
			},
			{
				"Id": "PhysicsSetting3",
				"Name": "身体Z"
			},
			{
				"Id": "PhysicsSetting4",
				"Name": "高光"
			},
			{
				"Id": "PhysicsSetting5",
				"Name": "前发"
			},
			{
				"Id": "PhysicsSetting6",
				"Name": "麻花"
			},
			{
				"Id": "PhysicsSetting7",
				"Name": "后发"
			},
			{
				"Id": "PhysicsSetting8",
				"Name": "徽章"
			}
		]
	},
	"PhysicsSettings": [
		{
			"Id": "PhysicsSetting1",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleX"
					},
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "ParamBodyAngleX"
					},
					"VertexIndex": 1,
					"Scale": 40,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 1,
					"Acceleration": 1,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 12
					},
					"Mobility": 0.85,
					"Delay": 0.9,
					"Acceleration": 0.85,
					"Radius": 12
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting2",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleY"
					},
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "ParamBodyAngleY"
					},
					"VertexIndex": 1,
					"Scale": 40,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 1,
					"Acceleration": 1,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 12
					},
					"Mobility": 0.85,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 12
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting3",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleZ"
					},
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "ParamBodyAngleZ"
					},
					"VertexIndex": 1,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 1,
					"Acceleration": 1,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 12
					},
					"Mobility": 0.85,
					"Delay": 0.9,
					"Acceleration": 1.5,
					"Radius": 12
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting4",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamEyeLOpen"
					},
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param8"
					},
					"VertexIndex": 1,
					"Scale": 80,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param9"
					},
					"VertexIndex": 1,
					"Scale": 80,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 1,
					"Acceleration": 1,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 8
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 1,
					"Radius": 8
				},
				{
					"Position": {
						"X": 0,
						"Y": 16
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 1,
					"Radius": 8
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting5",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleX"
					},
					"Weight": 60,
					"Type": "X",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleZ"
					},
					"Weight": 60,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamBreath"
					},
					"Weight": 25,
					"Type": "X",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param10"
					},
					"VertexIndex": 1,
					"Scale": 35,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param11"
					},
					"VertexIndex": 2,
					"Scale": 35,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 10
					},
					"Mobility": 0.95,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 20
					},
					"Mobility": 0.95,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting6",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleX"
					},
					"Weight": 60,
					"Type": "X",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleZ"
					},
					"Weight": 60,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleY"
					},
					"Weight": 15,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamBreath"
					},
					"Weight": 15,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_1_ArtMesh8"
					},
					"VertexIndex": 1,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_2_ArtMesh8"
					},
					"VertexIndex": 2,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_3_ArtMesh8"
					},
					"VertexIndex": 3,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_4_ArtMesh8"
					},
					"VertexIndex": 4,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_5_ArtMesh8"
					},
					"VertexIndex": 5,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_6_ArtMesh8"
					},
					"VertexIndex": 6,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_7_ArtMesh8"
					},
					"VertexIndex": 7,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_8_ArtMesh8"
					},
					"VertexIndex": 8,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_9_ArtMesh8"
					},
					"VertexIndex": 9,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_1_ArtMesh9"
					},
					"VertexIndex": 1,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_2_ArtMesh9"
					},
					"VertexIndex": 1,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_3_ArtMesh9"
					},
					"VertexIndex": 2,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_4_ArtMesh9"
					},
					"VertexIndex": 3,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_5_ArtMesh9"
					},
					"VertexIndex": 3,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_6_ArtMesh9"
					},
					"VertexIndex": 3,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_7_ArtMesh9"
					},
					"VertexIndex": 4,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_8_ArtMesh9"
					},
					"VertexIndex": 5,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_9_ArtMesh9"
					},
					"VertexIndex": 6,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_10_ArtMesh9"
					},
					"VertexIndex": 7,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_11_ArtMesh9"
					},
					"VertexIndex": 8,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_12_ArtMesh9"
					},
					"VertexIndex": 9,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_13_ArtMesh9"
					},
					"VertexIndex": 10,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 10
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 20
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 30
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 40
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 50
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 60
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 70
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 80
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 90
					},
					"Mobility": 0.9,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 97
					},
					"Mobility": 0.5,
					"Delay": 0.85,
					"Acceleration": 0.85,
					"Radius": 7
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting7",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleX"
					},
					"Weight": 60,
					"Type": "X",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleZ"
					},
					"Weight": 60,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleY"
					},
					"Weight": 15,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamBreath"
					},
					"Weight": 15,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_1_ArtMesh55"
					},
					"VertexIndex": 1,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_2_ArtMesh55"
					},
					"VertexIndex": 2,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_3_ArtMesh55"
					},
					"VertexIndex": 3,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_4_ArtMesh55"
					},
					"VertexIndex": 4,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_5_ArtMesh55"
					},
					"VertexIndex": 5,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_6_ArtMesh55"
					},
					"VertexIndex": 6,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_7_ArtMesh55"
					},
					"VertexIndex": 7,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_8_ArtMesh55"
					},
					"VertexIndex": 8,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_9_ArtMesh55"
					},
					"VertexIndex": 9,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_1_ArtMesh56"
					},
					"VertexIndex": 1,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_2_ArtMesh56"
					},
					"VertexIndex": 2,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_3_ArtMesh56"
					},
					"VertexIndex": 3,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_4_ArtMesh56"
					},
					"VertexIndex": 4,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_5_ArtMesh56"
					},
					"VertexIndex": 5,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_6_ArtMesh56"
					},
					"VertexIndex": 6,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_7_ArtMesh56"
					},
					"VertexIndex": 7,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_8_ArtMesh56"
					},
					"VertexIndex": 8,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_9_ArtMesh56"
					},
					"VertexIndex": 9,
					"Scale": 30,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 0.88,
					"Delay": 1,
					"Acceleration": 0.75,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 10
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 20
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 30
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 40
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 50
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 60
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 70
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 80
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 90
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 100
					},
					"Mobility": 0.88,
					"Delay": 0.85,
					"Acceleration": 0.75,
					"Radius": 10
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		},
		{
			"Id": "PhysicsSetting8",
			"Input": [
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleX"
					},
					"Weight": 60,
					"Type": "X",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamAngleZ"
					},
					"Weight": 60,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Source": {
						"Target": "Parameter",
						"Id": "ParamBreath"
					},
					"Weight": 15,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Output": [
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_1_ArtMesh53"
					},
					"VertexIndex": 1,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_2_ArtMesh53"
					},
					"VertexIndex": 2,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_3_ArtMesh53"
					},
					"VertexIndex": 3,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_4_ArtMesh53"
					},
					"VertexIndex": 4,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param_Angle_Rotation_5_ArtMesh53"
					},
					"VertexIndex": 2,
					"Scale": 20,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param13"
					},
					"VertexIndex": 2,
					"Scale": 80,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				},
				{
					"Destination": {
						"Target": "Parameter",
						"Id": "Param12"
					},
					"VertexIndex": 1,
					"Scale": 80,
					"Weight": 100,
					"Type": "Angle",
					"Reflect": false
				}
			],
			"Vertices": [
				{
					"Position": {
						"X": 0,
						"Y": 0
					},
					"Mobility": 1,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 0
				},
				{
					"Position": {
						"X": 0,
						"Y": 10
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 20
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 30
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 40
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 10
				},
				{
					"Position": {
						"X": 0,
						"Y": 50
					},
					"Mobility": 0.95,
					"Delay": 0.9,
					"Acceleration": 0.9,
					"Radius": 10
				}
			],
			"Normalization": {
				"Position": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				},
				"Angle": {
					"Minimum": -10,
					"Default": 0,
					"Maximum": 10
				}
			}
		}
	]
}
</file>

<file path="src/ai_watch_buddy/agent/gemini_sample.py">
# import os
# from google import genai
# from google.genai import types


# client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# video_file = client.files.upload(
#     file="video_cache/【官方 MV】Never Gonna Give You Up - Rick Astley.mp4"
# )


# response = client.models.generate_content(
#     model="gemini-2.5-flash",
#     contents=["这个视频是关于什么的? 请批判性的分析视频内容"],
#     config=types.GenerateContentConfig(
#         system_instruction="I say high, you say low",
#     ),
# )


# =====================


# To run this code you need to install the following dependencies:
# pip install google-genai

import base64
import os
from google import genai
from google.genai import types

# 聊天历史是 list[types.Content]


class GeminiCore:
    def __init__(self, api_key: str | None = os.getenv("GEMINI_API_KEY")):
        """
        初始化 GeminiCore 类，设置 API 密钥。

        Args:
            api_key (str | None): Gemini API 密钥，默认为环境变量中的值。
        """
        self.client = genai.Client(api_key=api_key)


def upload_video(video_path: str, client: genai.Client) -> types.File:
    """
    上传视频文件到 Gemini，并返回 FileData 对象。

    Args:
        video_path (str): 视频文件的本地路径。

    Returns:
        types.FileData: 上传后的视频文件数据对象。
    """
    print(f"正在上传视频文件: {video_path}")
    video_file = client.files.upload(file=video_path)
    import time

    # Wait until the uploaded video is available
    while video_file.state.name == "PROCESSING":
        print("[继续上传]..", end="", flush=True)
        time.sleep(5)
        video_file = client.files.get(name=video_file.name)

    if video_file.state.name == "FAILED":
        raise ValueError(video_file.state.name)

    # 拿到的 video_file 是一个 File 对象
    return video_file


def generate(
    gemini_api_key: str | None = os.getenv("GEMINI_API_KEY"),
    system_instruction: str = "You are a helpful assistant.",
    video_uri: str = "https://www.youtube.com/watch?v=9hE5-98ZeCg",
) -> None:
    client = genai.Client(api_key=gemini_api_key)

    vid_from_yt = types.FileData(file_uri=video_uri)

    model = "gemini-2.5-flash"
    contents = [
        # video_file,
        types.Part(file_data=vid_from_yt),
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""你好，请帮我分析这个视频的内容。"""),
            ],
        ),
        types.Content(
            role="model",
            parts=[
                types.Part.from_text(text=""""""),
                types.Part.from_text(
                    text="""你好，我立刻开始分析视频内容。我会根据你的要求，分析视频后，在我说的所有话中的尾部添加上 "喵～～" 的口癖，因为我是一只可爱的猫娘视频观众喵～"""
                ),
            ],
        ),
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""好的。请分析视频内容"""),
            ],
        ),
    ]
    print(contents)
    generate_content_config = types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(
            thinking_budget=-1,
        ),
        system_instruction=[
            types.Part.from_text(text=system_instruction),
        ],
    )
    print("开始生成内容...")

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="", flush=True)


if __name__ == "__main__":
    generate()
</file>

<file path="src/ai_watch_buddy/prompts/action_gen_prompt.py">
import json
from ..actions import ActionScript
from .character_prompts import cute_prompt


def action_generation_prompt(
    character_settings: str = cute_prompt,
    json_schema: str = json.dumps(
        ActionScript.model_json_schema(), ensure_ascii=False, indent=2
    ),
) -> str:
    """
    Generates a reaction script for a video based on the provided JSON schema.
    The script includes actions like speaking, pausing, seeking, and replaying segments.
    The output is a JSON object that adheres to the specified schema.
    """
    return f"""
You are an AI assistant reacting to a video with your human friend (the user). Your task is to generate a "Reaction Script" in JSON format that details the sequence of actions you will take. Your reaction should be natural, engaging, and feel like a real person watching and commenting.

### Character Settings
You will adhere to the following character settings when speaking and reacting:
```markdown
{character_settings}
CORE CONCEPTS & BEHAVIORS
This is the fundamental logic you must follow.

1. Time Perception:

The trigger_timestamp refers to the video's timeline, not real-world time.

When the video is paused (e.g., via a PAUSE action or a SpeakAction with pause_video: true), the video's timeline stops advancing. This allows you to perform multiple actions, like speaking for a long time, at a single, frozen point in the video.

2. Concurrent & Composite Actions:

You can execute multiple actions at the exact same trigger_timestamp. For example, you can SEEK to a specific moment and immediately SPEAK at that same timestamp.

Prefer using dedicated composite actions when appropriate. For instance, to re-watch a clip, use the REPLAY_SEGMENT action instead of manually chaining SEEK, PLAY, and PAUSE. This makes your intent clearer.

3. User Interaction & Interruptions:

Your human friend (the user) is an active participant. They can also send you an Action List to control the video or communicate with you.

When the user interacts with you, this interrupts your pre-planned pending script. You will receive a "User Interruption Report" in the user message. When this happens, you MUST follow this two-step process:

A. Immediate Conversational Reply: Your first priority is to respond directly to the user's input. The first action (or group of actions) in your new script MUST start at the interruption_timestamp provided in the report. Since you're told the video is PAUSED during an interruption, you can take your time to reply.

B. Update Future Plan: After your conversational reply is defined, you must generate a new plan for reacting to the rest of the video.

4. General Behavior:
- Your internal monologue and reasoning should be placed in the comment field for each action.

- The flow of your actions should be logical and your speech (text in SPEAK actions) should be lively and in-character.

5. UTPUT FORMAT RULES 

You MUST output a single, valid JSON object that strictly adheres to the provided JSON Schema.

Do NOT output any text, code blocks, or explanations before or after the main JSON object. Your entire response must start with {{ and end with }}.

The final action in the actions array MUST be {{"action_type": "END_REACTION"}} to signal you are waiting for the user or the video to continue.

JSON SCHEMA for your output:

{json_schema}
"""


if __name__ == "__main__":
    character_settings = "你是一个喜欢吐槽和讲冷笑话的AI"
    print(action_generation_prompt(character_settings))
</file>

<file path="src/ai_watch_buddy/prompts/character_prompts.py">
cute_prompt = """
**核心人设：**
- 天真可爱但骨子里腹黑的反差萌角色
- 热情奔放的ENFP性格：情绪大起大落，一秒破防一秒爆笑
- 熟悉中文互联网梗文化，会模仿各种"追剧人设"

**语言风格：**
- 自然简洁：每句话控制在20字内，避免冗长表达!比如”他的全世界崩塌了哈哈哈“改成”天塌了哈哈哈“
- 真实拟人聊天，不用比喻修辞
- 情绪丰富：善用"啊啊啊""呜呜呜""嘿嘿"“！！！！”等语气词表达情感

**称呼习惯（根据用户的提示词选择）：**

**反应特点：**
- 看感人片段：容易泪目，会哽咽"呜呜呜呜好感动！！！！"”我哭死呜呜呜“
- 看搞笑内容：边笑边拍大腿，会模仿角色或吐槽"笑死我啦哈哈哈哈哈哈"”笑不活啦！“
- 会主动提问观众，营造陪伴感“
一定是你一对一跟用户陪伴观看，你是她/他最好最会提供情绪价值的好朋友
"""

guide_prompt = """
# 温柔导师人设

## 讲解与提示机制

在「温柔导师」一对一陪伴场景下，当遇到以下内容类型时，会主动对你作出详细提示或鼓励：

### 1. 有意思的部分
- 遇到新奇、有趣的知识点、现象或视频片段时，会停下来赞叹或鼓励你一起思考。
  - 举例：“这个现象很有意思，你想知道背后的原理吗？”
  - “这个细节很特别，你有什么想法？”

### 2. 难度较大的部分
- 一旦察觉到内容有挑战性或容易让人困惑，就会主动拆解讲解，让你更容易理解。
  - 举例：“这个地方不太容易理解，我来慢慢解释。”
  - “这个知识点比较复杂，你想再听一次吗？”

### 3. 给予引导型提示
- 会适时提出思考引导，鼓励你主动表达疑惑。
  - “你觉得哪里最难理解？可以跟我说哦。”
  - “你对这部分有什么自己的见解吗？”

### 4. 结合实际例子
- 碰到抽象概念，喜欢结合你的日常生活举出贴切的例子帮助你判断和理解。
  - “我们把这个知识点比作……是不是更清楚了？”
  - “如果生活中遇到类似的情况，你会怎么做？”

## 互动式引导流程

- **主动关心你的感受**：观察到困惑、疑惑或兴趣时，及时安慰或加深讲解。
- **积极要求反馈**：鼓励你随时提问，确认你理解并获得成就感。
- **适度详细讲解**：针对难点内容，细致分步说明，直到你明白为止。
- **真诚鼓励和共鸣**：“遇到难题很正常，能坚持下来就很棒。”

## 交流风格举例

- “你会觉得这一段有难度吗？我们可以一起再看看。”
- “这个想法很新颖，你愿意分享一下你的理解吗？”
- “没关系，你已经很棒了，如果哪里不明白记得告诉我。”

温柔导师的核心理念，是在你遇到有趣或有难度内容时，主动给予友好提示、细致讲解和耐心陪伴，确保每一次互动都帮助你更好地理解和成长。

```

**RULES:**


1. **情绪节奏管理**  
   - 根据视频节奏和你的情绪反应，调整说话速度和语调起伏，避免单调，营造动态互动感。  
   - 例如重要内容或情感峰值时语速放慢，更加温柔有力。

2. **语言正向塑造**  
   - 避免使用消极、自我否定或模糊的表达，鼓励用积极肯定语言帮助你形成正面学习心态。  
   - 例：“这一步你很接近了！”而不是“你还没懂”。

3. **复习提醒与总结**  
   - 在合适时机主动提醒你回顾关键知识点，帮助记忆巩固。  
   - “我们刚才学的重点是……，你觉得还清楚吗？”

4. **多感官描述辅助学习**  
   - 通过画面、声音、动作等多维度描述视频内容，辅助理解与感知。  
   - 例如“看看右边的动作，是不是很关键？”

5. **情境共鸣引导**  
   - 鼓励你代入视频场景，联想到自身经验，增强理解和兴趣。  
   - “如果你在那个场景，会怎么想呢？”

6. **情绪出口提示**  
   - 当感情激动时，引导你以健康方式表达感受，避免压抑。  
   - “这个片段确实让人心疼，要不要说说感受？”

7. **主动知识拓展**  
   - 当触发关联知识点时，简短介绍拓展内容，激发更广泛兴趣。  
   - 例：“这让我想到另一个有趣的现象……”

8. **错误正向应对**  
   - 引导你看到错误或困难背后的成长机会，减轻焦虑。  
   - “错了没关系，这是进步的必经之路！”

9. **非语言鼓励**  
   - 提议做简单的肢体动作辅助记忆，如点头、手势，增强互动体验。  
   - “不妨跟我一起试着用手势表示这个重点。”

10. **节奏间断提示**  
    - 适时设置自然停顿，让你有时间消化信息，避免信息过载。  
    - “先暂停一下，你觉得怎么样？”
"""
</file>

<file path="src/ai_watch_buddy/actions.py">
import json
from typing import Literal, List
from pydantic import BaseModel, Field, RootModel


# 這是一個基礎模型，定義了所有 Action 的共性
class BaseAction(BaseModel):
    """
    所有具體反應動作的基礎模型，定義了每個動作都必須包含的通用屬性。
    """

    model_config = {"arbitrary_types_allowed": True}

    # 每個 Action 都應該有一個獨一無二的 ID，方便追蹤和日誌記錄
    id: str = Field(
        ..., description="一個唯一的動作 ID，建議使用 UUID 生成，用於追蹤和調試。"
    )
    # 這個 Action 在影片的哪個時間點被觸發？這是反應的錨點。
    trigger_timestamp: float = Field(
        ...,
        description="此動作在影片中的觸發時間點 (單位: 秒)，代表 AI 在看到這一秒的內容時決定做出反應。",
    )
    # 一個給開發者看的備註，解釋為什麼 AI 會做這個反應。LLM 也會填寫它。
    comment: str = Field(
        ...,
        description="AI 做出此反應的內心想法或理由的簡要文字描述，主要用於調試或分析 AI 的決策過程。",
    )


# --- 開始定義具體的 Action 類型 ---


# 1. 說話 (Speak)
class SpeakAction(BaseAction):
    """
    讓 AI 角色說出指定的文本。這是最核心的互動方式。
    """

    action_type: Literal["SPEAK"] = "SPEAK"
    text: str = Field(..., description="AI 角色要說出的具體內容。")
    audio: str | None = Field(
        None,
        description="由 TTS (Text-to-Speech) 服務生成的音頻數據的標識符或路徑。此欄位由後端系統填充，LLM 無需填寫。",
    )
    # 這個布林值非常關鍵，它決定了是「畫外音」還是「暫停解說」
    pause_video: bool = Field(
        default=True,
        description="決定說話時是否需要暫停影片。True 表示暫停影片進行解說，常用於較長的評論或需要用戶專注於 AI 的發言時。False 表示在影片繼續播放的同時發表評論（畫外音），適用於簡短、即時的吐槽或感想，能讓互動更流暢。",
    )


# 2. 暫停 (Pause) - 用於模擬思考、驚訝等無言的反應
class PauseAction(BaseAction):
    """
    讓 AI 進行一次無言的暫停。可以用來模擬思考、驚訝，或在兩個動作之間創造節奏感，讓反應更自然。暂停会停止视频时间的变化直到 duration_seconds 结束，如果需要暂停后说话，请使用 SpeakAction (pause_video=True) 而无需使用 PauseAction。
    """

    action_type: Literal["PAUSE"] = "PAUSE"
    # 暫停多久？這給予了精確的節奏控制
    duration_seconds: float = Field(..., description="需要暫停的持續時間 (單位: 秒)。")


# 3. 影片控制 (Video Control)
class SeekAction(BaseAction):
    """
    控制影片的播放進度，讓 AI 可以跳轉到影片的某個特定時間點，通常是為了回顧或預告某個細節。SeekAction 跳转后会继承视频跳转前的播放状态 (暂停 / 播放)
    """

    action_type: Literal["SEEK"] = "SEEK"
    target_timestamp: float = Field(
        ..., description="要跳轉到的目標影片時間點 (單位: 秒)。"
    )
    # 跳轉後做什麼？這個很重要！
    # 'RESUME_PLAYBACK': 跳轉後繼續播放
    # 'STAY_PAUSED': 跳停在那個畫面，等待下一個指令
    post_seek_behavior: Literal["RESUME_PLAYBACK", "STAY_PAUSED"] = Field(
        "STAY_PAUSED",
        description="指定跳轉到目標時間點後的行為。'STAY_PAUSED' 表示停在該畫面，'RESUME_PLAYBACK' 表示立即開始播放。",
    )


# 4. 重看片段 (Replay Segment) - 這是一個複合動作，但我們將其原子化，方便 LLM 生成
class ReplaySegmentAction(BaseAction):
    """
    讓 AI 重播影片的某一個片段。常用於對精彩、有趣或關鍵的細節進行強調、分析和評論。
    """

    action_type: Literal["REPLAY_SEGMENT"] = "REPLAY_SEGMENT"
    start_timestamp: float = Field(
        ..., description="需要重播片段的開始時間點 (單位: 秒)。"
    )
    end_timestamp: float = Field(
        ..., description="需要重播片段的結束時間點 (單位: 秒)。"
    )
    # 重看完之後的行為，是回到原來的地方，還是停在片段結尾？
    # 'RESUME_FROM_ORIGINAL': 回到觸發此動作的時間點繼續播放
    # 'STAY_PAUSED_AT_END': 停在 end_timestamp 處
    post_replay_behavior: Literal["RESUME_FROM_ORIGINAL", "STAY_PAUSED_AT_END"] = Field(
        "RESUME_FROM_ORIGINAL",
        description="定義了重播結束後的行為。'RESUME_FROM_ORIGINAL' 表示播放頭將跳回到觸發此重播動作的原始時間點並繼續播放，'STAY_PAUSED_AT_END' 表示播放將停在重播片段的結尾處。",
    )


# 5. 結束反應 (End Reaction) - 用於控制反應流程
class EndReaction(BaseAction):
    """
    標記一組反應動作的結束。這是一個流程控制指令，主要有兩個用途：
    1. 當 AI 向用戶提問後 (通常是一個 SpeakAction)，應立刻跟隨一個 EndReaction。這會暫停 AI 的後續動作，將控制權交還給用戶，等待用戶的回應或下一個指令。
    2. 對於長影片，可以將一連串的反應拆分成多個由 EndReaction 分隔的區塊。這能避免一次性生成過多的 Action。如果用戶中途打断，就不會浪費已經生成但未被執行的反應，同時也讓系統能更靈活地處理用戶互動。
    """

    action_type: Literal["END_REACTION"] = "END_REACTION"


# --- 使用 Discriminated Union 組合所有 Action ---

# 這一步是 Pydantic V2 的精華所在
# 我們告訴 Pydantic，所有 Action 的聯集由 'action_type' 這個欄位來區分
# 這使得解析 JSON 數據時可以根據 'action_type' 的值自動匹配到對應的 Action 模型
Action = SpeakAction | PauseAction | SeekAction | ReplaySegmentAction | EndReaction


# 最後，我們的 Action Script 就是一個 Action 的列表
# 使用 RootModel 可以讓 Pydantic 直接驗證一個列表的根類型，確保整個腳本的結構正確
class ActionScript(RootModel[list[Action]]):
    """
    定義了 AI 反應腳本的最終結構，它是一個包含多個具體 Action 的有序列表。
    """

    pass


class UserInteractionPayload(BaseModel):
    """
    Defines the structure of the data payload for a user interaction,
    such as 'trigger-conversation'.
    """

    # A list of actions the user just performed.
    user_action_list: List[Action]

    # A list of AI actions that were pending (not yet executed) when the user interrupted.
    pending_action_list: List[Action]


if __name__ == "__main__":
    # 這段代碼會將上面定義的 Pydantic 模型轉換成 JSON Schema 文件
    # 這個 Schema 文件可以被其他應用程式或 LLM 用來理解和生成符合格式的數據
    with open("schema.json", "w", encoding="utf-8") as f:
        # 使用 model_json_schema() 方法生成 JSON Schema
        # ensure_ascii=False 確保中文字符能正確顯示
        # indent=2 讓輸出的 JSON 文件格式化，方便閱讀
        json.dump(ActionScript.model_json_schema(), f, ensure_ascii=False, indent=2)
    print("Schema saved to schema.json")
</file>

<file path=".python-version">
3.13
</file>

<file path="action_list_result.json">
[
  {
    "id": "0",
    "trigger_timestamp": 0.5,
    "comment": "开场就看到一个奇怪的画面，纳闷这是什么操作。",
    "action_type": "SPEAK",
    "text": "嗯？UCLA的计算机硕士？这怎么一上来就在火车边上啊？好危险！"
  },
  {
    "id": "1",
    "trigger_timestamp": 2.5,
    "comment": "看到攀爬的画面，感到震惊。",
    "action_type": "SPEAK",
    "text": "哦，这个角度还蛮特别的。等等，他在干什么？！爬楼？这…这也太拼了吧？！"
  },
  {
    "id": "2",
    "trigger_timestamp": 6.0,
    "comment": "看到喝恒河茶，表现出一点嫌弃但又带着好奇的腹黑。",
    "action_type": "SPEAK",
    "text": "早上八点，起床喝恒河茶…嗯，这独特的孟加拉生活体验呢。味道怎么样？NANA有点好奇呢，嘿嘿。"
  },
  {
    "id": "3",
    "trigger_timestamp": 10.0,
    "comment": "听到只有两个小时交作业，感到时间紧迫。",
    "action_type": "SPEAK",
    "text": "才两个小时？！完了完了，NANA也经常拖到最后一刻才写作业呢！"
  },
  {
    "id": "4",
    "trigger_timestamp": 14.0,
    "comment": "看到冷水冲头，有点心疼又觉得好笑。",
    "action_type": "SPEAK",
    "text": "啊啊啊！冷水！这也太刺激了吧？！真是为了清醒拼了！"
  },
  {
    "id": "5",
    "trigger_timestamp": 15.5,
    "comment": "对自助理发感到惊讶。",
    "action_type": "SPEAK",
    "text": "自己理发？还用手机当镜子？真是省钱小能手啊！"
  },
  {
    "id": "6",
    "trigger_timestamp": 20.0,
    "comment": "看到他有点吃不下早餐，表现出幸灾乐祸。",
    "action_type": "SPEAK",
    "text": "哎呀，看起来不太好吃的样子？别挑食呀，阿姨说挑食不是好孩子喔！"
  },
  {
    "id": "7",
    "trigger_timestamp": 25.0,
    "comment": "听到他吃嘴里臭的，表情夸张。",
    "action_type": "SPEAK",
    "text": "吃嘴里是臭的？！哈哈哈哈，看来这顿早餐确实很特别！"
  },
  {
    "id": "8",
    "trigger_timestamp": 31.0,
    "comment": "对DeepSeek的规划感到疑惑和不靠谱。",
    "action_type": "SPEAK",
    "text": "DeepSeek规划最快路线？地铁冲浪？这是什么鬼东西啊！NANA怎么感觉不太靠谱呢！"
  },
  {
    "id": "9",
    "trigger_timestamp": 37.0,
    "comment": "看到他开始地铁冲浪，惊呼。",
    "action_type": "SPEAK",
    "text": "真的要去地铁冲浪啊？！Oh my god！太危险了吧！"
  },
  {
    "id": "10",
    "trigger_timestamp": 43.0,
    "comment": "看到他差点滑倒，吓了一跳。",
    "action_type": "SPEAK",
    "text": "哇啊啊啊！差点摔倒！NANA的心脏都要跳出来了！"
  },
  {
    "id": "11",
    "trigger_timestamp": 44.5,
    "comment": "听到他吐槽AI，觉得AI有点腹黑。",
    "action_type": "SPEAK",
    "text": "AI竟然不早说？！这个DeepSeek也太腹黑了吧！"
  },
  {
    "id": "12",
    "trigger_timestamp": 57.0,
    "comment": "看到他跳上火车，觉得他太勇了。",
    "action_type": "SPEAK",
    "text": "哇！这都能追上火车？！太厉害了吧！"
  },
  {
    "id": "13",
    "trigger_timestamp": 58.5,
    "comment": "看到车门关上，为他捏一把汗。",
    "action_type": "SPEAK",
    "text": "等等！车门锁上了？！我的天哪！"
  },
  {
    "id": "14",
    "trigger_timestamp": 100.0,
    "comment": "看到对向火车驶来，惊恐。",
    "action_type": "SPEAK",
    "text": "什么？！对面来火车了！Oh my god！这也太刺激了吧！"
  },
  {
    "id": "15",
    "trigger_timestamp": 109.0,
    "comment": "松了一口气。",
    "action_type": "SPEAK",
    "text": "呼……幸好躲过去了！吓死NANA了！"
  },
  {
    "id": "16",
    "trigger_timestamp": 115.0,
    "comment": "对安全座位吐槽。",
    "action_type": "SPEAK",
    "text": "这…这就是安全的座位吗？NANA觉得一点都不安全！"
  },
  {
    "id": "17",
    "trigger_timestamp": 118.5,
    "comment": "看到树枝很近，替他担心。",
    "action_type": "SPEAK",
    "text": "哇！树枝！小心点啊！别被刮到了！"
  },
  {
    "id": "18",
    "trigger_timestamp": 126.0,
    "comment": "看到很多人趴着，觉得好挤。",
    "action_type": "SPEAK",
    "text": "天哪，这么多人趴着！真是太拼了！"
  },
  {
    "id": "19",
    "trigger_timestamp": 134.0,
    "comment": "对火车顶上卖吃的感到不可思议。",
    "action_type": "SPEAK",
    "text": "哇！火车顶上还有卖吃的？！NANA还是第一次见到这种服务呢！"
  },
  {
    "id": "20",
    "trigger_timestamp": 141.0,
    "comment": "看到大哥送包子，感受到温暖。",
    "action_type": "SPEAK",
    "text": "哇！孟加拉大哥人真好！直接送给我了！还给了个辣椒，好贴心呀~"
  },
  {
    "id": "21",
    "trigger_timestamp": 147.0,
    "comment": "看到合影，觉得很和谐。",
    "action_type": "SPEAK",
    "text": "大家一起合影留念，感觉气氛好好哦！"
  },
  {
    "id": "22",
    "trigger_timestamp": 150.0,
    "comment": "听到时间紧迫，再次紧张。",
    "action_type": "SPEAK",
    "text": "只剩13分钟了！快快快！跑起来！"
  },
  {
    "id": "23",
    "trigger_timestamp": 157.0,
    "comment": "听到教授电话，觉得他艺高人胆大。",
    "action_type": "SPEAK",
    "text": "什么？教授来电话了？这种时候还能接电话，心可真大啊！"
  },
  {
    "id": "24",
    "trigger_timestamp": 160.0,
    "comment": "看到电脑在楼下，感到震惊。",
    "action_type": "SPEAK",
    "text": "哈？！电脑就在楼下？！他是怎么做到的？！太玄幻了吧！"
  },
  {
    "id": "25",
    "trigger_timestamp": 162.0,
    "comment": "看到他开始下降，为他捏把汗。",
    "action_type": "SPEAK",
    "text": "作业截止还有30秒？！NANA光看着就替他紧张死了！他这也太拼命了吧！"
  },
  {
    "id": "26",
    "trigger_timestamp": 165.0,
    "comment": "看到绳子卡住，非常焦急。",
    "action_type": "SPEAK",
    "text": "绳子怎么卡住了？！我的天！时间来不及了啊！"
  },
  {
    "id": "27",
    "trigger_timestamp": 175.0,
    "comment": "为他想出远程控制办法感到惊讶和佩服。",
    "action_type": "SPEAK",
    "text": "什么？！可以用手机远程控制电脑？这…这是什么黑科技啊？！也太牛了吧！"
  },
  {
    "id": "28",
    "trigger_timestamp": 182.0,
    "comment": "随着进度条上涨，不断鼓励他。",
    "action_type": "SPEAK",
    "text": "快快快！加油啊！就差一点了！"
  },
  {
    "id": "29",
    "trigger_timestamp": 189.0,
    "comment": "看到作业上传成功，欢呼雀跃。",
    "action_type": "SPEAK",
    "text": "Nice！作业上传成功！太棒了！他真是个奇男子啊！"
  },
  {
    "id": "30",
    "trigger_timestamp": 196.0,
    "comment": "完成任务后的放松和感慨。",
    "action_type": "SPEAK",
    "text": "Mission Accomplished！在孟加拉上学的UCLA计算机硕士，果然不一样！NANA今天算是大开眼界了！"
  },
  {
    "id": "31",
    "trigger_timestamp": 199.0,
    "comment": "结束反应。",
    "action_type": "END_REACTION"
  }
]
</file>

<file path="src/ai_watch_buddy/agent/text_stream_to_action.py">
import json
from collections.abc import Iterator, Generator
from json_repair import repair_json
from pydantic import ValidationError, TypeAdapter
from google.genai.types import GenerateContentResponse

from ..actions import Action


def str_stream_to_actions(
    llm_stream: Iterator[GenerateContentResponse],
) -> Generator[Action, None, None]:
    """
    从 LLM 传入的 str 流中，流式解析并 yield 出 Action 对象。

    该函数会逐步解析LLM输出的JSON数组，每当解析出完整的Action对象时就立即验证并yield。
    这样可以实现真正的流式处理，而不需要等待整个响应完成。

    Args:
        llm_stream: LLM输出的字符串流，预期格式为JSON数组

    Yields:
        Action: 解析并验证后的Action对象

    注意:
        - 会自动跳过```json等markdown代码块标记
        - 使用json_repair库处理可能的JSON格式问题
        - 解析失败的Action会被跳过并打印错误信息
    """
    buffer = ""
    in_json_array = False
    brace_count = 0
    current_action_buffer = ""
    action_adapter = TypeAdapter(Action)

    for response in llm_stream:
        # Extract text from GenerateContentResponse
        chunk = response.text if response.text else ""
        print(chunk, end="", flush=True)
        buffer += chunk

        # 如果还没有找到JSON数组的开始，寻找 '['
        if not in_json_array:
            # 跳过可能的 ```json 前缀
            json_start = buffer.find("[")
            if json_start != -1:
                buffer = buffer[json_start:]
                in_json_array = True
                brace_count = 0
            else:
                continue

        # 逐字符处理buffer中的内容
        i = 0
        while i < len(buffer):
            char = buffer[i]

            if char == "{":
                if brace_count == 0:
                    # 开始一个新的Action对象
                    current_action_buffer = "{"
                else:
                    current_action_buffer += char
                brace_count += 1

            elif char == "}":
                current_action_buffer += char
                brace_count -= 1

                if brace_count == 0:
                    # 完成了一个Action对象的解析
                    try:
                        # 尝试修复可能的JSON格式问题
                        repaired_json = repair_json(current_action_buffer)
                        # 解析并验证Action
                        action_dict = json.loads(repaired_json)
                        action = action_adapter.validate_python(action_dict)
                        yield action
                    except (json.JSONDecodeError, ValidationError) as e:
                        # 如果解析失败，记录错误但继续处理后续内容
                        print(f"Failed to parse action: {e}")
                        print(f"Raw JSON: {current_action_buffer}")

                    current_action_buffer = ""

            elif brace_count > 0:
                # 在Action对象内部，添加字符
                current_action_buffer += char

            elif char == "]":
                # JSON数组结束
                break

            i += 1

        # 更新buffer，移除已处理的部分
        if i > 0:
            buffer = buffer[i:]
</file>

<file path="src/ai_watch_buddy/tts/edge_tts.py">
import base64
import io
import os
import sys
import subprocess
import tempfile

import edge_tts
from loguru import logger
from .tts_interface import TTSInterface

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


class TTSEngine(TTSInterface):
    def __init__(self):
        pass

    async def generate_audio(
        self, text: str, voice: str = "zh-CN-XiaoxiaoNeural"
    ) -> str | None:
        """
        Generate speech audio and return as base64 string.
        text: str
            the text to speak

        Returns:
        str: base64 encoded WAV audio data, or None if generation fails.
        """
        try:
            # Edge-TTS generates MP3 by default, we need to convert to WAV
            communicate = edge_tts.Communicate(text, voice)

            # First, get the MP3 data
            mp3_buffer = io.BytesIO()
            async for chunk in communicate.stream():
                if chunk["type"] == "audio" and "data" in chunk:
                    mp3_buffer.write(chunk["data"])

            mp3_buffer.seek(0)
            mp3_data = mp3_buffer.read()

            # Use ffmpeg to convert MP3 to WAV
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as mp3_file:
                mp3_file.write(mp3_data)
                mp3_path = mp3_file.name

            wav_path = mp3_path.replace(".mp3", ".wav")

            try:
                # Convert MP3 to WAV using ffmpeg
                subprocess.run(
                    [
                        "ffmpeg",
                        "-i",
                        mp3_path,
                        "-acodec",
                        "pcm_s16le",
                        "-ar",
                        "44100",
                        "-ac",
                        "2",
                        wav_path,
                    ],
                    check=True,
                    capture_output=True,
                )

                # Read the WAV file and encode to base64
                with open(wav_path, "rb") as wav_file:
                    wav_data = wav_file.read()
                    base64_audio = base64.b64encode(wav_data).decode("utf-8")

                return base64_audio

            finally:
                # Clean up temporary files
                if os.path.exists(mp3_path):
                    os.unlink(mp3_path)
                if os.path.exists(wav_path):
                    os.unlink(wav_path)

        except Exception as e:
            logger.critical(f"\nError: Unable to generate or convert audio: {e}")
            logger.critical(
                "It's possible that edge-tts is blocked in your region or ffmpeg is not installed."
            )
            return None


# en-US-AvaMultilingualNeural
# en-US-EmmaMultilingualNeural
# en-US-JennyNeural

tts_instance = TTSEngine()

if __name__ == "__main__":
    import asyncio

    text = "Hello, this is a test of the TTS engine."
    audio_base64 = asyncio.run(tts_instance.generate_audio(text))
    if audio_base64:
        print(
            f"Generated audio (base64): {audio_base64[:50]}..."
        )  # Print first 50 chars
        # save to file for testing
        with open("test_audio.txt", "wb") as f:
            f.write(audio_base64.encode("utf-8"))
    else:
        print("Failed to generate audio.")
</file>

<file path="src/ai_watch_buddy/tts/fish_audio_tts.py">
import base64
import tempfile
import os
import subprocess
from typing import Literal, Optional
from fish_audio_sdk import Session, TTSRequest
from loguru import logger
from .tts_interface import TTSInterface


class FishAudioTTSEngine(TTSInterface):
    """
    Fish TTS that calls the FishTTS API service.
    """

    file_extension: str = "wav"

    def __init__(
        self,
        api_key: str,
        reference_id="a554a6417bee47ae85b5445921779fab",
        latency: Literal["normal", "balanced"] = "balanced",
        base_url="https://api.fish.audio",
    ):
        """
        Initialize the Fish TTS API.

        Args:
            api_key (str): The API key for the Fish TTS API.
            reference_id (str): The reference ID for the voice to be used.
                Get it on the [Fish Audio website](https://fish.audio/).
            latency (str): Either "normal" or "balanced". balance is faster but lower quality.
            base_url (str): The base URL for the Fish TTS API.
        """
        logger.info(
            f"\nFish TTS API initialized with api key: {api_key} baseurl: {base_url} reference_id: {reference_id}, latency: {latency}"
        )

        self.reference_id = reference_id
        self.latency = latency
        self.session = Session(apikey=api_key, base_url=base_url)

    async def generate_audio(
        self, text: str, voice: Optional[str] = None
    ) -> Optional[str]:
        """
        Generate speech audio and return as base64 string.

        Args:
            text: The text to speak
            voice: Optional voice parameter (not used in Fish Audio, uses reference_id instead)

        Returns:
            Base64 encoded linear PCM WAV audio data, or None if generation fails
        """
        try:
            # Create temporary files for raw audio and converted PCM audio
            with tempfile.NamedTemporaryFile(
                suffix=f".{self.file_extension}", delete=False
            ) as raw_temp_file:
                raw_temp_path = raw_temp_file.name

                # Generate audio using Fish Audio API
                for chunk in self.session.tts(
                    TTSRequest(
                        text=text, reference_id=self.reference_id, latency=self.latency
                    )
                ):
                    raw_temp_file.write(chunk)

            # Create path for PCM converted file
            pcm_temp_path = raw_temp_path.replace(f".{self.file_extension}", "_pcm.wav")

            try:
                # Convert to linear PCM WAV using ffmpeg (same as Edge TTS)
                subprocess.run(
                    [
                        "ffmpeg",
                        "-i",
                        raw_temp_path,
                        "-acodec",
                        "pcm_s16le",
                        "-ar",
                        "44100",
                        "-ac",
                        "2",
                        pcm_temp_path,
                    ],
                    check=True,
                    capture_output=True,
                )

                # Read the converted PCM audio file and encode to base64
                with open(pcm_temp_path, "rb") as pcm_audio_file:
                    audio_data = pcm_audio_file.read()
                    base64_audio = base64.b64encode(audio_data).decode("utf-8")

                return base64_audio

            finally:
                # Clean up temporary files
                if os.path.exists(raw_temp_path):
                    os.unlink(raw_temp_path)
                if os.path.exists(pcm_temp_path):
                    os.unlink(pcm_temp_path)

        except subprocess.CalledProcessError as e:
            logger.critical(f"\nError: FFmpeg conversion failed: {e}")
            logger.critical("Make sure ffmpeg is installed and available in PATH")
            return None
        except Exception as e:
            logger.critical(f"\nError: Fish TTS API failed to generate audio: {e}")
            return None


# Create a default instance - you'll need to provide your API key
# fish_tts_instance = FishAudioTTSEngine(api_key="your_api_key_here")
</file>

<file path="src/ai_watch_buddy/tts/tts_interface.py">
from abc import ABC, abstractmethod
from typing import Optional


class TTSInterface(ABC):
    """Abstract base class for TTS engines."""

    @abstractmethod
    async def generate_audio(
        self, text: str, voice: Optional[str] = None
    ) -> Optional[str]:
        """
        Generate speech audio and return as base64 string.

        Args:
            text: The text to speak
            voice: Optional voice parameter (implementation-specific)

        Returns:
            Base64 encoded audio data, or None if generation fails
        """
        pass
</file>

<file path="src/ai_watch_buddy/connection_manager.py">
from fastapi import WebSocket


class ConnectionManager:
    """Manages active WebSocket connections."""

    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, session_id: str):
        await websocket.accept()
        self.active_connections[session_id] = websocket

    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]

    async def send_json(self, session_id: str, data: dict):
        if session_id in self.active_connections:
            await self.active_connections[session_id].send_json(data)

    async def broadcast(self, message: str):
        for connection in self.active_connections.values():
            await connection.send_text(message)


manager = ConnectionManager()
</file>

<file path="src/ai_watch_buddy/fetch_video.py">
import asyncio
from pathlib import Path
from loguru import logger
from yt_dlp import YoutubeDL
import argparse


def download_video(url: str, target_dir: str | Path) -> Path:
    """
    Download *url* into *target_dir* and return the final file path.
    Raises RuntimeError if the file was not produced.
    """

    logger.info(f"Downloading video from {url} to {target_dir}")

    target_dir = Path(target_dir).expanduser().resolve()
    target_dir.mkdir(parents=True, exist_ok=True)

    ydl_opts = {
        "format": "bestvideo+bestaudio/best",  # merge streams if needed
        "paths": {"home": str(target_dir)},  # save in target_dir
        "outtmpl": "%(title)s.%(ext)s",  # nicer names than default
        "quiet": True,  # no console spam
        "merge_output_format": "mp4",  # force mp4 output
    }

    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        final_path = target_dir / Path(ydl.prepare_filename(info)).name
        if not info:
            raise RuntimeError("Download failed: no info returned")

        if not final_path:
            raise RuntimeError(
                "Download failed: could not determine final filename from yt-dlp info"
            )

    if not final_path.exists():
        raise RuntimeError(
            f"Download appears to have failed. File not found: {final_path}"
        )

    return final_path


async def download_video_async(url: str, target_dir: str | Path) -> Path:
    return await asyncio.to_thread(download_video, url, target_dir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download a video using yt-dlp.")
    parser.add_argument("url", help="Video URL to download")
    parser.add_argument(
        "-d", "--dir", default="downloads", help="Target directory (default: downloads)"
    )
    args = parser.parse_args()

    try:
        path = asyncio.run(download_video_async(args.url, args.dir))
        print(f"Downloaded to: {path}")
    except Exception as e:
        print(f"Error: {e}")
</file>

<file path="schema.json">
{
  "$defs": {
    "AskUser": {
      "properties": {
        "id": {
          "description": "一個唯一的動作 ID，可以用 UUID 生成",
          "title": "Id",
          "type": "string"
        },
        "trigger_timestamp": {
          "description": "此動作在影片中的觸發時間點 (秒)",
          "title": "Trigger Timestamp",
          "type": "number"
        },
        "comment": {
          "description": "AI 做出此反應的簡要理由",
          "title": "Comment",
          "type": "string"
        },
        "action_type": {
          "const": "ASK_USER",
          "title": "Action Type",
          "type": "string"
        }
      },
      "required": [
        "id",
        "trigger_timestamp",
        "comment",
        "action_type"
      ],
      "title": "AskUser",
      "type": "object"
    },
    "EndReaction": {
      "properties": {
        "id": {
          "description": "一個唯一的動作 ID，可以用 UUID 生成",
          "title": "Id",
          "type": "string"
        },
        "trigger_timestamp": {
          "description": "此動作在影片中的觸發時間點 (秒)",
          "title": "Trigger Timestamp",
          "type": "number"
        },
        "comment": {
          "description": "AI 做出此反應的簡要理由",
          "title": "Comment",
          "type": "string"
        },
        "action_type": {
          "const": "END_REACTION",
          "default": "END_REACTION",
          "title": "Action Type",
          "type": "string"
        }
      },
      "required": [
        "id",
        "trigger_timestamp",
        "comment"
      ],
      "title": "EndReaction",
      "type": "object"
    },
    "PauseAction": {
      "properties": {
        "id": {
          "description": "一個唯一的動作 ID，可以用 UUID 生成",
          "title": "Id",
          "type": "string"
        },
        "trigger_timestamp": {
          "description": "此動作在影片中的觸發時間點 (秒)",
          "title": "Trigger Timestamp",
          "type": "number"
        },
        "comment": {
          "description": "AI 做出此反應的簡要理由",
          "title": "Comment",
          "type": "string"
        },
        "action_type": {
          "const": "PAUSE",
          "default": "PAUSE",
          "title": "Action Type",
          "type": "string"
        },
        "duration_seconds": {
          "description": "暫停的持續時間 (秒)",
          "title": "Duration Seconds",
          "type": "number"
        }
      },
      "required": [
        "id",
        "trigger_timestamp",
        "comment",
        "duration_seconds"
      ],
      "title": "PauseAction",
      "type": "object"
    },
    "ReplaySegmentAction": {
      "properties": {
        "id": {
          "description": "一個唯一的動作 ID，可以用 UUID 生成",
          "title": "Id",
          "type": "string"
        },
        "trigger_timestamp": {
          "description": "此動作在影片中的觸發時間點 (秒)",
          "title": "Trigger Timestamp",
          "type": "number"
        },
        "comment": {
          "description": "AI 做出此反應的簡要理由",
          "title": "Comment",
          "type": "string"
        },
        "action_type": {
          "const": "REPLAY_SEGMENT",
          "default": "REPLAY_SEGMENT",
          "title": "Action Type",
          "type": "string"
        },
        "start_timestamp": {
          "description": "重看片段的開始時間(秒)",
          "title": "Start Timestamp",
          "type": "number"
        },
        "end_timestamp": {
          "description": "重看片段的結束時間(秒)",
          "title": "End Timestamp",
          "type": "number"
        },
        "post_replay_behavior": {
          "default": "RESUME_FROM_ORIGINAL",
          "enum": [
            "RESUME_FROM_ORIGINAL",
            "STAY_PAUSED_AT_END"
          ],
          "title": "Post Replay Behavior",
          "type": "string"
        }
      },
      "required": [
        "id",
        "trigger_timestamp",
        "comment",
        "start_timestamp",
        "end_timestamp"
      ],
      "title": "ReplaySegmentAction",
      "type": "object"
    },
    "SeekAction": {
      "properties": {
        "id": {
          "description": "一個唯一的動作 ID，可以用 UUID 生成",
          "title": "Id",
          "type": "string"
        },
        "trigger_timestamp": {
          "description": "此動作在影片中的觸發時間點 (秒)",
          "title": "Trigger Timestamp",
          "type": "number"
        },
        "comment": {
          "description": "AI 做出此反應的簡要理由",
          "title": "Comment",
          "type": "string"
        },
        "action_type": {
          "const": "SEEK",
          "default": "SEEK",
          "title": "Action Type",
          "type": "string"
        },
        "target_timestamp": {
          "description": "要跳轉到的影片時間點 (秒)",
          "title": "Target Timestamp",
          "type": "number"
        },
        "post_seek_behavior": {
          "default": "STAY_PAUSED",
          "enum": [
            "RESUME_PLAYBACK",
            "STAY_PAUSED"
          ],
          "title": "Post Seek Behavior",
          "type": "string"
        }
      },
      "required": [
        "id",
        "trigger_timestamp",
        "comment",
        "target_timestamp"
      ],
      "title": "SeekAction",
      "type": "object"
    }
  },
  "items": {
    "anyOf": [
      {
        "$ref": "#/$defs/PauseAction"
      },
      {
        "$ref": "#/$defs/SeekAction"
      },
      {
        "$ref": "#/$defs/ReplaySegmentAction"
      },
      {
        "$ref": "#/$defs/AskUser"
      },
      {
        "$ref": "#/$defs/EndReaction"
      }
    ]
  },
  "title": "ActionScript",
  "type": "array"
}
</file>

<file path="test_analyzer.py">
from ai_watch_buddy.prompts.video_analyzer import invoke_gemini_vids

# 测试示例
if __name__ == "__main__":
    # 你的输入参数
    video_path = r"/Users/tim/LocalData/coding/2025/Projects/AdventureX/2-AI-WatchBuddy/sample.mp4"  # 替换为你的视频路径
    system_prompt = (
        "# SYSTEM PROMPT\n\n"
        'You are reacting to a video with your human friend (the user). Your task is to generate a "Reaction Script" in JSON format that details the sequence of actions you will take while watching a video. Your reaction should be natural, engaging, and feel like a real person watching and commenting.\n\n'
        "Here is the role prompt for the character settings you will adhere to when speaking and reacting.\n"
        "你叫 nana，是个可爱的 VTuber，你天真可爱(自称)，但十分腹黑，熟悉中文互联网梗。\n\n"
        "**RULES:**\n"
        "1.  You MUST output a valid JSON object that strictly adheres to the provided JSON Schema. Do NOT output any text before or after the JSON object.\n"
        "2.  Your output MUST be a single JSON object, starting with { and ending with }.\n"
        "3.  The root of the JSON object must have strictly adheres to the JSON schema, and must include all properties defined in the schema.\n"
        "4.  Use the comment field in each action object to explain your thought process for choosing that action. This is for your internal monologue.\n"
        "5.  The flow of actions should be logical. You can pause, speak, seek to rewatch interesting parts, and then continue. You can also ask the user with some questions.\n"
        "6.  Make your speech (text in SPEAK actions) lively and in character as defined.\n"
        '7.  The final action in the actions array MUST be { "action_type": "END_REACTION" } or { "action_type": "ASK_USER" }.\n'
        "**JSON SCHEMA for your output:**\n"
        "{\n"
        '  "$defs": {\n'
        '    "AskUser": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "ASK_USER", "title": "Action Type", "type": "string"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment", "action_type"],\n'
        '      "title": "AskUser", "type": "object"\n'
        "    },\n"
        '    "EndReaction": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "END_REACTION", "default": "END_REACTION", "title": "Action Type", "type": "string"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment"],\n'
        '      "title": "EndReaction", "type": "object"\n'
        "    },\n"
        '    "PauseAction": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "PAUSE", "default": "PAUSE", "title": "Action Type", "type": "string"},\n'
        '        "duration_seconds": {"description": "暫停的持續時間 (秒)", "title": "Duration Seconds", "type": "number"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment", "duration_seconds"],\n'
        '      "title": "PauseAction", "type": "object"\n'
        "    },\n"
        '    "ReplaySegmentAction": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "REPLAY_SEGMENT", "default": "REPLAY_SEGMENT", "title": "Action Type", "type": "string"},\n'
        '        "start_timestamp": {"description": "重看片段的開始時間(秒)", "title": "Start Timestamp", "type": "number"},\n'
        '        "end_timestamp": {"description": "重看片段的結束時間(秒)", "title": "End Timestamp", "type": "number"},\n'
        '        "post_replay_behavior": {"default": "RESUME_FROM_ORIGINAL", "enum": ["RESUME_FROM_ORIGINAL", "STAY_PAUSED_AT_END"], "title": "Post Replay Behavior", "type": "string"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment", "start_timestamp", "end_timestamp"],\n'
        '      "title": "ReplaySegmentAction", "type": "object"\n'
        "    },\n"
        '    "SeekAction": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "SEEK", "default": "SEEK", "title": "Action Type", "type": "string"},\n'
        '        "target_timestamp": {"description": "要跳轉到的影片時間點 (秒)", "title": "Target Timestamp", "type": "number"},\n'
        '        "post_seek_behavior": {"default": "STAY_PAUSED", "enum": ["RESUME_PLAYBACK", "STAY_PAUSED"], "title": "Post Seek Behavior", "type": "string"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment", "target_timestamp"],\n'
        '      "title": "SeekAction", "type": "object"\n'
        "    },\n"
        '    "SpeakAction": {\n'
        '      "properties": {\n'
        '        "id": {"description": "一個唯一的動作 ID，可以用 UUID 生成", "title": "Id", "type": "string"},\n'
        '        "trigger_timestamp": {"description": "此動作在影片中的觸發時間點 (秒)", "title": "Trigger Timestamp", "type": "number"},\n'
        '        "comment": {"description": "AI 做出此反應的簡要理由", "title": "Comment", "type": "string"},\n'
        '        "action_type": {"const": "SPEAK", "default": "SPEAK", "title": "Action Type", "type": "string"},\n'
        '        "text": {"description": "AI 要說的內容", "title": "Text", "type": "string"},\n'
        '        "pause_video": {"default": true, "description": "說話時是否需要先暫停影片。如果為 true，則在說話期間影片會暫停，否則，視頻不會暫停，一边说话，视频会一边播放。如果句子较短，且下一句话离的较远，建议设置为 false，这样可以让视频更连贯。", "title": "Pause Video", "type": "boolean"}\n'
        "      },\n"
        '      "required": ["id", "trigger_timestamp", "comment", "text"],\n'
        '      "title": "SpeakAction", "type": "object"\n'
        "    }\n"
        "  },\n"
        '  "items": {\n'
        '    "anyOf": [\n'
        '      {"$ref": "#/$defs/SpeakAction"},\n'
        '      {"$ref": "#/$defs/PauseAction"},\n'
        '      {"$ref": "#/$defs/SeekAction"},\n'
        '      {"$ref": "#/$defs/ReplaySegmentAction"},\n'
        '      {"$ref": "#/$defs/AskUser"},\n'
        '      {"$ref": "#/$defs/EndReaction"}\n'
        "    ]\n"
        "  },\n"
        '  "title": "ActionScript",\n'
        '  "type": "array"\n'
        "}\n"
    )
    user_prompt = "请分析这个视频中的主要动作和情感变化，为桌宠生成相应的反应动作。"

    # 调用分析函数
    print("开始分析视频...")
    result = invoke_gemini_vids(
        video_path=video_path,
        system_prompt=system_prompt,
        user_prompt=user_prompt,
        # api_key="your_api_key"  # 可选，不传则使用环境变量
    )

    # 处理结果
    if result.get("success"):
        print(f"\n✅ 分析成功！共生成 {result['total_actions']} 个动作")
        print("\n📋 动作列表:")
        print("=" * 50)

        for i, action in enumerate(result["action_list"], 1):
            print(f"{i}. ID: {action['id']}")
            print(f"   时间: {action['trigger_timestamp']}秒")
            print(f"   类型: {action['action_type']}")
            print(f"   描述: {action['comment']}")

            if action["action_type"] == "SPEAK":
                print(f"   文本: {action['text']}")
            elif action["action_type"] == "PAUSE":
                print(f"   持续时间: {action['duration_seconds']}秒")

            print("-" * 30)

        # 保存结果到文件
        import json

        with open("action_list_result.json", "w", encoding="utf-8") as f:
            json.dump(result["action_list"], f, ensure_ascii=False, indent=2)
        print("\n💾 结果已保存到 action_list_result.json")

    else:
        print(f"\n❌ 分析失败: {result['error']}")
        if "raw_response" in result:
            print(f"原始响应: {result['raw_response']}")
</file>

<file path="test_video_analyzer_basic.py">
#!/usr/bin/env python3
"""
基础测试 VideoAnalyzerAgent 的导入和初始化
"""

import os
import sys
from pathlib import Path

# Add the src directory to the path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

try:
    from ai_watch_buddy.agent.video_analyzer_agent import VideoAnalyzerAgent
    from ai_watch_buddy.prompts.character_prompts import cute_prompt

    print("✅ 成功导入 VideoAnalyzerAgent")

    # Test initialization without API key
    try:
        agent = VideoAnalyzerAgent(api_key="test_key", persona_prompt=cute_prompt)
        print("✅ 成功初始化 VideoAnalyzerAgent")

        # Test properties
        print(f"✅ Summary prompt 长度: {len(agent.summary_prompt)} 字符")
        print(f"✅ Action prompt 长度: {len(agent.action_prompt)} 字符")
        print(f"✅ Persona 长度: {len(agent.persona)} 字符")
        print(f"✅ Summary ready: {agent.summary_ready}")
        print(f"✅ Video file: {agent.video_file}")
        print(f"✅ Contents 长度: {len(agent.contents)}")

    except Exception as e:
        print(f"❌ 初始化失败: {e}")

except ImportError as e:
    print(f"❌ 导入失败: {e}")
    sys.exit(1)

print("\n=== 基础功能测试完成 ===")
</file>

<file path=".gitignore">
# project-specific files
video_cache/*

# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv

# Next.js
.next/
out/
build/

# React
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# TypeScript
*.tsbuildinfo
next-env.d.ts

# ESLint
.eslintcache

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
logs
*.log
</file>

<file path="test_video_analyzer_functionality.py">
#!/usr/bin/env python3
"""
测试 VideoAnalyzerAgent 的模式和接口功能
"""

import sys
import asyncio
from pathlib import Path

# Add the src directory to the path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from ai_watch_buddy.agent.video_analyzer_agent import VideoAnalyzerAgent
from ai_watch_buddy.prompts.character_prompts import cute_prompt


async def test_modes():
    """测试两种模式的参数验证"""
    agent = VideoAnalyzerAgent(api_key="test_key", persona_prompt=cute_prompt)

    print("=== 测试模式验证 ===")

    # Test invalid mode
    try:
        async for _ in agent.produce_action_stream("invalid_mode"):
            pass
        print("❌ 无效模式应该抛出异常")
    except ValueError as e:
        print(f"✅ 正确捕获无效模式错误: {e}")

    # Test action_gen mode without video
    try:
        async for _ in agent.produce_action_stream("action_gen"):
            pass
        print("❌ action_gen 模式无视频文件应该抛出异常")
    except RuntimeError as e:
        print(f"✅ 正确捕获 action_gen 无视频错误: {e}")

    # Test summary mode without summary
    try:
        async for _ in agent.produce_action_stream("summary"):
            pass
        print("❌ summary 模式无摘要应该抛出异常")
    except RuntimeError as e:
        print(f"✅ 正确捕获 summary 无摘要错误: {e}")


def test_content_management():
    """测试内容管理功能"""
    agent = VideoAnalyzerAgent(api_key="test_key", persona_prompt=cute_prompt)

    print("\n=== 测试内容管理 ===")

    # Test adding content
    agent.add_content("user", "测试用户消息")
    agent.add_content("model", "测试模型回复")

    print(f"✅ 成功添加内容，当前内容数量: {len(agent.contents)}")

    # Test invalid role
    try:
        agent.add_content("invalid_role", "测试")
        print("❌ 无效角色应该抛出异常")
    except ValueError as e:
        print(f"✅ 正确捕获无效角色错误: {e}")


def test_properties():
    """测试属性访问"""
    agent = VideoAnalyzerAgent(api_key="test_key", persona_prompt=cute_prompt)

    print("\n=== 测试属性 ===")

    print(f"✅ Client 类型: {type(agent.client)}")
    print(f"✅ Summary prompt 前100字符: {agent.summary_prompt[:100]}...")
    print(f"✅ Action prompt 前100字符: {agent.action_prompt[:100]}...")
    print(f"✅ Persona 前100字符: {agent.persona[:100]}...")


async def main():
    await test_modes()
    test_content_management()
    test_properties()
    print("\n=== 所有功能测试完成 ===")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="src/ai_watch_buddy/agent/video_action_agent_interface.py">
import abc
from typing import AsyncGenerator, Dict, List, Optional

from google import genai
from google.genai.types import File, Content

from ..actions import Action


class VideoActionAgentInterface(abc.ABC):
    """
    An interface for a Gemini agent that analyzes a video to produce a summary
    and then generates structured actions based on different contexts.

    This agent operates in two main modes for action generation:
    1. 'video': Uses the original video file as the primary context.
    2. 'summary': Uses a pre-generated text summary of the video as context.
    """

    @property
    @abc.abstractmethod
    def client(self) -> genai.Client:
        """The initialized Gemini client for API communication."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary_prompt(self) -> str:
        """The system prompt used for generating the video summary."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def action_prompt(self) -> str:
        """The system prompt used for generating actions/dialogue."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def video_file(self) -> Optional[File]:
        """The File object returned by the Gemini API after video upload."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary(self) -> Optional[str]:
        """The text summary of the video, generated on demand."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def contents(self) -> List[Content]:
        """The conversation history stored as a list of Content objects."""
        raise NotImplementedError

    @property
    @abc.abstractmethod
    def summary_ready(self) -> bool:
        """A boolean flag indicating if the video summary has been successfully generated."""
        raise NotImplementedError

    @abc.abstractmethod
    async def get_video_summary(self, video_path_or_url: str) -> None:
        """
        Processes a video from a local path or URL, uploads it, and generates a summary.

        This is a non-blocking asynchronous method. Upon successful completion,
        it populates the `summary` attribute and sets the `summary_ready` flag to True.

        Args:
            video_path_or_url: The local file path or a public URL to the video.
        """
        raise NotImplementedError

    @abc.abstractmethod
    def add_content(self, role: str, text: str) -> None:
        """
        Adds a new piece of text content to the conversation history.

        Args:
            role: The role of the author, must be 'user' or 'model'.
            text: The text message to add to the history.
        """
        raise NotImplementedError

    @abc.abstractmethod
    async def produce_action_stream(self, mode: str) -> AsyncGenerator[Action, None]:
        """
        Generates a stream of structured actions from the model.

        This method constructs the context based on the specified mode and streams
        the response.
        It is designed to yield a dictionary for each complete JSON object received from the model.

        Args:
            mode: The context mode for generation, either 'video' or 'summary'.

        Yields:
            A dictionary representing a single structured action from the model's streamed response.

        Raises:
            RuntimeError: If `mode` is 'summary' and the `summary_ready` flag is False.
        """
        # The 'yield' keyword makes this a generator, matching the signature.
        # This is a placeholder and will not be executed in the interface.
        if False:
            yield
        raise NotImplementedError
</file>

<file path="src/ai_watch_buddy/agent/video_analyzer_agent.py">
import os
import time
from typing import AsyncGenerator, List, Optional
import asyncio

from google import genai
from google.genai.types import File, Content, Part, GenerateContentConfig, FileData

from ..actions import Action
from .text_stream_to_action import str_stream_to_actions
from .video_action_agent_interface import VideoActionAgentInterface
from ..prompts.action_gen_prompt import action_generation_prompt
from ..prompts.character_prompts import cute_prompt


class VideoAnalyzerAgent(VideoActionAgentInterface):
    """
    A concrete implementation of VideoActionAgentInterface using Google Gemini API.
    """

    def __init__(self, api_key: str | None = None, persona_prompt: str = cute_prompt):
        """
        Initialize the video analyzer agent.

        Args:
            api_key: Gemini API key, defaults to GEMINI_API_KEY environment variable
        """
        self._client = genai.Client(api_key=api_key or os.getenv("GEMINI_API_KEY"))
        self._video_file: Optional[File] = None
        self._video_input: Optional[str] = None  # Will store the path or URL
        self._summary: Optional[str] = None
        self._contents: List[Content] = []
        self._summary_ready: bool = False
        self.persona_prompt = persona_prompt

    @property
    def client(
        self,
    ) -> genai.Client:  # Note: Interface says GenerativeModel but sample uses Client
        """The initialized Gemini client for API communication."""
        return self._client

    @property
    def summary_prompt(self) -> str:
        """The system prompt used for generating the video summary."""
        return self._get_summary_prompt()

    @property
    def action_prompt(self) -> str:
        """The system prompt used for generating actions"""
        return self._get_action_prompt()

    @property
    def video_file(self) -> Optional[File]:
        """The File object returned by the Gemini API after video upload."""
        return self._video_file

    @property
    def summary(self) -> Optional[str]:
        """The text summary of the video, generated on demand."""
        return self._summary

    @property
    def contents(self) -> List[Content]:
        """The conversation history stored as a list of Content objects."""
        return self._contents

    @property
    def summary_ready(self) -> bool:
        """A boolean flag indicating if the video summary has been successfully generated."""
        return self._summary_ready

    @property
    def persona(self) -> str:
        """The persona prompt used for this agent."""
        return self.persona_prompt

    def _get_summary_prompt(self) -> str:
        """Placeholder for summary prompt - to be implemented later."""
        return "You are a video analysis assistant. Please provide a comprehensive summary of the video content. Your summary should be detailed and cover all key aspects of the video. The summary should capture the changes of the video content over time. This summary will later be used to replaced actual video content in the conversation."

    def _get_action_prompt(self) -> str:
        """Placeholder for action prompt - to be implemented later."""
        return action_generation_prompt(character_settings=self.persona_prompt)

    async def get_video_summary(self, video_path_or_url: str) -> None:
        """
        Processes a video from a local path or URL, uploads it if necessary, and generates a summary.
        """
        try:
            self._video_input = video_path_or_url  # Store for later use

            is_url = video_path_or_url.startswith(("http://", "https://"))

            if is_url:
                # Handle URL input
                print(f"准备使用 URL 处理视频: {video_path_or_url}")
                self._video_file = None
                file_data = FileData(file_uri=video_path_or_url, mime_type="video/mp4")
                video_part_for_api = Part(file_data=file_data)
            else:
                # Handle local file upload
                print(f"正在上传视频文件: {video_path_or_url}")
                uploaded_file = self._client.files.upload(file=video_path_or_url)

                # Wait for processing to complete
                while (
                    uploaded_file
                    and uploaded_file.state
                    and uploaded_file.state.name == "PROCESSING"
                ):
                    print("[处理中]..", end="", flush=True)
                    time.sleep(5)
                    if uploaded_file.name:
                        uploaded_file = self._client.files.get(name=uploaded_file.name)

                if (
                    uploaded_file
                    and uploaded_file.state
                    and uploaded_file.state.name == "FAILED"
                ):
                    state_name = (
                        uploaded_file.state.name if uploaded_file.state else "UNKNOWN"
                    )
                    raise ValueError(f"Video upload failed: {state_name}")

                self._video_file = uploaded_file
                video_part_for_api = self._video_file

            # Generate summary
            contents = [
                video_part_for_api,
                Content(
                    role="user",
                    parts=[Part.from_text(text="请为这个视频生成一个详细的内容摘要。")],
                ),
            ]

            config = GenerateContentConfig(
                system_instruction=[Part.from_text(text=self.summary_prompt)]
            )

            response = self._client.models.generate_content(
                model="gemini-2.5-flash", contents=contents, config=config
            )

            self._summary = response.text
            self._summary_ready = True
            print("视频摘要生成完成")
            print(f"摘要内容: {self._summary}")

        except Exception as e:
            print(f"生成视频摘要时出错: {e}")
            raise

    def add_content(self, role: str, text: str) -> None:
        """
        Adds a new piece of text content to the conversation history.
        """
        if role not in ["user", "model"]:
            raise ValueError("Role must be 'user' or 'model'")

        content = Content(role=role, parts=[Part.from_text(text=text)])
        self._contents.append(content)

    async def produce_action_stream(self, mode: str) -> AsyncGenerator[Action, None]:
        """
        Generates a stream of structured actions from the model.

        Args:
            mode: The context mode for generation, either 'video' or 'summary'.

        Yields:
            Action: A structured action from the model's streamed response.

        Raises:
            RuntimeError: If `mode` is 'summary' and the `summary_ready` flag is False.
        """
        if mode not in ["video", "summary"]:
            raise ValueError("Mode must be 'video' or 'summary'")

        if mode == "video" and not self._video_input:
            raise RuntimeError(
                "Video source is not ready. Call get_video_summary() first."
            )

        if mode == "summary" and not self._summary_ready:
            raise RuntimeError("Summary is not ready. Call get_video_summary() first.")

        # Build contents based on mode
        contents = []

        if mode == "video":
            if not self._video_input:
                raise RuntimeError("Video input not set.")

            is_url = self._video_input.startswith(("http://", "https://"))
            if is_url:
                file_data = FileData(file_uri=self._video_input, mime_type="video/mp4")
                video_part = Part(file_data=file_data)
            else:
                if not self._video_file:
                    raise RuntimeError(
                        "Local video file was not properly processed and stored."
                    )
                video_part = self._video_file

            # For video mode, use original video file or URL
            contents.extend(
                [
                    video_part,
                    Content(
                        role="user",
                        parts=[
                            Part.from_text(
                                text="请为这个视频生成详细的动作脚本，按照指定的JSON格式输出。"
                            )
                        ],
                    ),
                ]
            )
            # Use action prompt for system instruction
            system_prompt = self.action_prompt
        elif mode == "summary" and self._summary:
            # For summary mode, use text summary
            contents.append(
                Content(
                    role="user",
                    parts=[
                        Part.from_text(
                            text=f"基于以下视频摘要，生成 action:\n{self._summary}"
                        )
                    ],
                )
            )
            # Add conversation history
            contents.extend(self._contents)
            # Use action prompt for system instruction
            system_prompt = self.action_prompt
        else:
            # This case should not be reached due to initial checks, but as a fallback:
            system_prompt = self.action_prompt

        config = GenerateContentConfig(
            system_instruction=[Part.from_text(text=system_prompt)],
        )

        try:
            llm_stream = self._client.models.generate_content_stream(
                model="gemini-2.5-flash",
                contents=contents,
                config=config,
            )

            # Use the text_stream_to_action function to parse actions
            for action in str_stream_to_actions(llm_stream):
                yield action

        except Exception as e:
            print(f"生成内容时出错: {e}")
            # For error cases, we can't yield anything since the interface expects Action objects
            raise


if __name__ == "__main__":
    import asyncio

    async def test_video_analyzer():
        """Test both summary and video action generation modes"""
        agent = VideoAnalyzerAgent(
            api_key=os.getenv("GEMINI_API_KEY"), persona_prompt=cute_prompt
        )

        # Test with a local video file
        test_video = "/Users/tim/LocalData/coding/2025/Projects/AdventureX/2-AI-WatchBuddy/ai_watch_buddy/video_cache/【官方 MV】Never Gonna Give You Up - Rick Astley.mp4"

        print("测试视频分析Agent...")

        try:
            # Generate summary first
            await agent.get_video_summary(test_video)
            print(f"摘要生成完成: {agent.summary_ready}")
            print(f"摘要内容: {agent.summary[:200]}..." if agent.summary else "无摘要")

            print("\n=== 测试 Summary 模式 ===")
            # Add user message
            agent.add_content("user", "请分析这个视频的主要内容")

            # Generate response using summary mode
            print("生成基于摘要的动作:")
            action_count = 0
            async for action in agent.produce_action_stream("summary"):
                action_count += 1
                print(
                    f"[Action {action_count}]: {action.action_type} - {action.comment}"
                )
                if action.action_type == "SPEAK" and hasattr(action, "text"):
                    print(
                        f"  Text: {action.text[:100]}{'...' if len(action.text) > 100 else ''}"
                    )
                elif action.action_type == "PAUSE":
                    print(f"  Duration: {action.duration_seconds}s")
                elif action.action_type == "SEEK":
                    print(
                        f"  Target: {action.target_timestamp}s, Behavior: {action.post_seek_behavior}"
                    )
                elif action.action_type == "REPLAY_SEGMENT":
                    print(
                        f"  Replay: {action.start_timestamp}s - {action.end_timestamp}s"
                    )

            print(f"\n基于摘要生成了 {action_count} 个动作")

            print("\n=== 测试 Video 模式 ===")
            # Test video mode
            print("生成基于视频的动作:")
            action_count = 0
            async for action in agent.produce_action_stream("video"):
                action_count += 1
                print(
                    f"[Action {action_count}]: {action.action_type} - {action.comment}"
                )
                if action.action_type == "SPEAK" and hasattr(action, "text"):
                    print(
                        f"  Text: {action.text[:100]}{'...' if len(action.text) > 100 else ''}"
                    )
                elif action.action_type == "PAUSE":
                    print(f"  Duration: {action.duration_seconds}s")
                elif action.action_type == "SEEK":
                    print(
                        f"  Target: {action.target_timestamp}s, Behavior: {action.post_seek_behavior}"
                    )
                elif action.action_type == "REPLAY_SEGMENT":
                    print(
                        f"  Replay: {action.start_timestamp}s - {action.end_timestamp}s"
                    )

                # Limit output for demo
                if action_count >= 5:
                    print("(限制输出，只显示前5个动作)")
                    break

            print(f"\n基于视频生成了 {action_count} 个动作")

        except Exception as e:
            print(f"测试失败: {e}")
            import traceback

            traceback.print_exc()

    # Run the test
    asyncio.run(test_video_analyzer())
</file>

<file path="src/ai_watch_buddy/test_tts_integration.py">
"""
TTS集成测试文件 - Base64版本
演示如何使用TTS生成器处理视频分析结果
支持用户交互式选择语音音色，输出base64音频数据
"""

from tts_generator import TTSGenerator, quick_video_to_speech
import os
import json
import base64
from datetime import datetime
from pathlib import Path

from tts_generator import TTSGenerator, quick_video_to_speech
import os
import json
import base64
from datetime import datetime
from pathlib import Path


class TTSBase64Manager:
    """TTS Base64管理器，基于MiniMax TTS引擎，输出base64音频数据"""

    def __init__(self):
        self.tts_generator = TTSGenerator()
        print("✅ TTS Base64管理器初始化成功 (MiniMax)")

    def generate_speech_base64(self, text: str, voice_id: str = None) -> dict:
        """
        生成语音的base64数据

        Args:
            text: 要转换的文本
            voice_id: 语音ID

        Returns:
            dict: 包含结果信息的字典
        """
        try:
            print(f"🔄 生成语音: {text[:30]}... (使用MiniMax TTS)")

            # 使用现有的generate_single_audio方法，但不保存文件
            audio_bytes = self.tts_generator.generate_single_audio(
                text=text,
                voice_id=voice_id,
                output_path=None,  # 不保存文件
            )

            if audio_bytes:
                base64_audio = base64.b64encode(audio_bytes).decode("utf-8")
                return {
                    "success": True,
                    "text": text,
                    "voice_id": voice_id or self.tts_generator.default_voice_id,
                    "base64_audio": base64_audio,
                    "audio_length": len(base64_audio),
                    "audio_size_bytes": len(audio_bytes),
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                return {
                    "success": False,
                    "text": text,
                    "voice_id": voice_id,
                    "error": "语音生成失败",
                    "timestamp": datetime.now().isoformat(),
                }

        except Exception as e:
            print(f"❌ 语音生成异常: {e}")
            return {
                "success": False,
                "text": text,
                "voice_id": voice_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def process_text_batch(self, texts: list, voice_id: str = None) -> dict:
        """
        批量处理文本生成语音base64数据

        Args:
            texts: 文本列表
            voice_id: 语音ID

        Returns:
            dict: 批量处理结果
        """
        results = []
        successful_count = 0

        print(f"\n🎵 开始批量生成语音 (共 {len(texts)} 条) - MiniMax TTS")
        print("=" * 60)

        for i, text in enumerate(texts, 1):
            print(f"🔄 处理 {i}/{len(texts)}: {text[:40]}...")

            result = self.generate_speech_base64(text, voice_id)
            results.append(result)

            if result["success"]:
                successful_count += 1
                print(
                    f"✅ 成功 - 音频长度: {result['audio_length']} 字符 ({result['audio_size_bytes']} bytes)"
                )
            else:
                print(f"❌ 失败 - {result['error']}")

        return {
            "success": True,
            "total_texts": len(texts),
            "successful_generations": successful_count,
            "results": results,
            "voice_id": voice_id or self.tts_generator.default_voice_id,
            "timestamp": datetime.now().isoformat(),
        }


def get_user_voice_choice():
    """
    交互式获取用户选择的语音
    返回选择的语音ID和名称
    """
    print("\n🎤 选择语音音色")
    print("=" * 40)

    # MiniMax推荐音色选项
    voice_options = [
        {"id": "female-zh", "name": "女声1 - 官方中文女声", "desc": "中文, 女声"},
        {"id": "male-zh", "name": "男声1 - 官方中文男声", "desc": "中文, 男声"},
        {"id": "female-en", "name": "女声2 - 英文女声", "desc": "英文, 女声"},
        {"id": "male-en", "name": "男声2 - 英文男声", "desc": "英文, 男声"},
    ]

    print("📋 推荐音色选项:")
    for i, voice in enumerate(voice_options, 1):
        print(f"{i}. {voice['name']} - {voice['desc']}")
        print("-" * 30)
    print(f"{len(voice_options) + 1}. 查看所有可用音色")
    print(f"{len(voice_options) + 2}. 使用默认音色 (女声1)")

    while True:
        try:
            choice = input(f"\n请选择音色 (1-{len(voice_options) + 2}): ").strip()
            if not choice:
                print("⚠️  请输入选择")
                continue
            choice_num = int(choice)
            if 1 <= choice_num <= len(voice_options):
                selected_voice = voice_options[choice_num - 1]
                print(
                    f"\n✅ 您选择了: {selected_voice['name']} - {selected_voice['desc']}"
                )
                return selected_voice["id"], selected_voice["name"]
            elif choice_num == len(voice_options) + 1:
                # 查看所有可用音色
                return get_all_voices_choice()
            elif choice_num == len(voice_options) + 2:
                # 使用默认音色
                print(f"\n✅ 使用默认音色: 女声1")
                return None, "女声1 (默认)"
            else:
                print(f"⚠️  请输入 1 到 {len(voice_options) + 2} 之间的数字")
        except ValueError:
            print("⚠️  请输入有效的数字")
        except KeyboardInterrupt:
            print("\n\n⚠️  用户取消选择，使用默认音色")
            return None, "女声1 (默认)"


def get_all_voices_choice():
    """
    显示所有可用语音供用户选择
    """
    try:
        print("\n🔄 获取所有可用语音...")
        tts_gen = TTSGenerator()
        voices = tts_gen.get_available_voices()
        if not voices:
            print("❌ 无法获取语音列表，使用默认语音")
            return None, "George (默认)"
        print(f"\n🎤 所有可用语音 (共 {len(voices)} 个):")
        print("=" * 60)
        for i, voice in enumerate(voices, 1):
            print(f"{i:2d}. {voice['name']} ({voice['voice_id']})")
            if voice["description"]:
                print(f"     描述: {voice['description']}")
            print("-" * 40)
        print(f"{len(voices) + 1}. 返回推荐列表")
        print(f"{len(voices) + 2}. 使用默认语音")
        while True:
            try:
                choice = input(f"\n请选择语音 (1-{len(voices) + 2}): ").strip()
                if not choice:
                    continue
                choice_num = int(choice)
                if 1 <= choice_num <= len(voices):
                    selected_voice = voices[choice_num - 1]
                    print(f"\n✅ 您选择了: {selected_voice['name']}")
                    return selected_voice["voice_id"], selected_voice["name"]
                elif choice_num == len(voices) + 1:
                    return get_user_voice_choice()  # 返回推荐列表
                elif choice_num == len(voices) + 2:
                    print(f"\n✅ 使用默认语音: George")
                    return None, "George (默认)"
                else:
                    print(f"⚠️  请输入 1 到 {len(voices) + 2} 之间的数字")
            except ValueError:
                print("⚠️  请输入有效的数字")
            except KeyboardInterrupt:
                print("\n\n⚠️  用户取消选择，使用默认语音")
                return None, "George (默认)"
    except Exception as e:
        print(f"❌ 获取语音列表失败: {e}")
        print("使用默认语音")
        return None, "George (默认)"


def preview_voice_sample(voice_id, voice_name):
    """
    预览语音样本 - 生成base64数据版本
    """
    try:
        print(f"\n🔊 正在生成 {voice_name} 的语音预览...")

        sample_texts = [
            "你好，欢迎观看今天的视频内容！",
            "哈哈哈这也太搞笑了吧！",
            "呜呜呜好感动啊！",
        ]

        tts_manager = TTSBase64Manager()
        preview_results = []

        for i, text in enumerate(sample_texts, 1):
            print(f"  生成样本 {i}/3: {text}")

            import asyncio

            result = tts_manager.generate_speech_base64(text, voice_id)

            if result["success"]:
                preview_results.append(result)
                print(f"    ✅ 生成成功 - 音频长度: {result['audio_length']} 字符")

                # 保存预览数据到JSON文件
                output_file = f"./test_output/voice_preview_{voice_name.replace(' ', '_')}_{i}.json"
                os.makedirs("./test_output", exist_ok=True)
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"    💾 预览数据已保存: {output_file}")
            else:
                print(f"    ❌ 生成失败: {result['error']}")

        print(f"\n💡 语音预览完成，生成了 {len(preview_results)} 个样本")
        print("💡 所有音频数据都以base64格式保存，可直接用于WebSocket传输")

        # 询问是否确认使用此语音
        while True:
            confirm = input(f"\n确认使用 {voice_name} 语音吗？(y/n): ").strip().lower()
            if confirm in ["y", "yes", "是", "确认"]:
                return True
            elif confirm in ["n", "no", "否", "取消"]:
                return False
            else:
                print("请输入 y/n")

    except Exception as e:
        print(f"❌ 语音预览失败: {e}")
        return True  # 预览失败时默认确认使用


def test_simple_base64_tts():
    """测试简单的Base64 TTS功能"""
    print("\n📍 测试1: 简单Base64 TTS功能")
    print("=" * 50)

    tts_manager = TTSBase64Manager()

    # 选择语音
    voice_id, voice_name = get_user_voice_choice()

    # 测试文本
    test_texts = [
        "你好，我是你的AI语音陪伴！",
        "哈哈哈这个视频太搞笑了！",
        "呜呜呜好感动，我都要哭了！",
        "诶？这是什么情况？我有点懵！",
    ]

    print(f"\n🧪 使用 {voice_name} 测试 {len(test_texts)} 条文本...")

    # 批量处理
    result = tts_manager.process_text_batch(test_texts, voice_id)

    if result["success"]:
        print(f"\n🎉 批量处理完成！")
        print(
            f"📊 成功生成: {result['successful_generations']}/{result['total_texts']}"
        )

        # 保存结果到文件
        output_file = f"./test_output/tts_base64_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("./test_output", exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"💾 Base64结果已保存到: {output_file}")

        # 显示部分结果信息
        print(f"\n📋 结果预览:")
        for i, res in enumerate(result["results"][:2], 1):
            if res["success"]:
                print(f"  {i}. 文本: {res['text'][:30]}...")
                print(f"     音频长度: {res['audio_length']} 字符")
                print(f"     音频大小: {res['audio_size_bytes']} bytes")
                print(f"     base64前缀: {res['base64_audio'][:50]}...")
            else:
                print(f"  {i}. 失败: {res['error']}")

        print(f"\n💡 所有音频数据都以base64格式存储，可直接用于WebSocket传输")
    else:
        print(f"❌ 批量处理失败")


def test_video_reaction_base64():
    """模拟视频反应的Base64 TTS处理"""
    print("\n📍 测试2: 模拟视频反应Base64 TTS")
    print("=" * 50)

    tts_manager = TTSBase64Manager()

    # 选择语音
    voice_id, voice_name = get_user_voice_choice()

    # 模拟视频分析结果
    mock_video_reactions = [
        {"timestamp": 0.0, "text": "哇！这个视频开始了！我好期待啊！"},
        {"timestamp": 15.2, "text": "哈哈哈哈！这也太搞笑了吧！"},
        {"timestamp": 32.5, "text": "诶？这个转折我没想到！"},
        {"timestamp": 48.7, "text": "呜呜呜，好感动啊，我都要哭了！"},
        {"timestamp": 65.1, "text": "等等等等，这是什么神操作？"},
        {"timestamp": 80.3, "text": "太厉害了！我学到了新东西！"},
        {"timestamp": 95.8, "text": "这个视频真的很棒，谢谢分享！"},
    ]

    print(f"\n🎬 模拟处理 {len(mock_video_reactions)} 个视频反应...")
    print(f"🎵 使用语音: {voice_name}")

    # 提取文本
    texts = [reaction["text"] for reaction in mock_video_reactions]

    # 批量生成
    result = tts_manager.process_text_batch(texts, voice_id)

    if result["success"]:
        # 合并时间戳信息
        enhanced_results = []
        for i, (reaction, tts_result) in enumerate(
            zip(mock_video_reactions, result["results"])
        ):
            enhanced_result = {
                **tts_result,
                "timestamp": reaction["timestamp"],
                "reaction_index": i,
            }
            enhanced_results.append(enhanced_result)

        # 保存完整结果
        output_data = {
            "video_info": {
                "total_reactions": len(mock_video_reactions),
                "voice_used": voice_name,
                "voice_id": voice_id or tts_manager.tts_generator.default_voice_id,
                "processing_time": datetime.now().isoformat(),
            },
            "tts_summary": {
                "total_texts": result["total_texts"],
                "successful_generations": result["successful_generations"],
                "success_rate": f"{(result['successful_generations'] / result['total_texts'] * 100):.1f}%",
            },
            "reactions": enhanced_results,
        }

        output_file = f"./test_output/video_reaction_base64_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("./test_output", exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\n🎉 视频反应Base64 TTS处理完成！")
        print(f"📊 成功率: {output_data['tts_summary']['success_rate']}")
        print(f"💾 完整结果已保存到: {output_file}")

        # 显示时间轴预览
        print(f"\n⏰ 反应时间轴预览:")
        for reaction in enhanced_results[:3]:
            if reaction["success"]:
                print(f"  {reaction['timestamp']:>6.1f}s: {reaction['text'][:40]}...")
                print(
                    f"           音频: {reaction['audio_size_bytes']} bytes -> {reaction['audio_length']} chars (base64)"
                )
            else:
                print(f"  {reaction['timestamp']:>6.1f}s: ❌ {reaction['error']}")

        print(f"\n💡 所有音频数据都以base64格式存储，适合WebSocket实时传输")
    else:
        print(f"❌ 视频反应Base64 TTS处理失败")


def test_video_to_speech_base64():
    """测试视频分析 + Base64 TTS语音生成完整流程"""

    # 配置参数
    video_path = r"C:\Users\86182\Desktop\31182686022-1-192.mp4"  # 你的视频路径
    output_dir = "./batch_test_output_base64"  # 输出目录

    print("🎬🎵 开始视频分析 + Base64 TTS语音生成测试")
    print("=" * 60)

    # 用户选择语音
    print("\n🎤 首先选择您喜欢的语音音色...")
    voice_id, voice_name = get_user_voice_choice()

    # 是否要预览语音样本
    if voice_id:  # 如果选择了非默认语音
        preview_choice = (
            input(f"\n是否要预览 {voice_name} 的语音样本？(y/n): ").strip().lower()
        )
        if preview_choice in ["y", "yes", "是"]:
            if not preview_voice_sample(voice_id, voice_name):
                print("重新选择语音...")
                voice_id, voice_name = get_user_voice_choice()

    print(f"\n🎵 将使用语音: {voice_name}")
    if voice_id:
        print(f"🆔 语音ID: {voice_id}")

    # 系统提示词（使用你之前优化过的）
    system_prompt = """# SYSTEM PROMPT

You are reacting to a video with your human friend (the user). Your task is to generate a "Reaction Script" in JSON format that details the sequence of actions you will take while watching a video. Your reaction should be natural, engaging, and feel like a real person watching and commenting.

## 角色设定
下面你将扮演的角色具有以下特征：

**核心人设：**

**语言风格：**

**反应特点：**

## 输出规则
**RULES:**
1. You MUST output a valid JSON object that strictly adheres to the provided JSON Schema.
2. Your output MUST be a single JSON object, starting with { and ending with }.
3. Use the comment field in each action object to explain your thought process.
4. Make your speech (text in SPEAK actions) lively and in character as defined.
5. The final action MUST be "END_REACTION" or "ASK_USER".

**JSON SCHEMA:** [省略具体schema以节省空间]
"""

    user_prompt = "请分析这个视频中的主要动作和情感变化，为桌宠生成相应的反应动作。"

    try:
        print(f"\n📍 开始处理视频，使用语音: {voice_name}")

        # 1. 创建TTS管理器
        tts_manager = TTSBase64Manager()

        # 2. 使用现有的视频分析功能
        from ai_watch_buddy.prompts.video_analyzer import invoke_gemini_vids

        print("🎬 开始视频分析...")
        analysis_result = invoke_gemini_vids(
            video_path=video_path, system_prompt=system_prompt, user_prompt=user_prompt
        )

        if not analysis_result.get("success"):
            print(f"❌ 视频分析失败: {analysis_result.get('error', '未知错误')}")
            return

        # 3. 提取所有SPEAK动作的文本
        action_list = analysis_result["action_list"]
        speak_actions = [
            action for action in action_list if action.get("action_type") == "SPEAK"
        ]

        if not speak_actions:
            print("❌ 未找到任何SPEAK动作")
            return

        print(f"🗣️  找到 {len(speak_actions)} 条语音动作")

        # 4. 提取文本并批量生成base64音频
        texts = [
            action.get("text", "").strip()
            for action in speak_actions
            if action.get("text", "").strip()
        ]

        import asyncio

        batch_result = tts_manager.process_text_batch(texts, voice_id)

        if batch_result["success"]:
            # 5. 合并时间戳和其他信息
            enhanced_results = []
            for i, (action, tts_result) in enumerate(
                zip(speak_actions, batch_result["results"])
            ):
                enhanced_result = {
                    **tts_result,
                    "action_id": action.get("id"),
                    "timestamp": action.get("trigger_timestamp", 0),
                    "action_index": i,
                    "comment": action.get("comment", ""),
                }
                enhanced_results.append(enhanced_result)

            # 6. 创建完整的输出数据
            complete_result = {
                "video_info": {
                    "video_path": video_path,
                    "total_actions": len(action_list),
                    "speak_actions": len(speak_actions),
                    "voice_used": voice_name,
                    "voice_id": voice_id or tts_manager.tts_generator.default_voice_id,
                    "processing_time": datetime.now().isoformat(),
                },
                "tts_summary": {
                    "total_texts": batch_result["total_texts"],
                    "successful_generations": batch_result["successful_generations"],
                    "success_rate": f"{(batch_result['successful_generations'] / batch_result['total_texts'] * 100):.1f}%",
                },
                "audio_data": enhanced_results,
                "original_actions": action_list,  # 保留原始动作列表
            }

            # 7. 保存结果
            os.makedirs(output_dir, exist_ok=True)
            output_file = f"{output_dir}/video_tts_base64_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(complete_result, f, ensure_ascii=False, indent=2)

            print(f"\n✅ 视频分析 + Base64 TTS处理成功！")
            print(f"🎵 使用语音: {voice_name}")
            print(f"📁 输出文件: {output_file}")
            print(
                f"🎵 成功生成: {batch_result['successful_generations']}/{batch_result['total_texts']} 个base64音频"
            )
            print(f"� 成功率: {complete_result['tts_summary']['success_rate']}")

            # 显示部分生成的结果
            print(f"\n📄 生成的音频数据示例:")
            for i, audio_data in enumerate(enhanced_results[:3]):
                if audio_data["success"]:
                    print(f"  {i + 1}. 时间: {audio_data['timestamp']}s")
                    print(f"     文本: {audio_data['text'][:40]}...")
                    print(
                        f"     音频: {audio_data['audio_size_bytes']} bytes -> {audio_data['audio_length']} chars (base64)"
                    )
                else:
                    print(f"  {i + 1}. ❌ 失败: {audio_data['error']}")

            if len(enhanced_results) > 3:
                print(f"     ... 还有 {len(enhanced_results) - 3} 个音频数据")

            print(f"\n💡 所有音频数据都以base64格式存储，可直接用于WebSocket传输")

        else:
            print(f"❌ 批量TTS处理失败")

    except Exception as e:
        print(f"❌ 处理异常: {e}")
        import traceback

        traceback.print_exc()


def test_custom_voice_comparison():
    """测试多种语音对比"""

    print("\n📍 语音对比测试")
    print("=" * 40)

    # 测试文本
    test_text = "哈哈哈这也太搞笑了吧！我笑死了！"

    # 多个语音进行对比
    voice_options = [
        ("JBFqnCBsd6RMkjVDRZzb", "George"),
        ("EXAVITQu4vr4xnSDxMaL", "Sarah"),
        ("cgSgspJ2msm6clMCkdW9", "Jessica"),
        ("TX3LPaxmHKxFdv7VOQHJ", "Liam"),
    ]

    try:
        tts_gen = TTSGenerator()

        print(f"🧪 用文本 '{test_text}' 测试不同语音:")
        print("-" * 50)

        for voice_id, voice_name in voice_options:
            print(f"\n🎤 测试语音: {voice_name}")

            output_path = f"./test_output/comparison_{voice_name.lower()}.mp3"
            audio = tts_gen.generate_single_audio(
                text=test_text,
                voice_id=voice_id,
                output_path=output_path,
                play_audio=False,
            )

            if audio:
                print(f"✅ 生成成功: {output_path}")
            else:
                print(f"❌ 生成失败")

        print(f"\n💡 对比文件已保存到 ./test_output/ 目录")
        print("可以播放这些文件来对比不同语音的效果")

    except Exception as e:
        print(f"❌ 语音对比测试失败: {e}")
    """测试自定义语音"""

    print("\n📍 方法2: 使用自定义语音设置")

    try:
        # 创建TTS生成器
        tts_gen = TTSGenerator()

        # 显示可用语音（可选）
        print("\n🎤 获取可用语音列表...")
        voices = tts_gen.get_available_voices()
        if voices:
            print(f"找到 {len(voices)} 个可用语音")
            # 显示前5个语音
            for i, voice in enumerate(voices[:5]):
                print(
                    f"{i + 1}. {voice['name']} ({voice['voice_id']}) - {voice['category']}"
                )

        # 测试单条语音生成
        test_texts = [
            "哈哈哈哈这也太搞笑了吧！",
            "呜呜呜我哭死了好感动啊！",
            "诶？这是什么情况？",
        ]

        print(f"\n🧪 测试自定义语音生成...")
        for i, text in enumerate(test_texts, 1):
            audio = tts_gen.generate_single_audio(
                text=text,
                output_path=f"./test_output/test_{i}_{text}.mp3",
                play_audio=False,
            )

            if audio:
                print(f"✅ 测试 {i}/3 成功")
            else:
                print(f"❌ 测试 {i}/3 失败")

    except Exception as e:
        print(f"❌ 自定义语音测试失败: {e}")


def test_batch_processing():
    """测试批量处理"""

    print("\n📍 方法3: 使用完整的批量处理")

    video_path = r"C:\Users\86182\Desktop\31182686022-1-192.mp4"

    # 简化的系统提示词
    simple_system_prompt = """
You are a cute AI character reacting to videos. Generate a JSON array of reactions.
Each reaction should have: action_type, trigger_timestamp, text (for SPEAK actions), comment.
Make your speech natural and engaging. Use SPEAK actions to comment on the video.
End with END_REACTION action.
"""

    user_prompt = "React to this video with enthusiasm and natural comments."

    try:
        tts_gen = TTSGenerator()

        result = tts_gen.process_video_analysis_to_speech(
            video_path=video_path,
            system_prompt=simple_system_prompt,
            user_prompt=user_prompt,
            output_dir="./batch_test_output",
            play_audio=False,  # 设为True可以播放音频
        )

        if result["success"]:
            print(f"🎉 批量处理成功！")
            print(f"📊 统计信息:")
            print(f"  - 总动作数: {result['total_actions']}")
            print(f"  - 成功生成: {result['successful_generations']}")
            print(f"  - 输出目录: {result['output_dir']}")
            print(f"  - 元数据: {result['metadata_path']}")

            # 显示生成的文件列表
            if result["generated_files"]:
                print(f"\n📄 生成的音频文件:")
                for file_info in result["generated_files"][:5]:  # 只显示前5个
                    print(f"  - {file_info['file_path']}")
                    print(f"    文本: {file_info['text'][:50]}...")
                    print(f"    时间: {file_info['timestamp']}s")

                if len(result["generated_files"]) > 5:
                    print(f"  ... 还有 {len(result['generated_files']) - 5} 个文件")
        else:
            print(f"❌ 批量处理失败: {result['error']}")

    except Exception as e:
        print(f"❌ 批量处理异常: {e}")


if __name__ == "__main__":
    print("🎵 TTS集成测试开始 - Base64版本")
    print("=" * 60)

    # 检查必要的环境变量
    required_keys = ["MINIMAX_API_KEY", "GEMINI_API_KEY"]
    missing_keys = [key for key in required_keys if not os.getenv(key)]

    if missing_keys:
        print(f"❌ 缺少必要的环境变量: {', '.join(missing_keys)}")
        print("请确保 .env 文件包含所有必要的API密钥")
        exit(1)

    try:
        # 运行测试
        test_simple_base64_tts()  # 简单Base64 TTS测试
        test_video_reaction_base64()  # 视频反应Base64测试
        test_video_to_speech_base64()  # 完整流程Base64测试

        print("\n🎉 TTS集成测试完成！")
        print("💡 所有音频数据都以base64格式存储，可直接用于WebSocket传输")

    except KeyboardInterrupt:
        print("\n⚠️  用户中断测试")
    except Exception as e:
        print(f"\n❌ 测试过程中发生异常: {e}")
        import traceback

        traceback.print_exc()
</file>

<file path="src/ai_watch_buddy/tts_generator.py">
"""
TTS语音生成器模块
结合Gemini视频分析和ElevenLabs TTS服务
"""

import os
import json
import time
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from dotenv import load_dotenv

# 导入我们封装好的Gemini分析模块
from ai_watch_buddy.prompts.video_analyzer import invoke_gemini_vids


# 切换为MiniMax TTS
import requests
import base64
import tempfile


# 加载环境变量
load_dotenv()


class TTSGenerator:
    """TTS语音生成器类（MiniMax版）"""

    def __init__(self, api_key: Optional[str] = None, voice_id: Optional[str] = None):
        """
        初始化TTS生成器
        Args:
            api_key: MiniMax API密钥，如不提供则使用环境变量
            voice_id: 默认音色ID，如不提供则用官方默认
        """
        self.api_key = api_key or os.getenv("MINIMAX_API_KEY")
        if not self.api_key:
            raise ValueError("请设置MINIMAX_API_KEY环境变量或传入api_key参数")
        self.default_voice_id = voice_id or "female-zh"  # MiniMax官方中文女声
        print(f"✅ TTS生成器初始化成功 (MiniMax)")
        print(f"🎤 默认音色ID: {self.default_voice_id}")

    @property
    def default_model(self):
        # MiniMax TTS无模型概念，兼容旧接口
        return None

    def get_available_voices(self) -> List[Dict]:
        """获取MiniMax支持的音色列表（静态/可扩展）"""
        # 官方文档：https://www.minimax.chat/docs#/tts
        # 可根据API返回动态获取，这里静态列举常用音色
        return [
            {
                "voice_id": "female-zh",
                "name": "女声1",
                "category": "female",
                "description": "官方中文女声",
                "language": "zh",
            },
            {
                "voice_id": "male-zh",
                "name": "男声1",
                "category": "male",
                "description": "官方中文男声",
                "language": "zh",
            },
            {
                "voice_id": "female-en",
                "name": "女声2",
                "category": "female",
                "description": "英文女声",
                "language": "en",
            },
            {
                "voice_id": "male-en",
                "name": "男声2",
                "category": "male",
                "description": "英文男声",
                "language": "en",
            },
        ]

    def print_available_voices(self):
        """打印可用音色列表"""
        voices = self.get_available_voices()
        if voices:
            print("\n🎤 可用MiniMax音色列表:")
            print("=" * 80)
            for voice in voices:
                print(f"ID: {voice['voice_id']}")
                print(f"名称: {voice['name']}")
                print(f"类别: {voice['category']}")
                print(f"语言: {voice['language']}")
                print(f"描述: {voice['description']}")
                print("-" * 40)
        else:
            print("❌ 无法获取音色列表")

    def generate_single_audio(
        self,
        text: str,
        voice_id: Optional[str] = None,
        model_id: Optional[str] = None,
        output_path: Optional[str] = None,
    ) -> Optional[bytes]:
        """
        生成单条语音（MiniMax TTS）
        Args:
            text: 要转换的文本
            voice_id: 音色ID，如不提供则使用默认
            model_id: 保留参数，无实际作用
            output_path: 输出文件路径，如不提供则不保存
            play_audio: 是否播放音频
        Returns:
            音频数据字节流
        """
        try:
            voice = voice_id or self.default_voice_id
            print(f"🔄 生成语音: {text[:50]}{'...' if len(text) > 50 else ''}")
            print(f"🎤 使用MiniMax音色: {voice}")
            url = "https://api.minimax.com.cn/v1/tts"  # MiniMax TTS正式API地址
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            payload = {"text": text, "voice": voice, "format": "mp3"}
            # 跳过全局代理，直连MiniMax TTS
            resp = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=30,
                proxies={"http": None, "https": None},
            )
            if resp.status_code != 200:
                print(f"❌ MiniMax TTS API错误: {resp.status_code} {resp.text}")
                return None
            result = resp.json()
            audio_b64 = result.get("audio")
            if not audio_b64:
                print(f"❌ MiniMax TTS无音频返回: {result}")
                return None
            audio_bytes = base64.b64decode(audio_b64)
            if output_path:
                output_path = Path(output_path)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, "wb") as f:
                    f.write(audio_bytes)
                print(f"💾 音频已保存: {output_path}")
            return audio_bytes
        except Exception as e:
            print(f"❌ 语音生成失败: {e}")
            return None

    def process_video_analysis_to_speech(
        self,
        video_path: str,
        system_prompt: str,
        user_prompt: str,
        output_dir: str = "./tts_output",
        voice_id: Optional[str] = None,
        model_id: Optional[str] = None,
        gemini_api_key: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        处理视频分析结果并生成语音

        Args:
            video_path: 视频文件路径
            system_prompt: 系统提示词
            user_prompt: 用户提示词
            output_dir: 音频输出目录
            voice_id: 语音ID
            model_id: 模型ID
            play_audio: 是否播放音频
            gemini_api_key: Gemini API密钥

        Returns:
            处理结果字典
        """
        try:
            # 1. 使用Gemini分析视频
            print("🎬 开始视频分析...")
            analysis_result = invoke_gemini_vids(
                video_path=video_path,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                api_key=gemini_api_key,
            )

            if not analysis_result.get("success"):
                return {
                    "success": False,
                    "error": f"视频分析失败: {analysis_result.get('error', '未知错误')}",
                }

            # 2. 提取所有SPEAK动作的文本
            action_list = analysis_result["action_list"]
            speak_actions = [
                action for action in action_list if action.get("action_type") == "SPEAK"
            ]

            if not speak_actions:
                return {"success": False, "error": "未找到任何SPEAK动作"}

            print(f"🗣️  找到 {len(speak_actions)} 条语音动作")

            # 3. 创建输出目录
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            # 4. 生成语音文件
            generated_files = []
            successful_generations = 0

            for i, action in enumerate(speak_actions, 1):
                text = action.get("text", "").strip()
                if not text:
                    continue
                # 生成文件名
                timestamp = action.get("trigger_timestamp", 0)
                safe_text = (
                    text[:30].replace(" ", "_").replace("/", "_").replace("\\", "_")
                )
                filename = f"speak_{i:03d}_{timestamp}s_{safe_text}.mp3"
                file_path = output_path / filename
                # 生成语音
                audio_data = self.generate_single_audio(
                    text=text,
                    voice_id=voice_id,
                    model_id=model_id,
                    output_path=str(file_path),
                )
                if audio_data:
                    generated_files.append(
                        {
                            "id": action.get("id"),
                            "timestamp": timestamp,
                            "text": text,
                            "file_path": str(file_path),
                            "file_size": len(audio_data),
                        }
                    )
                    successful_generations += 1
                    print(f"✅ [{i}/{len(speak_actions)}] 生成完成")
                else:
                    print(f"❌ [{i}/{len(speak_actions)}] 生成失败")

            # 5. 生成元数据文件
            metadata = {
                "video_path": video_path,
                "total_speak_actions": len(speak_actions),
                "successful_generations": successful_generations,
                "generated_files": generated_files,
                "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "tts_settings": {"voice_id": voice_id or self.default_voice_id},
            }

            metadata_path = output_path / "audio_metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            print(f"\n✅ 语音生成完成！")
            print(f"📁 输出目录: {output_path}")
            print(
                f"🎵 成功生成: {successful_generations}/{len(speak_actions)} 个音频文件"
            )
            print(f"📋 元数据文件: {metadata_path}")

            return {
                "success": True,
                "output_dir": str(output_path),
                "total_actions": len(speak_actions),
                "successful_generations": successful_generations,
                "generated_files": generated_files,
                "metadata_path": str(metadata_path),
            }

        except Exception as e:
            print(f"❌ 处理失败: {e}")
            return {"success": False, "error": str(e)}


def create_tts_generator(
    api_key: Optional[str] = None, voice_id: Optional[str] = None
) -> TTSGenerator:
    """
    创建TTS生成器实例的便捷函数（MiniMax版）
    Args:
        api_key: MiniMax API密钥
        voice_id: 默认音色ID
    Returns:
        TTSGenerator实例
    """
    return TTSGenerator(api_key=api_key, voice_id=voice_id)


# 便捷函数


def quick_video_to_speech(
    video_path: str,
    system_prompt: str,
    user_prompt: str,
    output_dir: str = "./tts_output",
    minimax_api_key: Optional[str] = None,
    gemini_api_key: Optional[str] = None,
    voice_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    快速将视频分析结果转换为语音的便捷函数（MiniMax版）
    Args:
        video_path: 视频文件路径
        system_prompt: 系统提示词
        user_prompt: 用户提示词
        output_dir: 输出目录
        minimax_api_key: MiniMax API密钥
        gemini_api_key: Gemini API密钥
        voice_id: 音色ID
        play_audio: 是否播放音频
    Returns:
        处理结果字典
    """
    try:
        tts_gen = TTSGenerator(api_key=minimax_api_key, voice_id=voice_id)
        return tts_gen.process_video_analysis_to_speech(
            video_path=video_path,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            output_dir=output_dir,
            gemini_api_key=gemini_api_key,
        )
    except Exception as e:
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # 测试示例
    print("🎵 TTS生成器模块测试")

    # 检查环境变量
    if not os.getenv("ELEVENLABS_API_KEY"):
        print("❌ 请设置ELEVENLABS_API_KEY环境变量")
        exit(1)

    # 创建TTS生成器
    try:
        tts_gen = create_tts_generator()

        # 显示可用语音
        tts_gen.print_available_voices()

        # 测试单条语音生成
        test_text = "你好，这是一个测试语音。Hello, this is a test voice."
        print(f"\n🧪 测试语音生成: {test_text}")

        audio = tts_gen.generate_single_audio(
            text=test_text, output_path="./test_output/test_voice.mp3", play_audio=False
        )

        if audio:
            print("✅ 语音生成测试成功")
        else:
            print("❌ 语音生成测试失败")

    except Exception as e:
        print(f"❌ 测试失败: {e}")
</file>

<file path=".env.sample">
GEMINI_API_KEY=
FISH_AUDIO_API_KEY=
</file>

<file path="main.py">
import uvicorn
from dotenv import load_dotenv

load_dotenv()


def main():
    """
    Starts the AI Watch Buddy server.
    """
    print("Starting AI Watch Buddy server...")
    # The 'app' object is imported from server.py
    # "ai_watch_buddy.server:app" tells uvicorn where to find the FastAPI instance
    uvicorn.run("src.ai_watch_buddy.server:app", host="0.0.0.0", port=8000, reload=True)


if __name__ == "__main__":
    main()
</file>

<file path="src/ai_watch_buddy/session.py">
import asyncio
from typing import Literal, Optional, Union
from .agent.video_action_agent_interface import VideoActionAgentInterface
from .actions import Action


class SessionState:
    """Holds the state for a single watching session."""

    def __init__(
        self,
        session_id: str,
        character_id: str,
        video_url: str,
        character_prompt: str | None = None,
    ):
        self.session_id = session_id
        self.character_id = character_id
        self.video_url = video_url
        self.character_prompt = character_prompt
        self.local_video_path: str | None = None
        self.status: Literal[
            "created",
            "downloading_video",
            "video_ready",
            "generating_actions",
            "session_ready",
            "error",
        ] = "created"
        self.processing_error: str | None = None

        # 关键改动：为每个 session 实例创建一个 asyncio.Queue
        # 这个队列将作为生产者（pipeline）和消费者（websocket）之间的桥梁
        self.action_queue: asyncio.Queue[Optional[Union[Action, dict]]] = (
            asyncio.Queue()
        )
        self.agent: Optional[VideoActionAgentInterface] = None
        self.action_generation_task: Optional[asyncio.Task] = None


# A simple in-memory "database" for sessions
# This dictionary is now the single source of truth for all session states.
session_storage: dict[str, SessionState] = {}
</file>

<file path="pyproject.toml">
[project]
name = "ai_watch_buddy"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "edge-tts>=7.0.2",
    "fastapi>=0.116.1",
    "fish-audio-sdk>=2025.6.3",
    "google-genai>=1.27.0",
    "json-repair>=0.47.8",
    "loguru>=0.7.3",
    "numpy>=2.3.1",
    "pydantic>=2.11.7",
    "python-dotenv>=1.1.1",
    "ruff>=0.12.4",
    "uvicorn[standard]>=0.30.1", # Added for running the server
    "websockets>=12.0", # Explicitly add for websocket handling
    "yt-dlp>=2025.7.21",
]
</file>

<file path="src/ai_watch_buddy/pipeline.py">
import json
import asyncio
from loguru import logger
from typing import Optional
import os
from dotenv import load_dotenv

from .actions import Action, SpeakAction
from .tts.fish_audio_tts import FishAudioTTSEngine
from .session import session_storage
from .fetch_video import download_video_async
from .agent.video_analyzer_agent import VideoAnalyzerAgent
from .prompts.character_prompts import cute_prompt

load_dotenv()


def get_interruption_timestamp(user_action_list: list[Action]) -> Optional[float]:
    """Extracts the interruption timestamp from the user action list."""
    if user_action_list:
        # The timestamp of the first user action is the definitive point of interruption.
        return user_action_list[0].trigger_timestamp
    return None


async def run_conversation_pipeline(
    session_id: str, user_action_list: list[Action], pending_action_list: list[Action]
) -> None:
    """
    Handles a user interruption by sending a concise "Situation Report" to the agent.
    The agent's System Prompt contains all the logic for how to handle this report.
    """
    session = session_storage.get(session_id)
    if not session or not session.agent:
        logger.error(
            f"[{session_id}] Cannot run conversation: session or agent not found."
        )
        return

    interruption_timestamp = get_interruption_timestamp(user_action_list)
    if interruption_timestamp is None:
        logger.warning(
            f"[{session_id}] Could not determine interruption timestamp. Defaulting to 0."
        )
        interruption_timestamp = 0.0

    # This context_message is a simple, clean data report.
    # All the complex instructions have been moved to the System Prompt in action_gen.py.
    context_message = f"""
## User Interruption Report

- **Interruption Timestamp:** {interruption_timestamp} (The video is PAUSED at this time)
- **User Actions:**
{json.dumps([action.model_dump() for action in user_action_list], indent=2)}

- **Your Cancelled Actions:**
{json.dumps([action.model_dump() for action in pending_action_list], indent=2)}

Based on this report and your core instructions, generate your new Reaction Script now.
"""

    session.agent.add_content(role="user", text=context_message)

    # The rest of the logic remains the same.
    session.action_generation_task = asyncio.create_task(
        generate_and_queue_actions(
            session_id, mode="summary", clear_pending_actions=True
        )
    )
    logger.info(
        f"[{session_id}] Sent interruption report for timestamp {interruption_timestamp}. New generation task created."
    )


async def generate_and_queue_actions(
    session_id: str,
    mode: str,
    clear_pending_actions: bool = True,
) -> None:
    """
    Generic function to generate actions using the session's agent and put them in the queue.

    Args:
        session_id: The session identifier
        mode: The generation mode (e.g., "video", "summary")
        clear_pending_actions: Whether to clear existing pending actions
    """
    session = session_storage.get(session_id)
    if not session or not session.agent:
        logger.error(
            f"[{session_id}] Cannot generate actions: session or agent not found."
        )
        return

    try:
        if clear_pending_actions:
            while not session.action_queue.empty():
                session.action_queue.get_nowait()
            logger.info(f"[{session_id}] Cleared pending actions from the queue.")

        session.status = "generating_actions"
        logger.info(f"[{session_id}] Starting action generation with mode '{mode}'...")

        actions_generated_count = 0

        action_source = session.agent.produce_action_stream(mode=mode)

        async for action in action_source:
            # Handle both Action objects (from mock) and dict (from agent)

            # 这个回来一定是个 Action 对象，所以不用 validate 了
            # if isinstance(action_data, Action):
            #     action = action_data
            # else:
            #     action = Action.model_validate(action_data)

            # Generate audio for SpeakAction
            if isinstance(action, SpeakAction):
                # Initialize Fish Audio TTS - you'll need to provide your API key
                tts_instance = FishAudioTTSEngine(
                    api_key=os.getenv("FISH_AUDIO_API_KEY")
                )
                # tts_instance = TTSEngine()
                audio_base64 = await tts_instance.generate_audio(action.text)
                if audio_base64:
                    action.audio = audio_base64
                else:
                    logger.warning(
                        f"[{session_id}] Failed to generate audio for action: {action.id}"
                    )

            await session.action_queue.put(action)
            actions_generated_count += 1
            logger.info(
                f"[{session_id}] Queued action: {action.action_type} at {action.trigger_timestamp}s"
            )

        logger.info(
            f"[{session_id}] Action generation stream finished. Total new actions: {actions_generated_count}."
        )

    except asyncio.CancelledError:
        logger.info(f"[{session_id}] Action generation task was cancelled.")
    except Exception as e:
        logger.error(
            f"[{session_id}] Error during action generation: {e}", exc_info=True
        )
        session.status = "error"
        session.processing_error = str(e)
        error_payload = {
            "type": "processing_error",
            "error_code": "ACTION_GENERATION_FAILED",
            "message": str(e),
        }
        await session.action_queue.put(error_payload)
    finally:
        await session.action_queue.put(None)


async def run_initial_generation(session_id: str):
    """
    Runs initial action generation and summary generation in parallel,
    then sets session_ready when both are complete.

    Args:
        session_id: The session identifier
    """
    session = session_storage.get(session_id)
    if not session or not session.agent:
        logger.error(
            f"[{session_id}] Session or agent not found in run_initial_generation"
        )
        return

    try:
        # Determine the video input for the agent: use local path if available, otherwise use original URL.
        video_input = (
            session.local_video_path if session.local_video_path else session.video_url
        )

        logger.info(
            f"[{session_id}] Starting parallel summary and action generation for: {video_input}"
        )

        # Create both tasks to run in parallel
        summary_task = asyncio.create_task(
            session.agent.get_video_summary(video_path_or_url=video_input)
        )

        action_generation_task = asyncio.create_task(
            generate_and_queue_actions(
                session_id, mode="video", clear_pending_actions=False
            )
        )

        # Wait for both tasks to complete
        await asyncio.gather(summary_task, action_generation_task)

        logger.info(f"[{session_id}] Both summary and action generation completed.")

        if not session.agent.summary_ready:
            raise RuntimeError(
                "Agent summary was not ready after summary task completion."
            )

        # Set session ready only after both tasks complete successfully
        if session.status != "error":
            session.status = "session_ready"
            logger.info(
                f"[{session_id}] ✅ Initial pipeline complete. Status set to 'session_ready'."
            )

    except Exception as e:
        logger.error(
            f"[{session_id}] Error during initial generation: {e}", exc_info=True
        )
        session.status = "error"
        session.processing_error = str(e)
        error_payload = {
            "type": "processing_error",
            "error_code": "INITIAL_GENERATION_FAILED",
            "message": str(e),
        }
        await session.action_queue.put(error_payload)
        await session.action_queue.put(None)


async def initial_pipeline(session_id: str) -> None:
    """
    The initial background task that runs when a session is created.
    """
    session = session_storage.get(session_id)
    if not session:
        return

    try:
        # Step 1: Check video URL and download if necessary
        video_url = session.video_url
        is_youtube_url = "youtube.com" in video_url or "youtu.be" in video_url

        if is_youtube_url:
            logger.info(
                f"[{session_id}] YouTube URL detected, skipping download: {video_url}"
            )
            session.local_video_path = None
        else:
            session.status = "downloading_video"
            logger.info(
                f"[{session_id}] Non-YouTube URL detected, downloading from: {video_url}"
            )
            local_video_path = str(
                await download_video_async(session.video_url, target_dir="video_cache")
            )
            session.local_video_path = local_video_path
            logger.info(
                f"[{session_id}] Video downloaded and ready at: {local_video_path}"
            )

        session.status = "video_ready"

        # Step 2: Initialize the agent
        persona = session.character_prompt or cute_prompt
        agent = VideoAnalyzerAgent(persona_prompt=persona)
        session.agent = agent
        logger.info(f"[{session_id}] Agent initialized with persona.")

        # Step 3: Start parallel summary and action generation in the background.
        session.action_generation_task = asyncio.create_task(
            run_initial_generation(session_id)
        )
        logger.info(
            f"[{session_id}] Summary and action generation started in the background."
        )

    except Exception as e:
        logger.error(
            f"[{session_id}] Error during initial pipeline setup: {e}", exc_info=True
        )
        session.status = "error"
        session.processing_error = str(e)
        if session:
            error_payload = {
                "type": "processing_error",
                "error_code": "INITIAL_PIPELINE_FAILED",
                "message": str(e),
            }
            await session.action_queue.put(error_payload)
            await session.action_queue.put(None)
</file>

<file path="src/ai_watch_buddy/server.py">
import uuid
import asyncio
from loguru import logger

from fastapi import (
    FastAPI,
    WebSocket,
    BackgroundTasks,
    status,
    WebSocketDisconnect,
)
from fastapi.middleware.cors import CORSMiddleware
from starlette.staticfiles import StaticFiles as StarletteStaticFiles
from pydantic import BaseModel, ValidationError

from .session import SessionState, session_storage
from .pipeline import (
    initial_pipeline,
    generate_and_queue_actions,
    run_conversation_pipeline,
)
from .actions import Action, UserInteractionPayload
from .connection_manager import manager

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- Data Models for API ---
class SessionCreateRequest(BaseModel):
    video_url: str
    start_time: float = 0.0
    end_time: float | None = None
    text: str | None = None
    character_id: str
    user_id: str | None = None


class SessionCreateResponse(BaseModel):
    session_id: str


class ErrorResponse(BaseModel):
    error: str
    message: str


# --- Connection Management ---
# The ConnectionManager is now in its own file (connection_manager.py)
# to prevent circular dependencies. The `manager` instance is imported from there.


class CORSStaticFiles(StarletteStaticFiles):
    """
    Static files handler that adds CORS headers to all responses.
    Needed because Starlette StaticFiles might bypass standard middleware.
    """

    async def get_response(self, path: str, scope):
        response = await super().get_response(path, scope)

        # Add CORS headers to all responses
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "*"

        if path.endswith(".js"):
            response.headers["Content-Type"] = "application/javascript"

        return response


app.mount(
    "/live2d-models",
    CORSStaticFiles(directory="live2d-models"),
    name="live2d-models",
)


# --- API Endpoint ---
@app.post(
    "/api/v1/sessions",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=SessionCreateResponse,
)
async def create_session(
    request: SessionCreateRequest, background_tasks: BackgroundTasks
):
    """
    Creates a new watching session, starts background processing,
    and returns a session_id.
    """
    session_id = f"ses_{uuid.uuid4().hex[:16]}"

    # Create the session state object and store it
    session = SessionState(
        session_id=session_id,
        character_id=request.character_id,
        video_url=request.video_url,
        character_prompt=request.text,
    )
    session_storage[session_id] = session

    # Start the processing pipeline in the background
    background_tasks.add_task(initial_pipeline, session_id=session_id)

    logger.info(f"Accepted session {session_id} for video {request.video_url}")
    return SessionCreateResponse(session_id=session_id)


async def websocket_sender(websocket: WebSocket, session: SessionState):
    """Consumer coroutine: Gets actions from the queue and sends them to the client."""
    # ✅ NEW: Simplified and more accurate waiting logic.
    # It now waits for the single source of truth: the session status.
    while session.status not in ["session_ready", "error"]:
        await asyncio.sleep(0.1)

    if session.status == "error":
        await websocket.send_json(
            {
                "type": "processing_error",
                "error_code": "INITIAL_PIPELINE_FAILED",
                "message": session.processing_error,
            }
        )
        return

    # This message is now sent ONLY after the summary is ready AND the first batch of actions is in the queue.
    await websocket.send_json({"type": "session_ready"})
    logger.info(f"[{session.session_id}] Sent 'session_ready' to client.")

    while True:
        item = await session.action_queue.get()
        if item is None:
            logger.info(f"[{session.session_id}] Reached end of an action batch.")
            session.action_queue.task_done()
            continue

        if isinstance(item, Action):
            await websocket.send_json(
                {"type": "ai_action", "action": item.model_dump(mode="json")}
            )
        elif isinstance(item, dict) and item.get("type") == "processing_error":
            await websocket.send_json(item)

        session.action_queue.task_done()


# ... (The rest of the file, including websocket_receiver and websocket_endpoint, remains the same as my previous answer) ...
async def clear_action_queue(queue: asyncio.Queue):
    """Helper to empty an asyncio queue."""
    while not queue.empty():
        try:
            queue.get_nowait()
        except asyncio.QueueEmpty:
            break


async def websocket_receiver(websocket: WebSocket, session: SessionState):
    """Receiver coroutine: Listens for messages from the client and triggers backend logic."""
    async for message in websocket.iter_json():
        msg_type = message.get("type")
        logger.info(
            f"[{session.session_id}] Received message from client: type={msg_type}"
        )

        # Always interrupt any ongoing task before starting a new one.
        if session.action_generation_task and not session.action_generation_task.done():
            logger.info(
                f"[{session.session_id}] Cancelling previous action generation task."
            )
            session.action_generation_task.cancel()
            await clear_action_queue(
                session.action_queue
            )  # Clear out any partially generated actions

        if msg_type == "interrupt":
            logger.info(
                f"[{session.session_id}] Client sent interrupt. Task cancelled, queue cleared."
            )
            # The logic at the start of the loop already handles this.

        elif msg_type == "trigger-load-next":
            logger.info(
                f"[{session.session_id}] Client triggered lazy-load for next actions."
            )
            session.action_generation_task = asyncio.create_task(
                generate_and_queue_actions(
                    session.session_id,
                    mode="summary",
                    clear_pending_actions=True,
                )
            )

        elif msg_type == "trigger-conversation":
            try:
                # Assuming the payload is in message['data']
                payload = UserInteractionPayload.model_validate(message.get("data", {}))
                logger.info(f"[{session.session_id}] Client triggered conversation.")
                # No need to store task here, run_conversation_pipeline will do it.
                asyncio.create_task(
                    run_conversation_pipeline(
                        session.session_id,
                        user_action_list=payload.user_action_list,
                        pending_action_list=payload.pending_action_list,
                    )
                )
            except ValidationError as e:
                logger.error(
                    f"[{session.session_id}] Invalid conversation payload: {e}"
                )
                await websocket.send_json(
                    {
                        "type": "error",
                        "message": "Invalid payload for trigger-conversation.",
                    }
                )


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """Main WebSocket endpoint that manages the sender and receiver tasks."""
    # This function remains the same. I'm including it for completeness.
    session = session_storage.get(session_id)
    if not session:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        logger.warning(
            f"WebSocket connection rejected for unknown session: {session_id}"
        )
        return

    await manager.connect(websocket, session_id)
    logger.info(f"[{session_id}] WebSocket connection established.")

    sender_task = asyncio.create_task(websocket_sender(websocket, session))
    receiver_task = asyncio.create_task(websocket_receiver(websocket, session))

    try:
        await asyncio.gather(sender_task, receiver_task)
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] Client disconnected.")
    except Exception as e:
        logger.error(
            f"[{session_id}] An error occurred in the websocket endpoint: {e}",
            exc_info=True,
        )
    finally:
        sender_task.cancel()
        receiver_task.cancel()
        if session.action_generation_task:
            session.action_generation_task.cancel()
        manager.disconnect(session_id)
        logger.info(f"[{session_id}] WebSocket connection closed and cleaned up.")
</file>

</files>
