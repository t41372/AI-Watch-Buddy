This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  hooks/
    use-action-executor.ts
    use-action-queue.ts
    use-audio-task.ts
    use-draggable.ts
    use-live2d-expression.ts
    use-live2d-model.ts
    use-live2d-resize.ts
    use-message-handler.ts
    use-session.ts
    use-websocket.ts
  lib/
    react-player-config.ts
    utils.ts
  types/
    video-player.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/hooks/use-action-executor.ts">
import { useCallback, useRef, useState } from 'react';
import { AIAction } from './use-message-handler';
import { useAudioTask } from './use-audio-task';

export interface VideoPlayerControl {
  pause: () => void;
  resume: () => void;
  seek: (time: number) => void;
  getCurrentTime: () => number;
  getDuration: () => number;
  showPauseOverlay: () => void;
  hidePauseOverlay: () => void;
  showSpeakingOverlay: () => void;
  hideSpeakingOverlay: () => void;
  animateSeek: (targetTime: number, duration?: number) => Promise<void>;
}

export interface ActionExecutorOptions {
  videoPlayerControl?: VideoPlayerControl;
  onActionExecuted?: (action: AIAction) => void;
  onExecutionError?: (error: Error, action: AIAction) => void;
}

export interface UseActionExecutorReturn {
  executeAction: (action: AIAction) => Promise<void>;
  executeActions: (actions: AIAction[]) => Promise<void>;
  isExecuting: boolean;
}

export const useActionExecutor = (options: ActionExecutorOptions = {}): UseActionExecutorReturn => {
  const { videoPlayerControl, onActionExecuted, onExecutionError } = options;
  const { addAudioTask, stopCurrentAudioAndLipSync } = useAudioTask();
  
  const [isExecuting, setIsExecuting] = useState(false);

  // Execute a single action
  const executeAction = useCallback(async (action: AIAction): Promise<void> => {
    console.log(`Executing action: ${action.action_type} at ${action.trigger_timestamp}s`, action);

    try {
      switch (action.action_type) {
        case 'PAUSE':
          await executePauseAction(action);
          break;

        case 'SPEAK':
          await executeSpeakAction(action);
          break;

        case 'SEEK':
          await executeSeekAction(action);
          break;

        case 'REPLAY_SEGMENT':
          await executeReplaySegmentAction(action);
          break;

        case 'END_REACTION':
          await executeEndReactionAction(action);
          break;

        default:
          console.warn(`Unknown action type: ${action.action_type}`);
          break;
      }

      console.log(`Successfully executed action: ${action.id}`);
      // Mark action as executed after successful completion
      onActionExecuted?.(action);

    } catch (error) {
      console.error(`Failed to execute action ${action.id}:`, error);
      onExecutionError?.(error as Error, action);
      // Still mark as executed to prevent retry
      onActionExecuted?.(action);
    }
  }, [videoPlayerControl, onActionExecuted, onExecutionError]);

  // Execute multiple actions (typically with same trigger_time)
  const executeActions = useCallback(async (actions: AIAction[]): Promise<void> => {
    if (actions.length === 0) return;

    setIsExecuting(true);

    try {
      console.log(`Executing ${actions.length} actions in sequence`);
      
      // Execute actions in the correct order based on their priority
      // PAUSE -> SEEK -> REPLAY_SEGMENT -> SPEAK -> END_REACTION
      const sortedActions = [...actions].sort((a, b) => {
        const priorityOrder = { 'PAUSE': 0, 'SEEK': 1, 'REPLAY_SEGMENT': 2, 'SPEAK': 3, 'END_REACTION': 4 };
        const aPriority = priorityOrder[a.action_type] ?? 99;
        const bPriority = priorityOrder[b.action_type] ?? 99;
        return aPriority - bPriority;
      });

      for (const action of sortedActions) {
        await executeAction(action);
        
        // Small delay between actions if needed
        if (sortedActions.indexOf(action) < sortedActions.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 10));
        }
      }

    } finally {
      setIsExecuting(false);
    }
  }, [executeAction]);

  // Individual action executors
  const executePauseAction = useCallback(async (action: AIAction): Promise<void> => {
    if (!videoPlayerControl) {
      console.warn('No video player control available for PAUSE action');
      return;
    }

    const duration = action.duration_seconds || 1.0; // Default to 1 second if not specified
    console.log(`Pausing video for ${duration}s for action: ${action.comment}`);

    // Show pause overlay and pause the video
    videoPlayerControl.showPauseOverlay();
    videoPlayerControl.pause();

    // Wait for the specified duration, then resume
    await new Promise(resolve => setTimeout(resolve, duration * 1000));
    
    // Hide overlay and resume
    videoPlayerControl.hidePauseOverlay();
    videoPlayerControl.resume();

    console.log(`Resumed video after ${duration}s pause`);
  }, [videoPlayerControl]);

  const executeSpeakAction = useCallback(async (action: AIAction): Promise<void> => {
    if (!action.text) {
      console.warn('SPEAK action missing text content');
      return;
    }

    const shouldPauseVideo = action.pause_video !== false; // Default to true if not specified
    console.log(`AI speaking: "${action.text}" (pause_video: ${shouldPauseVideo})`);

    // Stop any current audio to prevent overlapping
    stopCurrentAudioAndLipSync();

    // Store the current video time and playing state
    let videoWasPlaying = false;
    let currentVideoTime = 0;
    
    // Show speaking overlay
    if (videoPlayerControl) {
      videoPlayerControl.showSpeakingOverlay();
    }
    
    // Pause video if requested
    if (shouldPauseVideo && videoPlayerControl) {
      // Check if video was playing before we pause it
      currentVideoTime = videoPlayerControl.getCurrentTime();
      // Assume video was playing if we're pausing it for speech
      videoWasPlaying = true;
      videoPlayerControl.pause();
    }

    // If audio data is available, use it for lip sync
    if (action.audio) {
      try {
        await addAudioTask({
          audioBase64: action.audio,
          volumes: [], // Will be calculated by the audio task
          sliceLength: 0.1, // Default slice length
          expressions: null // Could be extended to support expressions
        });
        console.log('Audio playback with lip sync completed');
      } catch (error) {
        console.error('Failed to play audio with lip sync:', error);
        // Fallback: just display the text
        console.log(`Fallback text display: "${action.text}"`);
      }
    } else {
      // No audio available, just display text
      console.log(`Text-only speech: "${action.text}"`);
      // Simulate a delay for text-only speech
      await new Promise(resolve => setTimeout(resolve, 2000));
    }

    // Hide speaking overlay
    if (videoPlayerControl) {
      videoPlayerControl.hideSpeakingOverlay();
    }

    // Resume video if it was paused and was playing before
    if (shouldPauseVideo && videoPlayerControl && videoWasPlaying) {
      // Ensure we're at the right position (in case something changed)
      const expectedTime = currentVideoTime + (action.duration_seconds || 0);
      const actualTime = videoPlayerControl.getCurrentTime();
      
      // If the video time hasn't advanced as expected, seek to where it should be
      if (Math.abs(actualTime - expectedTime) > 0.1) {
        console.log(`Video time mismatch after speech. Expected: ${expectedTime}s, Actual: ${actualTime}s`);
        // Don't seek - let the video continue from where it is
      }
      
      console.log('Resuming video playback after speech');
      videoPlayerControl.resume();
    }
  }, [addAudioTask, stopCurrentAudioAndLipSync, videoPlayerControl]);

  const executeSeekAction = useCallback(async (action: AIAction): Promise<void> => {
    if (!videoPlayerControl) {
      console.warn('No video player control available for SEEK action');
      return;
    }

    if (action.target_timestamp === undefined) {
      console.warn('SEEK action missing target_timestamp');
      return;
    }

    const duration = videoPlayerControl.getDuration();
    const targetTime = Math.max(0, Math.min(action.target_timestamp, duration));
    const currentTime = videoPlayerControl.getCurrentTime();
    
    // Calculate seek duration based on distance (300ms per 10 seconds, min 500ms, max 2000ms)
    const distance = Math.abs(targetTime - currentTime);
    const seekDuration = Math.max(500, Math.min(2000, distance * 30));

    console.log(`Animating seek from ${currentTime}s to ${targetTime}s over ${seekDuration}ms`);
    
    // Use animated seek instead of instant seek
    await videoPlayerControl.animateSeek(targetTime, seekDuration);

    // Handle post-seek behavior
    const postSeekBehavior = action.post_seek_behavior || 'STAY_PAUSED';
    if (postSeekBehavior === 'RESUME_PLAYBACK') {
      console.log('Resuming playback after seek');
      videoPlayerControl.resume();
    } else {
      console.log('Staying paused after seek');
    }
  }, [videoPlayerControl]);

  const executeReplaySegmentAction = useCallback(async (action: AIAction): Promise<void> => {
    if (!videoPlayerControl) {
      console.warn('No video player control available for REPLAY_SEGMENT action');
      return;
    }

    if (action.start_timestamp === undefined || action.end_timestamp === undefined) {
      console.warn('REPLAY_SEGMENT action missing start_timestamp or end_timestamp');
      return;
    }

    const duration = videoPlayerControl.getDuration();
    const startTime = Math.max(0, Math.min(action.start_timestamp, duration));
    const endTime = Math.max(startTime, Math.min(action.end_timestamp, duration));
    const originalTime = videoPlayerControl.getCurrentTime();

    console.log(`Replaying segment from ${startTime}s to ${endTime}s (original position: ${originalTime}s)`);

    // Animate seek to start of segment
    const seekDuration = Math.max(500, Math.min(1500, Math.abs(originalTime - startTime) * 30));
    await videoPlayerControl.animateSeek(startTime, seekDuration);
    videoPlayerControl.resume();

    // Wait for the segment to play
    const segmentDuration = endTime - startTime;
    await new Promise(resolve => setTimeout(resolve, segmentDuration * 1000));

    // Handle post-replay behavior
    const postReplayBehavior = action.post_replay_behavior || 'RESUME_FROM_ORIGINAL';
    if (postReplayBehavior === 'RESUME_FROM_ORIGINAL') {
      console.log(`Returning to original position: ${originalTime}s`);
      const returnDuration = Math.max(500, Math.min(1500, Math.abs(endTime - originalTime) * 30));
      await videoPlayerControl.animateSeek(originalTime, returnDuration);
      videoPlayerControl.resume();
    } else {
      console.log(`Staying at end of segment: ${endTime}s`);
      await videoPlayerControl.animateSeek(endTime, 300);
      videoPlayerControl.pause();
    }
  }, [videoPlayerControl]);

  const executeEndReactionAction = useCallback(async (action: AIAction): Promise<void> => {
    console.log(`End reaction reached: ${action.comment}`);

    // Stop any ongoing audio
    stopCurrentAudioAndLipSync();

    // Pause the video to wait for next user input or trigger
    if (videoPlayerControl) {
      videoPlayerControl.pause();
    }

    console.log('Reaction sequence ended, waiting for next trigger');
  }, [videoPlayerControl, stopCurrentAudioAndLipSync]);

  return {
    executeAction,
    executeActions,
    isExecuting
  };
};
</file>

<file path="src/hooks/use-action-queue.ts">
import { useState, useEffect, useCallback, useRef } from 'react';
import { AIAction } from './use-message-handler';
import { WebSocketMessage } from './use-websocket';

export interface ActionQueueOptions {
  threshold?: number; // Time threshold in seconds to request more actions
  sendMessage?: (message: WebSocketMessage) => void;
}

export interface UseActionQueueReturn {
  actionQueue: AIAction[];
  currentActionIndex: number;
  isExecuting: boolean;
  latestTriggerTime: number;
  addActions: (actions: AIAction[]) => void;
  clearQueue: () => void;
  resetQueue: () => void;
  getNextActions: (currentTime: number) => AIAction[];
  removeActions: (actionIds: string[]) => void;
  markActionExecuted: (actionId: string) => void;
  checkThreshold: (currentTime: number) => void;
}

export const useActionQueue = (options: ActionQueueOptions = {}): UseActionQueueReturn => {
  const { threshold = 30, sendMessage } = options; // Default 30 seconds threshold
  
  const [actionQueue, setActionQueue] = useState<AIAction[]>([]);
  const [executedActionIds, setExecutedActionIds] = useState<Set<string>>(new Set());
  const [isExecuting, setIsExecuting] = useState(false);
  const [hasRequestedMore, setHasRequestedMore] = useState(false);

  // Calculate current action index based on executed actions
  const currentActionIndex = actionQueue.findIndex(action => !executedActionIds.has(action.id));

  // Get latest trigger time from the queue
  const latestTriggerTime = actionQueue.length > 0
    ? Math.max(...actionQueue.map(action => action.trigger_timestamp))
    : 0;

  // Add new actions to the queue
  const addActions = useCallback((actions: AIAction[]) => {
    if (actions.length === 0) return;

    setActionQueue(prevQueue => {
      // Combine and sort all actions by trigger_timestamp
      const newQueue = [...prevQueue, ...actions];
      const sortedQueue = newQueue.sort((a, b) => {
        // Primary sort by trigger_timestamp
        if (a.trigger_timestamp !== b.trigger_timestamp) {
          return a.trigger_timestamp - b.trigger_timestamp;
        }
        // Secondary sort by action type priority for same trigger_timestamp
        // PAUSE -> SEEK -> REPLAY_SEGMENT -> SPEAK -> END_REACTION
        const priorityOrder = { 'PAUSE': 0, 'SEEK': 1, 'REPLAY_SEGMENT': 2, 'SPEAK': 3, 'END_REACTION': 4 };
        const aPriority = priorityOrder[a.action_type] ?? 99;
        const bPriority = priorityOrder[b.action_type] ?? 99;
        return aPriority - bPriority;
      });

      console.log(`Added ${actions.length} actions to queue. New queue size: ${sortedQueue.length}`);
      return sortedQueue;
    });

    // Reset the request flag when new actions arrive
    setHasRequestedMore(false);
  }, []);

  // Remove actions from queue by their IDs
  const removeActions = useCallback((actionIds: string[]) => {
    if (actionIds.length === 0) return;
    
    const idsToRemove = new Set(actionIds);
    setActionQueue(prevQueue => {
      const newQueue = prevQueue.filter(action => !idsToRemove.has(action.id));
      console.log(`Removed ${actionIds.length} actions from queue. New queue size: ${newQueue.length}`);
      return newQueue;
    });
  }, []);

  // Clear all actions from queue
  const clearQueue = useCallback(() => {
    setActionQueue([]);
    setExecutedActionIds(new Set());
    setIsExecuting(false);
    setHasRequestedMore(false);
  }, []);

  // Reset queue state but keep actions
  const resetQueue = useCallback(() => {
    setExecutedActionIds(new Set());
    setIsExecuting(false);
    setHasRequestedMore(false);
  }, []);

  // Get actions that should be executed at current time
  const getNextActions = useCallback((currentTime: number): AIAction[] => {
    // No tolerance - actions can be late but not early
    const actionsToExecute = actionQueue.filter(action => {
      const shouldExecute = !executedActionIds.has(action.id) &&
                           action.trigger_timestamp <= currentTime;
      return shouldExecute;
    });

    return actionsToExecute;
  }, [actionQueue, executedActionIds]);

  // Mark an action as executed
  const markActionExecuted = useCallback((actionId: string) => {
    setExecutedActionIds(prev => {
      const newSet = new Set(prev);
      newSet.add(actionId);
      return newSet;
    });
  }, []);

  // Threshold checking for requesting more actions
  const checkThreshold = useCallback((currentTime: number) => {
    // Don't request if we already requested or no sendMessage function
    if (hasRequestedMore || !sendMessage) {
      return;
    }

    // Check if we need more actions
    const timeUntilEnd = latestTriggerTime - currentTime;
    
    if (timeUntilEnd <= threshold && timeUntilEnd > 0) {
      console.log(`Requesting more actions: current=${currentTime}, latest=${latestTriggerTime}, threshold=${threshold}`);
      
      setHasRequestedMore(true);
      sendMessage({
        type: "trigger-load-next",
      });
    }
  }, [latestTriggerTime, threshold, hasRequestedMore, sendMessage]);

  return {
    actionQueue,
    currentActionIndex,
    isExecuting,
    latestTriggerTime,
    addActions,
    clearQueue,
    resetQueue,
    getNextActions,
    removeActions,
    markActionExecuted,
    checkThreshold
  };
};
</file>

<file path="src/hooks/use-audio-task.ts">
'use client';

/* eslint-disable func-names */
/* eslint-disable no-underscore-dangle */
/* eslint-disable @typescript-eslint/ban-ts-comment */
import { useRef, useCallback } from 'react';
import { useLive2DExpression } from '@/hooks/use-live2d-expression';
import * as LAppDefine from '@cubismsdksamples/lappdefine';

// Simple type alias for Live2D model
type Live2DModel = any;

interface AudioTaskOptions {
  audioBase64: string
  volumes: number[]
  sliceLength: number
  expressions?: string[] | number[] | null
}

/**
 * Custom hook for handling audio playback tasks with Live2D lip sync
 */
export const useAudioTask = () => {
  const { setExpression } = useLive2DExpression();

  // Track current audio and model
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const currentModelRef = useRef<Live2DModel | null>(null);

  /**
   * Stop current audio playback and lip sync
   */
  const stopCurrentAudioAndLipSync = useCallback(() => {
    if (currentAudioRef.current) {
      console.log('Stopping current audio and lip sync');
      const audio = currentAudioRef.current;
      audio.pause();
      audio.src = '';
      audio.load();

      const model = currentModelRef.current;
      if (model && model._wavFileHandler) {
        try {
          // Release PCM data to stop lip sync calculation in update()
          model._wavFileHandler.releasePcmData();
          console.log('Called _wavFileHandler.releasePcmData()');

          // Additional reset of state variables as fallback
          model._wavFileHandler._lastRms = 0.0;
          model._wavFileHandler._sampleOffset = 0;
          model._wavFileHandler._userTimeSeconds = 0.0;
          console.log('Also reset _lastRms, _sampleOffset, _userTimeSeconds as fallback');
        } catch (e) {
          console.error('Error stopping/resetting wavFileHandler:', e);
        }
      } else if (model) {
        console.warn('Current model does not have _wavFileHandler to stop/reset.');
      } else {
        console.log('No associated model found to stop lip sync.');
      }

      currentAudioRef.current = null;
      currentModelRef.current = null;
    } else {
      console.log('No current audio playing to stop.');
    }
  }, []);

  /**
   * Handle audio playback with Live2D lip sync
   */
  const handleAudioPlayback = (options: AudioTaskOptions): Promise<void> => new Promise((resolve) => {
    const { audioBase64, expressions } = options;

    try {
      // Process audio if available
      if (audioBase64) {
        // Change the MIME type to audio/mp3 which is more widely supported
        const audioDataUrl = `data:audio/wav;base64,${audioBase64}`;

        // Get Live2D manager and model
        const live2dManager = (window as any).getLive2DManager?.();
        if (!live2dManager) {
          console.error('Live2D manager not found');
          resolve();
          return;
        }

        const model = live2dManager.getModel(0);
        if (!model) {
          console.error('Live2D model not found at index 0');
          resolve();
          return;
        }
        console.log('Found model for audio playback');
        currentModelRef.current = model;

        if (!model._wavFileHandler) {
          console.warn('Model does not have _wavFileHandler for lip sync');
        } else {
          console.log('Model has _wavFileHandler available');
        }

        // Set expression if available
        const lappAdapter = (window as any).getLAppAdapter?.();
        if (lappAdapter && expressions?.[0] !== undefined) {
          setExpression(
            expressions[0],
            lappAdapter,
            `Set expression to: ${expressions[0]}`,
          );
        }

        // Start talk motion
        if (LAppDefine && LAppDefine.PriorityNormal) {
          console.log("Starting random 'Talk' motion");
          model.startRandomMotion(
            "Talk",
            LAppDefine.PriorityNormal,
          );
        } else {
          console.warn("LAppDefine.PriorityNormal not found - cannot start talk motion");
        }

        // Setup audio element
        const audio = new Audio();
        currentAudioRef.current = audio;
        let isFinished = false;

        const cleanup = () => {
          if (currentAudioRef.current === audio) {
            currentAudioRef.current = null;
            currentModelRef.current = null;
          }
          if (!isFinished) {
            isFinished = true;
            resolve();
          }
        };

        // Enhance lip sync sensitivity
        const lipSyncScale = 2.0;

        audio.addEventListener('canplaythrough', () => {
          // Check for interruption before playback
          if (currentAudioRef.current !== audio) {
            console.warn('Audio playback cancelled due to new audio');
            cleanup();
            return;
          }

          console.log('Starting audio playback with lip sync');
          audio.play().catch((err) => {
            console.error("Audio play error:", err);
            cleanup();
          });

          // Setup lip sync
          if (model._wavFileHandler) {
            if (!model._wavFileHandler._initialized) {
              console.log('Applying enhanced lip sync');
              model._wavFileHandler._initialized = true;

              const originalUpdate = model._wavFileHandler.update.bind(model._wavFileHandler);
              model._wavFileHandler.update = function (deltaTimeSeconds: number) {
                const result = originalUpdate(deltaTimeSeconds);
                // @ts-ignore
                this._lastRms = Math.min(2.0, this._lastRms * lipSyncScale);
                return result;
              };
            }

            if (currentAudioRef.current === audio) {
              model._wavFileHandler.start(audioDataUrl);
            } else {
              console.warn('WavFileHandler start skipped - audio was stopped');
            }
          }
        });

        audio.addEventListener('ended', () => {
          console.log("Audio playback completed");
          cleanup();
        });

        audio.addEventListener('error', (error) => {
          console.error("Audio playback error:", error);
          // Add more detailed error information
          const audioElement = error.target as HTMLAudioElement;
          console.error("Audio error code:", audioElement.error?.code);
          console.error("Audio error message:", audioElement.error?.message);
          cleanup();
        });

        // Set the source after adding event listeners
        audio.src = audioDataUrl;
        audio.load();
      } else {
        resolve();
      }
    } catch (error) {
      console.error('Audio playback setup error:', error);
      currentAudioRef.current = null;
      currentModelRef.current = null;
      resolve();
    }
  });

  /**
   * Add a new audio task to the queue
   */
  const addAudioTask = async (options: AudioTaskOptions) => {
    console.log(`Playing audio with expressions: ${options.expressions}`);
    await handleAudioPlayback(options);
  };

  return {
    addAudioTask,
    stopCurrentAudioAndLipSync,
  };
};
</file>

<file path="src/hooks/use-draggable.ts">
'use client';

import { useState, useCallback, useRef, RefObject } from 'react';

interface Position {
  x: number;
  y: number;
}

interface UseDraggableProps {
  initialPosition?: Position;
  onPositionChange?: (position: Position) => void;
  handleRef?: RefObject<HTMLElement>;
}

export const useDraggable = ({ 
  initialPosition = { x: 0, y: 0 },
  onPositionChange,
  handleRef
}: UseDraggableProps) => {
  const [isDragging, setIsDragging] = useState(false);
  const positionRef = useRef<Position>(initialPosition);
  const dragStartRef = useRef<Position>({ x: 0, y: 0 });

  const handleMouseDown = useCallback((e: React.MouseEvent) => {
    const target = e.target as HTMLElement;

    // Do not interfere with clicks on interactive elements like inputs or buttons
    if (
      target.tagName === "INPUT" ||
      target.tagName === "TEXTAREA" ||
      target.tagName === "BUTTON" ||
      target.isContentEditable
    ) {
      return;
    }

    setIsDragging(true);
    dragStartRef.current = {
      x: e.clientX - positionRef.current.x,
      y: e.clientY - positionRef.current.y,
    };
    // Prevent text selection while dragging
    e.preventDefault();
    e.stopPropagation();
  }, []);


  const handleMouseMove = useCallback((e: React.MouseEvent) => {
    if (!isDragging) return;

    const newX = e.clientX - dragStartRef.current.x;
    const newY = e.clientY - dragStartRef.current.y;
    
    positionRef.current = { x: newX, y: newY };

    if (onPositionChange) {
      onPositionChange({ x: newX, y: newY });
    }
  }, [isDragging, onPositionChange]);

  const handleMouseUp = useCallback(() => {
    setIsDragging(false);
  }, []);

  return {
    isDragging,
    listeners: {
      onMouseDown: handleMouseDown,
      onMouseMove: handleMouseMove,
      onMouseUp: handleMouseUp,
      onMouseLeave: handleMouseUp, // Also stop dragging if mouse leaves the element
    },
  };
};
</file>

<file path="src/hooks/use-live2d-expression.ts">
'use client';

import { useCallback } from 'react';
import { ModelInfo } from '@/context/live2d-config-context';

/**
 * Custom hook for handling Live2D model expressions
 */
export const useLive2DExpression = () => {
  /**
   * Set expression for Live2D model
   * @param expressionValue - Expression name (string) or index (number)
   * @param lappAdapter - LAppAdapter instance
   * @param logMessage - Optional message to log on success
   */
  const setExpression = useCallback((
    expressionValue: string | number,
    lappAdapter: any,
    logMessage?: string,
  ) => {
    try {
      if (typeof expressionValue === 'string') {
        // Set expression by name
        lappAdapter.setExpression(expressionValue);
      } else if (typeof expressionValue === 'number') {
        // Set expression by index
        const expressionName = lappAdapter.getExpressionName(expressionValue);
        if (expressionName) {
          lappAdapter.setExpression(expressionName);
        }
      }
      if (logMessage) {
        console.log(logMessage);
      }
    } catch (error) {
      console.error('Failed to set expression:', error);
    }
  }, []);

  /**
   * Reset expression to default
   * @param lappAdapter - LAppAdapter instance
   * @param modelInfo - Current model information
   */
  const resetExpression = useCallback((
    lappAdapter: any,
    modelInfo?: ModelInfo,
  ) => {
    if (!lappAdapter) return;

    try {
      // Check if model is loaded and has expressions
      const model = lappAdapter.getModel();
      if (!model || !model._modelSetting) {
        console.log('Model or model settings not loaded yet, skipping expression reset');
        return;
      }

      // If model has a default emotion defined, use it
      if (modelInfo?.defaultEmotion !== undefined) {
        setExpression(
          modelInfo.defaultEmotion,
          lappAdapter,
          `Reset expression to default: ${modelInfo.defaultEmotion}`,
        );
      } else {
        // Check if model has any expressions before trying to get the first one
        const expressionCount = lappAdapter.getExpressionCount();
        if (expressionCount > 0) {
          const defaultExpressionName = lappAdapter.getExpressionName(0);
          if (defaultExpressionName) {
            setExpression(
              defaultExpressionName,
              lappAdapter,
            );
          }
        }
      }
    } catch (error) {
      console.log('Failed to reset expression:', error);
    }
  }, [setExpression]);

  return {
    setExpression,
    resetExpression,
  };
};
</file>

<file path="src/hooks/use-live2d-model.ts">
'use client';

/* eslint-disable no-underscore-dangle */
/* eslint-disable @typescript-eslint/ban-ts-comment */
/* eslint-disable no-use-before-define */
/* eslint-disable no-param-reassign */
/* eslint-disable @typescript-eslint/no-unused-vars */
// @ts-nocheck
import { useEffect, useRef, useCallback, useState, RefObject } from "react";
import { ModelInfo } from "@/context/live2d-config-context";
import { updateModelConfig } from '@cubismsdksamples/lappdefine';
import { LAppDelegate } from '@cubismsdksamples/lappdelegate';
import { initializeLive2D } from '@cubismsdksamples/main';

interface UseLive2DModelProps {
  modelInfo: ModelInfo | undefined;
  canvasRef: RefObject<HTMLCanvasElement>;
}

interface Position {
  x: number;
  y: number;
}

// Thresholds for tap vs drag detection
const TAP_DURATION_THRESHOLD_MS = 200; // Max duration for a tap
const DRAG_DISTANCE_THRESHOLD_PX = 5; // Min distance to be considered a drag

function parseModelUrl(url: string): { baseUrl: string; modelDir: string; modelFileName: string } {
  try {
    const urlObj = new URL(url);
    const { pathname } = urlObj;

    const lastSlashIndex = pathname.lastIndexOf('/');
    if (lastSlashIndex === -1) {
      throw new Error('Invalid model URL format');
    }

    const fullFileName = pathname.substring(lastSlashIndex + 1);
    const modelFileName = fullFileName.replace('.model3.json', '');

    const secondLastSlashIndex = pathname.lastIndexOf('/', lastSlashIndex - 1);
    if (secondLastSlashIndex === -1) {
      throw new Error('Invalid model URL format');
    }

    const modelDir = pathname.substring(secondLastSlashIndex + 1, lastSlashIndex);
    const baseUrl = `${urlObj.protocol}//${urlObj.host}${pathname.substring(0, secondLastSlashIndex + 1)}`;

    return { baseUrl, modelDir, modelFileName };
  } catch (error) {
    console.error('Error parsing model URL:', error);
    throw error;
  }
}

export const useLive2DModel = ({
  modelInfo,
  canvasRef,
}: UseLive2DModelProps) => {
  const [isDragging, setIsDragging] = useState(false);
  const [position, setPosition] = useState<Position>({ x: 0, y: 0 });
  const dragStartPos = useRef<Position>({ x: 0, y: 0 }); // Screen coordinates at drag start
  const modelStartPos = useRef<Position>({ x: 0, y: 0 }); // Model coordinates at drag start
  const modelPositionRef = useRef<Position>({ x: 0, y: 0 });
  const prevModelUrlRef = useRef<string | null>(null);

  // --- State for Tap vs Drag ---
  const mouseDownTimeRef = useRef<number>(0);
  const mouseDownPosRef = useRef<Position>({ x: 0, y: 0 }); // Screen coords at mousedown
  const isPotentialTapRef = useRef<boolean>(false); // Flag for ongoing potential tap/drag action
  // ---

  useEffect(() => {
    const currentUrl = modelInfo?.url;
    const sdkScale = (window as any).LAppDefine?.CurrentKScale;
    const modelScale = modelInfo?.kScale !== undefined ? Number(modelInfo.kScale) : undefined;

    const needsUpdate = currentUrl &&
                        (currentUrl !== prevModelUrlRef.current ||
                         (sdkScale !== undefined && modelScale !== undefined && sdkScale !== modelScale));

    if (needsUpdate) {
      prevModelUrlRef.current = currentUrl;

      try {
        const { baseUrl, modelDir, modelFileName } = parseModelUrl(currentUrl);

        if (baseUrl && modelDir) {
          updateModelConfig(baseUrl, modelDir, modelFileName, Number(modelInfo.kScale));

          if ((window as any).LAppLive2DManager?.releaseInstance) {
            (window as any).LAppLive2DManager.releaseInstance();
          }
          initializeLive2D();
        }
      } catch (error) {
        console.error('Error processing model URL:', error);
      }
    }
  }, [modelInfo?.url, modelInfo?.kScale]);

  const getModelPosition = useCallback(() => {
    const adapter = (window as any).getLAppAdapter?.();
    if (adapter) {
      const model = adapter.getModel();
      if (model && model._modelMatrix) {
        const matrix = model._modelMatrix.getArray();
        return {
          x: matrix[12],
          y: matrix[13],
        };
      }
    }
    return { x: 0, y: 0 };
  }, []);

  const setModelPosition = useCallback((x: number, y: number) => {
    const adapter = (window as any).getLAppAdapter?.();
    if (adapter) {
      const model = adapter.getModel();
      if (model && model._modelMatrix) {
        const matrix = model._modelMatrix.getArray();

        const newMatrix = [...matrix];
        newMatrix[12] = x;
        newMatrix[13] = y;

        model._modelMatrix.setMatrix(newMatrix);
        modelPositionRef.current = { x, y };
      }
    }
  }, []);

  useEffect(() => {
    const timer = setTimeout(() => {
      const currentPos = getModelPosition();
      modelPositionRef.current = currentPos;
      setPosition(currentPos);
    }, 500);

    return () => clearTimeout(timer);
  }, [modelInfo?.url, getModelPosition]);

  const getCanvasScale = useCallback(() => {
    const canvas = document.getElementById('canvas') as HTMLCanvasElement;
    if (!canvas) return { width: 1, height: 1, scale: 1 };

    const { width } = canvas;
    const { height } = canvas;
    const scale = width / canvas.clientWidth;

    return { width, height, scale };
  }, []);

  const screenToModelPosition = useCallback((screenX: number, screenY: number) => {
    const { width, height, scale } = getCanvasScale();

    const x = ((screenX * scale) / width) * 2 - 1;
    const y = -((screenY * scale) / height) * 2 + 1;

    return { x, y };
  }, [getCanvasScale]);

  const handleMouseDown = useCallback((e: React.MouseEvent) => {
    // Drag functionality is now handled by useDraggable on the parent component.
    // This is kept to avoid breaking dependencies but the logic is removed.
  }, []);

  const handleMouseMove = useCallback((e: React.MouseEvent) => {
    // Drag functionality is now handled by useDraggable on the parent component.
  }, []);

  const handleMouseUp = useCallback((e: React.MouseEvent) => {
    // Drag functionality is now handled by useDraggable on the parent component.
    if (isDragging) {
      setIsDragging(false);
    }
    if (isPotentialTapRef.current) {
      isPotentialTapRef.current = false;
    }
  }, [isDragging]);

  const handleMouseLeave = useCallback((e: React.MouseEvent) => {
    // Stop any ongoing drag when mouse leaves the component
    if (isDragging) {
      console.log('[Drag] Mouse leave - stopping drag');
      setIsDragging(false);
    }
    if (isPotentialTapRef.current) {
      isPotentialTapRef.current = false;
    }
  }, [isDragging]);

  return {
    position,
    isDragging,
    handlers: {
      onMouseDown: handleMouseDown,
      onMouseMove: handleMouseMove,
      onMouseUp: handleMouseUp,
      onMouseLeave: handleMouseLeave,
    },
  };
};
</file>

<file path="src/hooks/use-live2d-resize.ts">
'use client';

/* eslint-disable no-use-before-define */
/* eslint-disable @typescript-eslint/ban-ts-comment */
/* eslint-disable no-underscore-dangle */
import { useEffect, useCallback, RefObject, useRef } from 'react';
import { ModelInfo } from '@/context/live2d-config-context';
import { LAppDelegate } from '@cubismsdksamples/lappdelegate';
import { LAppLive2DManager } from '@cubismsdksamples/lapplive2dmanager';

// Constants for model scaling behavior
const MIN_SCALE = 0.1;
const MAX_SCALE = 5.0;
const EASING_FACTOR = 0.3; // Controls animation smoothness
const WHEEL_SCALE_STEP = 0.03; // Scale change per wheel tick
const DEFAULT_SCALE = 1.0; // Default scale if not specified

interface UseLive2DResizeProps {
  containerRef: RefObject<HTMLDivElement>;
  modelInfo?: ModelInfo;
  showSidebar?: boolean; // Sidebar collapse state
}

/**
 * Applies scale to both model and view matrices
 * @param scale - The scale value to apply
 */
export const applyScale = (scale: number) => {
  try {
    const manager = LAppLive2DManager.getInstance();
    if (!manager) return;

    const model = manager.getModel(0);
    if (!model) return;

    // @ts-ignore
    model._modelMatrix.scale(scale, scale);
  } catch (error) {
    console.debug('Model not ready for scaling yet');
  }
};

/**
 * Hook to handle Live2D model resizing and scaling
 * Provides smooth scaling animation and window resize handling
 */
export const useLive2DResize = ({
  containerRef,
  modelInfo,
  showSidebar,
}: UseLive2DResizeProps) => {
  const animationFrameIdRef = useRef<number | null>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const isResizingRef = useRef<boolean>(false);

  // Initialize scale references
  const initialScale = modelInfo?.kScale || DEFAULT_SCALE;
  const lastScaleRef = useRef<number>(initialScale);
  const targetScaleRef = useRef<number>(initialScale);
  const animationFrameRef = useRef<number | null>(null);
  const isAnimatingRef = useRef<boolean>(false);
  const hasAppliedInitialScale = useRef<boolean>(false);

  // Previous container dimensions for change detection
  const lastContainerDimensionsRef = useRef<{width: number, height: number}>({ width: 0, height: 0 });

  // Previous sidebar state
  const prevSidebarStateRef = useRef<boolean | undefined>(showSidebar);

  /**
   * Reset scale state when model changes
   */
  useEffect(() => {
    const newInitialScale = modelInfo?.kScale || DEFAULT_SCALE;
    lastScaleRef.current = newInitialScale;
    targetScaleRef.current = newInitialScale;
    hasAppliedInitialScale.current = false;

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      isAnimatingRef.current = false;
    }

    const resizeHandle = requestAnimationFrame(() => {
      handleResize();
    });

    return () => cancelAnimationFrame(resizeHandle);
  }, [modelInfo?.url, modelInfo?.kScale]);

  /**
   * Smooth animation loop for scaling
   * Uses linear interpolation for smooth transitions
   */
  const animateEase = useCallback(() => {
    const clampedTargetScale = Math.max(
      MIN_SCALE,
      Math.min(MAX_SCALE, targetScaleRef.current),
    );

    const currentScale = lastScaleRef.current;
    const diff = clampedTargetScale - currentScale;

    const newScale = currentScale + diff * EASING_FACTOR;
    applyScale(newScale);
    lastScaleRef.current = newScale;

    animationFrameRef.current = requestAnimationFrame(animateEase);
  }, []);

  /**
   * Handles mouse wheel events for scaling
   * Initiates smooth scaling animation
   */
  const handleWheel = useCallback((e: WheelEvent) => {
    e.preventDefault();
    if (!modelInfo?.scrollToResize) return;

    const direction = e.deltaY > 0 ? -1 : 1;
    const increment = WHEEL_SCALE_STEP * direction;

    const currentActualScale = lastScaleRef.current;
    const newTargetScale = Math.max(
      MIN_SCALE,
      Math.min(MAX_SCALE, currentActualScale + increment),
    );

    targetScaleRef.current = newTargetScale;

    if (!isAnimatingRef.current) {
      isAnimatingRef.current = true;
      animationFrameRef.current = requestAnimationFrame(animateEase);
    }
  }, [modelInfo?.scrollToResize, animateEase]);

  /**
   * Pre-process container resize
   * Preserve aspect ratio temporarily before actual change
   */
  const beforeResize = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;

    isResizingRef.current = true;

    if (animationFrameIdRef.current !== null) {
      cancelAnimationFrame(animationFrameIdRef.current);
      animationFrameIdRef.current = null;
    }
  }, []);

  /**
   * Handles window/container resize events
   * Updates canvas dimensions and model scaling
   */
  const handleResize = useCallback(() => {
    const canvas = canvasRef.current;
    if (!canvas) {
      return;
    }

    if (!isResizingRef.current) {
      beforeResize();
    }

    try {
      const containerBounds = containerRef.current?.getBoundingClientRect();
      let { width, height } = containerBounds || { width: 0, height: 0 };

      // For the new layout, adjust dimensions to fit the right side container better
      if (width > 0 && height > 0) {
        // Scale down to fit better in the companion area
        const scaleFactor = 0.85; // Leave some padding
        width = width * scaleFactor;
        height = height * scaleFactor;
      }

      const lastDimensions = lastContainerDimensionsRef.current;
      const sidebarChanged = prevSidebarStateRef.current !== showSidebar;
      const dimensionsChanged = Math.abs(lastDimensions.width - width) > 1 || Math.abs(lastDimensions.height - height) > 1;
      const hasChanged = dimensionsChanged || sidebarChanged;

      if (!hasChanged && hasAppliedInitialScale.current) {
        isResizingRef.current = false;
        return;
      }

      lastContainerDimensionsRef.current = { width, height };
      prevSidebarStateRef.current = showSidebar;

      if (width === 0 || height === 0) {
        console.warn('[Resize] Width or Height is zero, skipping canvas/delegate update.');
        isResizingRef.current = false;
        return;
      }

      const dpr = window.devicePixelRatio || 1;
      canvas.width = Math.round(width * dpr);
      canvas.height = Math.round(height * dpr);
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;

      const delegate = LAppDelegate.getInstance();
      if (delegate) {
        delegate.onResize();
      } else {
        console.warn('[Resize] LAppDelegate instance not found.');
      }

      isResizingRef.current = false;
    } catch (error) {
      isResizingRef.current = false;
    }
  }, [containerRef, modelInfo?.kScale, modelInfo?.initialXshift, modelInfo?.initialYshift, showSidebar, beforeResize, canvasRef]);

  // Immediately respond to sidebar state changes
  useEffect(() => {
    if (prevSidebarStateRef.current !== showSidebar) {
      if (animationFrameIdRef.current !== null) {
        cancelAnimationFrame(animationFrameIdRef.current);
      }
      animationFrameIdRef.current = requestAnimationFrame(() => {
        handleResize();
        animationFrameIdRef.current = null;
      });
    }
  }, [showSidebar, handleResize]);

  // Set up event listeners and cleanup for wheel scaling
  useEffect(() => {
    const canvasElement = canvasRef.current;
    if (canvasElement) {
      canvasElement.addEventListener('wheel', handleWheel, { passive: false });
      return () => {
        canvasElement.removeEventListener('wheel', handleWheel);
      };
    }
    return undefined;
  }, [handleWheel, canvasRef]);

  // Clean up animations on unmount
  useEffect(() => () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = undefined;
    }
    if (animationFrameIdRef.current !== null) {
      cancelAnimationFrame(animationFrameIdRef.current);
      animationFrameIdRef.current = null;
    }
  }, []);

  // Monitor container size changes using ResizeObserver
  useEffect(() => {
    const containerElement = containerRef.current;
    if (!containerElement) {
      return undefined;
    }

    if (animationFrameIdRef.current !== null) cancelAnimationFrame(animationFrameIdRef.current);
    animationFrameIdRef.current = requestAnimationFrame(() => {
      handleResize();
      animationFrameIdRef.current = null;
    });

    const observer = new ResizeObserver(() => {
      if (!isResizingRef.current) {
        if (animationFrameIdRef.current !== null) cancelAnimationFrame(animationFrameIdRef.current);
        animationFrameIdRef.current = requestAnimationFrame(() => {
          handleResize();
          animationFrameIdRef.current = null;
        });
      }
    });

    observer.observe(containerElement);

    return () => {
      if (animationFrameIdRef.current !== null) {
        cancelAnimationFrame(animationFrameIdRef.current);
        animationFrameIdRef.current = null;
      }
      observer.disconnect();
    };
  }, [containerRef, handleResize]);

  // Monitor window size changes (mainly for 'pet' mode or fallback)
  useEffect(() => {
    const handleWindowResize = () => {
      if (!isResizingRef.current) {
        if (animationFrameIdRef.current !== null) cancelAnimationFrame(animationFrameIdRef.current);
        animationFrameIdRef.current = requestAnimationFrame(() => {
          handleResize();
          animationFrameIdRef.current = null;
        });
      }
    };

    window.addEventListener('resize', handleWindowResize);

    return () => {
      window.removeEventListener('resize', handleWindowResize);
      if (animationFrameIdRef.current !== null) {
        cancelAnimationFrame(animationFrameIdRef.current);
        animationFrameIdRef.current = null;
      }
    };
  }, [handleResize]);

  return { canvasRef, handleResize };
};

/**
 * Helper function to set model scale with device pixel ratio consideration
 * @deprecated This logic might be better handled within the view matrix scaling
 */
export const setModelScale = (
  model: any,
  kScale: string | number | undefined,
) => {
  if (!model || kScale === undefined) return;
  console.warn("setModelScale is potentially deprecated; scaling is primarily handled by view matrix now.");
};

/**
 * Helper function to center model in container with optional offset
 * This is now primarily handled within handleResize
 */
export const resetModelPosition = (
  model: any,
  width: number, // Logical width (CSS pixels)
  height: number, // Logical height (CSS pixels)
  initialXshift: number | undefined, // Shift in logical pixels
  initialYshift: number | undefined, // Shift in logical pixels
) => {
  if (!model || typeof model.setPosition !== 'function') return;

  const dpr = window.devicePixelRatio || 1;
  const canvasWidth = width * dpr; // Calculate canvas pixel dimensions
  const canvasHeight = height * dpr;

  const initXshiftPixels = Number(initialXshift || 0) * dpr; // Convert shift to canvas pixels
  const initYshiftPixels = Number(initialYshift || 0) * dpr;

  const centerX = canvasWidth / 2 + initXshiftPixels;
  const centerY = canvasHeight / 2 + initYshiftPixels;

  // @ts-ignore
  model.setPosition(centerX, centerY);
};
</file>

<file path="src/hooks/use-message-handler.ts">
import { useCallback, useState, useEffect } from 'react';
import { WebSocketMessage } from './use-websocket';

// Action types from AI backend - updated to match new schema
export interface AIAction {
  action_type: 'SPEAK' | 'PAUSE' | 'SEEK' | 'REPLAY_SEGMENT' | 'END_REACTION';
  id: string;
  trigger_timestamp: number;
  comment: string;

  // SPEAK action fields
  text?: string; // For SPEAK actions
  audio?: string; // For SPEAK actions (generated by TTS)
  pause_video?: boolean; // For SPEAK actions - whether to pause video while speaking

  // PAUSE action fields
  duration_seconds?: number; // For PAUSE actions

  // SEEK action fields
  target_timestamp?: number; // For SEEK actions
  post_seek_behavior?: 'RESUME_PLAYBACK' | 'STAY_PAUSED'; // For SEEK actions

  // REPLAY_SEGMENT action fields
  start_timestamp?: number; // For REPLAY_SEGMENT actions
  end_timestamp?: number; // For REPLAY_SEGMENT actions
  post_replay_behavior?: 'RESUME_FROM_ORIGINAL' | 'STAY_PAUSED_AT_END'; // For REPLAY_SEGMENT actions
}

export interface ProcessingError {
  error_code: string;
  message: string;
}

export type SessionStatus = 'idle' | 'preparing' | 'ready' | 'error';

export interface UseMessageHandlerReturn {
  sessionStatus: SessionStatus;
  processingError: ProcessingError | null;
  receivedActions: AIAction[];
  isSessionReady: boolean;
  handleMessage: (message: WebSocketMessage) => void;
  clearError: () => void;
  resetHandler: () => void;
  clearReceivedActions: () => void;
}

export const useMessageHandler = (): UseMessageHandlerReturn => {
  const [sessionStatus, setSessionStatus] = useState<SessionStatus>('idle');
  const [processingError, setProcessingError] = useState<ProcessingError | null>(null);
  const [receivedActions, setReceivedActions] = useState<AIAction[]>([]);

  const clearReceivedActions = useCallback(() => {
    setReceivedActions([]);
  }, []);

  // Handle incoming WebSocket messages
  const handleMessage = useCallback((message: WebSocketMessage) => {
    console.log('Received WebSocket message:', message);

    switch (message.type) {
      case 'session_ready':
        console.log(' Session is ready! Setting status to ready');
        setSessionStatus('ready');
        setProcessingError(null);
        break;

      case 'processing_error':
        console.error('Processing error received:', message);
        console.error('Full error details:', JSON.stringify(message, null, 2));
        const error: ProcessingError = {
          error_code: message.error_code || 'UNKNOWN_ERROR',
          message: message.message || 'An unknown error occurred'
        };
        setProcessingError(error);
        setSessionStatus('error');
        break;

      case 'ai_action':
        console.log('Received AI action message:', message);
        if (message.action && isValidSingleAction(message.action)) {
          const actionData = message.action;
          const action: AIAction = {
            action_type: actionData.action_type,
            id: actionData.id,
            trigger_timestamp: actionData.trigger_timestamp,
            comment: actionData.comment,
            text: actionData.text,
            audio: actionData.audio,
            pause_video: actionData.pause_video,
            duration_seconds: actionData.duration_seconds,
            target_timestamp: actionData.target_timestamp,
            post_seek_behavior: actionData.post_seek_behavior,
            start_timestamp: actionData.start_timestamp,
            end_timestamp: actionData.end_timestamp,
            post_replay_behavior: actionData.post_replay_behavior
          };

          setReceivedActions(prev => {
            // Insert action in the correct position based on trigger_timestamp
            const newActions = [...prev, action];
            return newActions.sort((a, b) => a.trigger_timestamp - b.trigger_timestamp);
          });
        } else {
          console.warn('Received invalid ai_action message:', message);
        }
        break;

      default:
        console.log('Unhandled message type:', message.type);
        break;
    }
  }, []);

  // Clear current error
  const clearError = useCallback(() => {
    setProcessingError(null);
    if (sessionStatus === 'error') {
      setSessionStatus('idle');
    }
  }, [sessionStatus]);

  // Reset all handler state
  const resetHandler = useCallback(() => {
    setSessionStatus('idle');
    setProcessingError(null);
    setReceivedActions([]);
  }, []);

  const isSessionReady = sessionStatus === 'ready';
  
  // Debug: Log session status changes
  useEffect(() => {
    console.log(' Message handler session status changed:', sessionStatus, 'isReady:', isSessionReady);
  }, [sessionStatus, isSessionReady]);

  return {
    sessionStatus,
    processingError,
    receivedActions,
    isSessionReady,
    handleMessage,
    clearError,
    resetHandler,
    clearReceivedActions
  };
};


// Helper function to validate single action data (for ai_action type)
function isValidSingleAction(actionData: any): boolean {
  return (
    actionData &&
    typeof actionData.action_type === 'string' &&
    ['SPEAK', 'PAUSE', 'SEEK', 'REPLAY_SEGMENT', 'END_REACTION'].includes(actionData.action_type) &&
    typeof actionData.id === 'string' &&
    typeof actionData.trigger_timestamp === 'number' &&
    typeof actionData.comment === 'string'
  );
}
</file>

<file path="src/hooks/use-session.ts">
import { useState, useCallback } from 'react';
import { useSettings } from '@/context/settings-context';

export interface SessionRequest {
  video_url: string;
  start_time: number;
  end_time?: number;
  text: string;
  character_id?: string;
  user_id?: string;
}

export interface SessionResponse {
  session_id: string;
}

export interface SessionError {
  error: string;
  message: string;
}

export type SessionStatus = 'idle' | 'creating' | 'created' | 'error';

export interface UseSessionReturn {
  status: SessionStatus;
  sessionId: string | null;
  error: SessionError | null;
  createSession: (request: SessionRequest) => Promise<string | null>;
  resetSession: () => void;
}

export const useSession = (): UseSessionReturn => {
  const { generalSettings } = useSettings();
  const [status, setStatus] = useState<SessionStatus>('idle');
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [error, setError] = useState<SessionError | null>(null);

  // Create a new session with the backend
  const createSession = useCallback(async (request: SessionRequest): Promise<string | null> => {
    try {
      setStatus('creating');
      setError(null);

      console.log('Creating session with request:', request);
      console.log('Request JSON:', JSON.stringify(request, null, 2));

      const response = await fetch(`${generalSettings.baseUrl}/api/v1/sessions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
      });

      if (!response.ok) {
        if (response.status === 422) {
          // Handle validation errors
          const errorData = await response.json();
          console.log('422 Error details:', errorData); // Debug log
          const sessionError: SessionError = {
            error: errorData.detail?.error || 'VALIDATION_ERROR',
            message: errorData.detail?.message || JSON.stringify(errorData)
          };
          setError(sessionError);
          setStatus('error');
          return null;
        } else {
          // Handle other HTTP errors
          const errorData = await response.text();
          console.log(`${response.status} Error:`, errorData); // Debug log
          const sessionError: SessionError = {
            error: 'HTTP_ERROR',
            message: `Server responded with status ${response.status}: ${errorData}`
          };
          setError(sessionError);
          setStatus('error');
          return null;
        }
      }

      const data: SessionResponse = await response.json();
      
      if (!data.session_id) {
        const sessionError: SessionError = {
          error: 'INVALID_RESPONSE',
          message: 'Server response missing session_id'
        };
        setError(sessionError);
        setStatus('error');
        return null;
      }

      setSessionId(data.session_id);
      setStatus('created');
      
      console.log('Session created successfully:', data.session_id);
      return data.session_id;

    } catch (err) {
      console.error('Failed to create session:', err);
      
      const sessionError: SessionError = {
        error: 'NETWORK_ERROR',
        message: err instanceof Error ? err.message : 'Network error occurred'
      };
      setError(sessionError);
      setStatus('error');
      return null;
    }
  }, []);

  // Reset session state
  const resetSession = useCallback(() => {
    setStatus('idle');
    setSessionId(null);
    setError(null);
  }, []);

  return {
    status,
    sessionId,
    error,
    createSession,
    resetSession
  };
};
</file>

<file path="src/hooks/use-websocket.ts">
import { useEffect, useRef, useState, useCallback } from 'react';

export interface WebSocketMessage {
  type: string;
  [key: string]: any;
}

export interface WebSocketOptions {
  url: string;
  autoReconnect?: boolean;
  reconnectInterval?: number;
  maxReconnectAttempts?: number;
  onMessage?: (message: WebSocketMessage) => void;
  onConnectionChange?: (status: WebSocketStatus) => void;
}

export type WebSocketStatus = 'connecting' | 'connected' | 'disconnected' | 'error';

export interface UseWebSocketReturn {
  status: WebSocketStatus;
  sendMessage: (message: WebSocketMessage) => void;
  connect: () => void;
  disconnect: () => void;
  lastMessage: WebSocketMessage | null;
  error: string | null;
}

export const useWebSocket = (options: WebSocketOptions): UseWebSocketReturn => {
  const {
    url,
    autoReconnect = true,
    reconnectInterval = 3000,
    maxReconnectAttempts = 5,
    onMessage,
    onConnectionChange
  } = options;

  const [status, setStatus] = useState<WebSocketStatus>('disconnected');
  const [lastMessage, setLastMessage] = useState<WebSocketMessage | null>(null);
  const [error, setError] = useState<string | null>(null);
  
  const wsRef = useRef<WebSocket | null>(null);
  const reconnectAttemptsRef = useRef(0);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Store callback functions in refs to avoid dependency issues
  const onConnectionChangeRef = useRef(onConnectionChange);
  const onMessageRef = useRef(onMessage);
  
  useEffect(() => {
    onConnectionChangeRef.current = onConnectionChange;
  }, [onConnectionChange]);

  useEffect(() => {
    onMessageRef.current = onMessage;
  }, [onMessage]);

  // Update connection status and notify parent component
  const updateStatus = useCallback((newStatus: WebSocketStatus) => {
    setStatus(newStatus);
    onConnectionChangeRef.current?.(newStatus);
  }, []);

  // Send message through WebSocket
  const sendMessage = useCallback((message: WebSocketMessage) => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      try {
        wsRef.current.send(JSON.stringify(message));
      } catch (err) {
        console.error('Failed to send WebSocket message:', err);
        setError('Failed to send message');
      }
    } else {
      console.warn('WebSocket is not connected. Cannot send message:', message);
      setError('WebSocket not connected');
    }
  }, []);

  // Connect to WebSocket
  const connect = useCallback(() => {
    if (!url || url.trim() === '') {
      console.warn('Cannot connect: WebSocket URL is empty');
      setError('WebSocket URL is not configured');
      return;
    }

    if (wsRef.current?.readyState === WebSocket.OPEN) {
      return; // Already connected
    }

    try {
      setError(null);
      updateStatus('connecting');
      
      wsRef.current = new WebSocket(url);

      wsRef.current.onopen = () => {
        console.log(' WebSocket connected successfully to:', url);
        updateStatus('connected');
        reconnectAttemptsRef.current = 0; // Reset reconnect attempts on successful connection
      };

      wsRef.current.onmessage = (event) => {
        try {
          const message: WebSocketMessage = JSON.parse(event.data);
          setLastMessage(message);
          onMessageRef.current?.(message);
        } catch (err) {
          console.error('Failed to parse WebSocket message:', event.data, err);
          setError('Invalid message format received');
        }
      };

      wsRef.current.onclose = (event) => {
        console.log(' WebSocket disconnected:', event.code, event.reason);
        updateStatus('disconnected');
        
        // Attempt to reconnect if autoReconnect is enabled and we haven't exceeded max attempts
        if (autoReconnect && reconnectAttemptsRef.current < maxReconnectAttempts) {
          reconnectAttemptsRef.current++;
          console.log(`Attempting to reconnect (${reconnectAttemptsRef.current}/${maxReconnectAttempts})...`);
          
          reconnectTimeoutRef.current = setTimeout(() => {
            connect();
          }, reconnectInterval);
        } else if (reconnectAttemptsRef.current >= maxReconnectAttempts) {
          setError(`Failed to reconnect after ${maxReconnectAttempts} attempts`);
        }
      };

      wsRef.current.onerror = (event) => {
        console.error(' WebSocket error:', event);
        console.error('WebSocket URL that failed:', url);
        updateStatus('error');
        setError('Connection error occurred');
      };

    } catch (err) {
      console.error('Failed to create WebSocket connection:', err);
      updateStatus('error');
      setError('Failed to create connection');
    }
  }, [url, autoReconnect, reconnectInterval, maxReconnectAttempts, updateStatus]);

  // Disconnect from WebSocket
  const disconnect = useCallback(() => {
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current);
      reconnectTimeoutRef.current = null;
    }

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
    
    updateStatus('disconnected');
    reconnectAttemptsRef.current = 0;
  }, [updateStatus]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (reconnectTimeoutRef.current) {
        clearTimeout(reconnectTimeoutRef.current);
        reconnectTimeoutRef.current = null;
      }

      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }
    };
  }, []);

  return {
    status,
    sendMessage,
    connect,
    disconnect,
    lastMessage,
    error
  };
};
</file>

<file path="src/lib/react-player-config.ts">
/**
 * React Player configuration for different video sources
 */

export interface ReactPlayerConfig {
  youtube: {
    rel?: number;
    iv_load_policy?: number;
    color?: 'red' | 'white';
    cc_load_policy?: number;
    disablekb?: number;
    enablejsapi?: number;
    fs?: number;
    hl?: string;
    origin?: string;
    playlist?: string;
    start?: number;
    end?: number;
  };
  vimeo: {
    byline?: boolean;
    portrait?: boolean;
    title?: boolean;
  };
  file: {
    attributes?: {
      crossOrigin?: string;
      preload?: string;
    };
    forceHLS?: boolean;
    forceDASH?: boolean;
  };
}

/**
 * Default configuration for react-player
 * Optimized for custom controls and multi-source support
 */
export const defaultPlayerConfig = {
  youtube: {
    rel: 0 as const,             // Don't show related videos
    iv_load_policy: 3 as const,  // Hide annotations
  },
  vimeo: {
    byline: false,      // Hide byline
    portrait: false,    // Hide portrait
    title: false,       // Hide title
  },
  file: {
    attributes: {
      crossOrigin: 'anonymous',
      preload: 'metadata',
    },
    forceHLS: false,      // Let react-player auto-detect
    forceDASH: false,     // Let react-player auto-detect
  },
};

/**
 * Supported video file formats
 */
export const supportedVideoFormats = [
  'mp4',
  'webm',
  'ogg',
  'avi',
  'mov',
  'wmv',
  'flv',
  'm4v',
  '3gp',
  'mkv',
];

/**
 * Supported video MIME types
 */
export const supportedMimeTypes = [
  'video/mp4',
  'video/webm',
  'video/ogg',
  'video/avi',
  'video/quicktime',
  'video/x-msvideo',
  'video/x-ms-wmv',
  'video/x-flv',
  'video/x-m4v',
  'video/3gpp',
  'video/x-matroska',
];

/**
 * Platform detection for video URLs
 */
export const detectVideoPlatform = (url: string): 'youtube' | 'bilibili' | 'vimeo' | 'file' | 'unknown' => {
  if (!url) return 'unknown';
  
  // YouTube patterns
  if (url.includes('youtube.com') || url.includes('youtu.be')) {
    return 'youtube';
  }
  
  // Bilibili patterns
  if (url.includes('bilibili.com') || url.includes('b23.tv')) {
    return 'bilibili';
  }
  
  // Vimeo patterns
  if (url.includes('vimeo.com')) {
    return 'vimeo';
  }
  
  // Check if it's a direct file URL
  const fileExtension = url.split('.').pop()?.toLowerCase();
  if (fileExtension && supportedVideoFormats.includes(fileExtension)) {
    return 'file';
  }
  
  return 'unknown';
};

/**
 * Validate video file format
 */
export const isValidVideoFile = (file: File): boolean => {
  const fileExtension = file.name.split('.').pop()?.toLowerCase();
  return fileExtension ? supportedVideoFormats.includes(fileExtension) : false;
};

/**
 * Validate video URL format
 */
export const isValidVideoUrl = (url: string): boolean => {
  try {
    new URL(url);
    const platform = detectVideoPlatform(url);
    return platform !== 'unknown';
  } catch {
    return false;
  }
};
</file>

<file path="src/lib/utils.ts">
import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
</file>

<file path="src/types/video-player.ts">
/**
 * TypeScript type definitions for video player functionality
 */

import ReactPlayer from 'react-player';

/**
 * Video progress information from react-player
 */
export interface VideoProgress {
  /** Fraction of the video that has been played (0-1) */
  played: number;
  /** Seconds of the video that have been played */
  playedSeconds: number;
  /** Fraction of the video that has been loaded (0-1) */
  loaded: number;
  /** Seconds of the video that have been loaded */
  loadedSeconds: number;
}

/**
 * Video duration information
 */
export interface VideoDuration {
  /** Total duration in seconds */
  duration: number;
}

/**
 * Video player state
 */
export interface VideoPlayerState {
  /** Current video URL */
  url: string | null;
  /** Whether video is currently playing */
  isPlaying: boolean;
  /** Whether video is loading */
  isLoading: boolean;
  /** Current playback time in seconds */
  currentTime: number;
  /** Total video duration in seconds */
  duration: number;
  /** Current volume (0-1) */
  volume: number;
  /** Current playback rate */
  playbackRate: number;
  /** Whether video is muted */
  muted: boolean;
  /** Current error if any */
  error: string | null;
}

/**
 * Video processing options for upload
 */
export interface VideoProcessingOptions {
  /** Start time in seconds */
  startTime?: number;
  /** End time in seconds */
  endTime?: number;
  /** Target FPS for processing */
  fps?: number;
  /** User requirements and instructions */
  userRequirements?: string;
  /** Additional files uploaded with video */
  additionalFiles?: File[];
}

/**
 * Video source information
 */
export interface VideoSource {
  /** Source type */
  type: 'file' | 'url';
  /** File object if type is 'file' */
  file?: File;
  /** URL string if type is 'url' */
  url?: string;
  /** Detected platform for URL sources */
  platform?: 'youtube' | 'bilibili' | 'vimeo' | 'file' | 'unknown';
}

/**
 * Video upload form state
 */
export interface VideoUploadState {
  /** Video source information */
  videoSource: VideoSource;
  /** Processing options */
  processingOptions: VideoProcessingOptions;
  /** Whether form is currently processing */
  isProcessing: boolean;
  /** Video preview information */
  preview?: {
    /** Thumbnail URL */
    thumbnail: string;
    /** Video duration in seconds */
    duration: number;
    /** Video title if available */
    title?: string;
  };
}

/**
 * React Player ref type for instance methods
 */
export type ReactPlayerRef = typeof ReactPlayer;

/**
 * Video player error types
 */
export interface VideoError {
  /** Error code */
  code: 'NETWORK_ERROR' | 'FORMAT_UNSUPPORTED' | 'DECODE_ERROR' | 'SRC_NOT_SUPPORTED' | 'UNKNOWN_ERROR';
  /** Human-readable error message */
  message: string;
  /** Whether the error is recoverable */
  recoverable: boolean;
  /** Original error object if available */
  originalError?: Error;
}

/**
 * Video player event handlers
 */
export interface VideoPlayerEventHandlers {
  /** Called when video is ready to play */
  onReady?: () => void;
  /** Called when video starts playing */
  onStart?: () => void;
  /** Called when play state changes */
  onPlay?: () => void;
  /** Called when video is paused */
  onPause?: () => void;
  /** Called on progress updates */
  onProgress?: (progress: VideoProgress) => void;
  /** Called when duration is available */
  onDuration?: (duration: number) => void;
  /** Called when video ends */
  onEnded?: () => void;
  /** Called when an error occurs */
  onError?: (error: VideoError) => void;
  /** Called when seeking starts */
  onSeeking?: () => void;
  /** Called when seeking ends */
  onSeeked?: () => void;
}

/**
 * Video player control methods
 */
export interface VideoPlayerControls {
  /** Play the video */
  play: () => void;
  /** Pause the video */
  pause: () => void;
  /** Seek to specific time in seconds */
  seekTo: (seconds: number) => void;
  /** Set volume (0-1) */
  setVolume: (volume: number) => void;
  /** Set playback rate */
  setPlaybackRate: (rate: number) => void;
  /** Toggle mute */
  toggleMute: () => void;
  /** Get current time */
  getCurrentTime: () => number;
  /** Get duration */
  getDuration: () => number;
}
</file>

</files>
